{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1d8a49f",
   "metadata": {},
   "source": [
    "# Notebook 06: Ergebnisanalyse und Visualisierung\n",
    "\n",
    "## Ziel dieses Notebooks\n",
    "\n",
    "In diesem Notebook werden wir:\n",
    "\n",
    "1. **Alle Ergebnisse laden** - LLM Baseline + SLM Finetuned\n",
    "2. **Umfassende Analyse** - St√§rken und Schw√§chen identifizieren\n",
    "3. **Visualisierungen erstellen** - Publikationsreife Plots\n",
    "4. **HTML-Report generieren** - F√ºr Pr√§sentation\n",
    "5. **Fazit formulieren** - Wissenschaftliche Schlussfolgerungen\n",
    "\n",
    "---\n",
    "\n",
    "## Forschungsfrage\n",
    "\n",
    "> **K√∂nnen spezialisierte kleine Sprachmodelle (3B) durch Finetuning die Performance von gro√üen generischen Modellen (7-8B) bei der ICD-10 Klassifikation √ºbertreffen?**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd11513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP: Imports und Umgebung\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Plot-Stil\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "print(\"Imports erfolgreich!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726dcdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# KONFIGURATION (Standalone)\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class PathConfig:\n",
    "    project_root: Path = field(default_factory=lambda: Path.cwd().parent)\n",
    "    outputs_dir: Path = field(default_factory=lambda: Path.cwd().parent / \"outputs\")\n",
    "    plots_dir: Path = field(default_factory=lambda: Path.cwd().parent / \"outputs\" / \"plots\")\n",
    "    reports_dir: Path = field(default_factory=lambda: Path.cwd().parent / \"outputs\" / \"reports\")\n",
    "    \n",
    "    def create_directories(self):\n",
    "        for attr_name in dir(self):\n",
    "            attr = getattr(self, attr_name)\n",
    "            if isinstance(attr, Path) and not attr_name.startswith('_'):\n",
    "                attr.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "paths = PathConfig()\n",
    "paths.create_directories()\n",
    "\n",
    "print(\"Pfade konfiguriert!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fdc3d9",
   "metadata": {},
   "source": [
    "## 1. Ergebnisse laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c5044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ALLE ERGEBNISSE LADEN\n",
    "# ============================================================\n",
    "\n",
    "# LLM Baseline\n",
    "llm_path = paths.reports_dir / \"llm_baseline_results.json\"\n",
    "if llm_path.exists():\n",
    "    with open(llm_path, 'r') as f:\n",
    "        llm_results = json.load(f)\n",
    "    print(f\"LLM Baseline: {len(llm_results)} Modelle\")\n",
    "else:\n",
    "    print(\"LLM Baseline nicht gefunden!\")\n",
    "    llm_results = {}\n",
    "\n",
    "# SLM Finetuned\n",
    "slm_path = paths.reports_dir / \"slm_finetuned_results.json\"\n",
    "if slm_path.exists():\n",
    "    with open(slm_path, 'r') as f:\n",
    "        slm_results = json.load(f)\n",
    "    print(f\"SLM Finetuned: {len(slm_results)} Modelle\")\n",
    "else:\n",
    "    print(\"SLM Finetuned nicht gefunden\")\n",
    "    slm_results = {}\n",
    "\n",
    "# Kombiniert\n",
    "all_results = {}\n",
    "all_results.update(llm_results)\n",
    "all_results.update(slm_results)\n",
    "\n",
    "print(f\"\\nGesamt: {len(all_results)} Modelle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dee0ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ERGEBNIS-DATAFRAME ERSTELLEN\n",
    "# ============================================================\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for key, metrics in all_results.items():\n",
    "    model_type = \"LLM\" if key.startswith(\"LLM\") else \"SLM\"\n",
    "    short_name = key.replace(\"LLM_\", \"\").replace(\"SLM_\", \"\").replace(\"_untrained\", \"\").replace(\"_finetuned\", \"\")\n",
    "    \n",
    "    results_list.append({\n",
    "        \"model_key\": key,\n",
    "        \"model_name\": short_name,\n",
    "        \"model_type\": model_type,\n",
    "        \"model_size\": metrics.get(\"model_size\", \"\"),\n",
    "        \"training\": metrics.get(\"training\", \"\"),\n",
    "        \"accuracy\": metrics.get(\"exact_match_accuracy\", 0),\n",
    "        \"prefix_3\": metrics.get(\"prefix_match_3\", 0),\n",
    "        \"prefix_1\": metrics.get(\"prefix_match_1\", 0),\n",
    "        \"precision\": metrics.get(\"precision\", 0),\n",
    "        \"recall\": metrics.get(\"recall\", 0),\n",
    "        \"f1\": metrics.get(\"f1\", 0),\n",
    "        \"n_samples\": metrics.get(\"n_samples\", 0),\n",
    "        \"eval_time\": metrics.get(\"eval_time_seconds\", 0),\n",
    "        \"samples_per_sec\": metrics.get(\"samples_per_second\", 0),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results_list)\n",
    "df = df.sort_values(\"accuracy\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Ergebnis-Tabelle:\")\n",
    "print(df[[\"model_name\", \"model_type\", \"model_size\", \"training\", \"accuracy\", \"f1\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b79a18",
   "metadata": {},
   "source": [
    "## 2. Hauptvergleich: LLM vs. SLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01931b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HAUPTVISUALISIERUNG\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Farben\n",
    "colors_dict = {\n",
    "    \"LLM\": \"#E74C3C\",   # Rot f√ºr LLM\n",
    "    \"SLM\": \"#27AE60\",   # Gr√ºn f√ºr SLM\n",
    "}\n",
    "\n",
    "# 1. Accuracy-Vergleich\n",
    "ax1 = axes[0]\n",
    "colors = [colors_dict[t] for t in df[\"model_type\"]]\n",
    "bars = ax1.bar(range(len(df)), df[\"accuracy\"], color=colors, edgecolor='black', linewidth=1.2)\n",
    "ax1.set_xticks(range(len(df)))\n",
    "ax1.set_xticklabels(df[\"model_name\"], rotation=45, ha='right', fontsize=10)\n",
    "ax1.set_ylabel('Exact Match Accuracy')\n",
    "ax1.set_title('ICD-10 Klassifikationsgenauigkeit')\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Baseline 50%')\n",
    "\n",
    "for bar, acc in zip(bars, df[\"accuracy\"]):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "             f'{acc:.1%}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# 2. Metriken-Radar (simuliert als Grouped Bar)\n",
    "ax2 = axes[1]\n",
    "metrics = [\"Accuracy\", \"Prefix-3\", \"Precision\", \"Recall\", \"F1\"]\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.8 / len(df)\n",
    "\n",
    "for i, (idx, row) in enumerate(df.iterrows()):\n",
    "    values = [row[\"accuracy\"], row[\"prefix_3\"], row[\"precision\"], row[\"recall\"], row[\"f1\"]]\n",
    "    color = colors_dict[row[\"model_type\"]]\n",
    "    ax2.bar(x + i*width, values, width, label=row[\"model_name\"], color=color, alpha=0.7 + 0.1*i)\n",
    "\n",
    "ax2.set_xticks(x + width * (len(df)-1) / 2)\n",
    "ax2.set_xticklabels(metrics)\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_title('Metriken-Vergleich')\n",
    "ax2.legend(fontsize=8, loc='lower right')\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# 3. Modellgr√∂√üe vs. Accuracy\n",
    "ax3 = axes[2]\n",
    "sizes_numeric = {\"3B\": 3, \"7B\": 7, \"8B\": 8}\n",
    "for idx, row in df.iterrows():\n",
    "    size = sizes_numeric.get(row[\"model_size\"], 5)\n",
    "    color = colors_dict[row[\"model_type\"]]\n",
    "    marker = 'o' if row[\"model_type\"] == \"LLM\" else 's'\n",
    "    ax3.scatter(size, row[\"accuracy\"], s=200, c=color, marker=marker, \n",
    "                edgecolor='black', linewidth=1.5, label=row[\"model_name\"], zorder=5)\n",
    "\n",
    "ax3.set_xlabel('Modellgr√∂√üe (Milliarden Parameter)')\n",
    "ax3.set_ylabel('Exact Match Accuracy')\n",
    "ax3.set_title('Gr√∂√üe vs. Performance')\n",
    "ax3.set_xlim(0, 10)\n",
    "ax3.set_ylim(0, 1)\n",
    "ax3.legend(fontsize=8)\n",
    "\n",
    "# Legende\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#E74C3C', edgecolor='black', label='LLM (Zero-Shot)'),\n",
    "    Patch(facecolor='#27AE60', edgecolor='black', label='SLM (Finetuned)'),\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='upper center', ncol=2, bbox_to_anchor=(0.5, 1.02), fontsize=11)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.savefig(paths.plots_dir / 'main_comparison.png', dpi=200, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Plot gespeichert: {paths.plots_dir / 'main_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d540d5",
   "metadata": {},
   "source": [
    "## 3. Detailanalyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116a1c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STATISTISCHE ANALYSE\n",
    "# ============================================================\n",
    "\n",
    "print(\"üìà Statistische Analyse\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# LLM vs. SLM Vergleich\n",
    "llm_df = df[df[\"model_type\"] == \"LLM\"]\n",
    "slm_df = df[df[\"model_type\"] == \"SLM\"]\n",
    "\n",
    "print(\"\\nLLM (Zero-Shot) Statistiken:\")\n",
    "if len(llm_df) > 0:\n",
    "    print(f\"   Accuracy: {llm_df['accuracy'].mean():.2%} (¬±{llm_df['accuracy'].std():.2%})\")\n",
    "    print(f\"   F1: {llm_df['f1'].mean():.4f}\")\n",
    "    print(f\"   Beste: {llm_df.iloc[0]['model_name']} ({llm_df['accuracy'].max():.2%})\")\n",
    "\n",
    "print(\"\\nSLM (Finetuned) Statistiken:\")\n",
    "if len(slm_df) > 0:\n",
    "    print(f\"   Accuracy: {slm_df['accuracy'].mean():.2%} (¬±{slm_df['accuracy'].std():.2%})\")\n",
    "    print(f\"   F1: {slm_df['f1'].mean():.4f}\")\n",
    "    print(f\"   Beste: {slm_df.iloc[0]['model_name']} ({slm_df['accuracy'].max():.2%})\")\n",
    "\n",
    "# Verbesserung berechnen\n",
    "if len(llm_df) > 0 and len(slm_df) > 0:\n",
    "    llm_best = llm_df['accuracy'].max()\n",
    "    slm_best = slm_df['accuracy'].max()\n",
    "    improvement = (slm_best - llm_best) / llm_best * 100\n",
    "    \n",
    "    print(f\"\\nVerbesserung durch Finetuning:\")\n",
    "    if improvement > 0:\n",
    "        print(f\"   +{improvement:.1f}% Accuracy-Gewinn\")\n",
    "    else:\n",
    "        print(f\"   {improvement:.1f}% Accuracy-Verlust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31b7091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EFFIZIENZ-ANALYSE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n‚ö° Effizienz-Analyse\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    size = {\"3B\": 3, \"7B\": 7, \"8B\": 8}.get(row[\"model_size\"], 5)\n",
    "    color = colors_dict[row[\"model_type\"]]\n",
    "    \n",
    "    # Bubble-Size basiert auf Speed\n",
    "    bubble_size = max(50, row[\"samples_per_sec\"] * 50) if row[\"samples_per_sec\"] > 0 else 100\n",
    "    \n",
    "    ax.scatter(size, row[\"accuracy\"], s=bubble_size, c=color, \n",
    "               alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "    ax.annotate(row[\"model_name\"], (size, row[\"accuracy\"]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "ax.set_xlabel('Modellgr√∂√üe (Milliarden Parameter)')\n",
    "ax.set_ylabel('Exact Match Accuracy')\n",
    "ax.set_title('Effizienz: Gr√∂√üe vs. Performance\\n(Bubble-Gr√∂√üe = Inference-Speed)')\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Legende\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#E74C3C', edgecolor='black', label='LLM (Zero-Shot)'),\n",
    "    Patch(facecolor='#27AE60', edgecolor='black', label='SLM (Finetuned)'),\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(paths.plots_dir / 'efficiency_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02774df4",
   "metadata": {},
   "source": [
    "## 4. HTML-Report generieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HTML REPORT GENERIEREN\n",
    "# ============================================================\n",
    "\n",
    "def generate_html_report(df: pd.DataFrame, llm_results: dict, slm_results: dict, output_path: Path):\n",
    "    \"\"\"Generiert einen HTML-Report.\"\"\"\n",
    "    \n",
    "    # Beste Modelle ermitteln\n",
    "    best_overall = df.iloc[0]\n",
    "    best_llm = df[df[\"model_type\"] == \"LLM\"].iloc[0] if len(df[df[\"model_type\"] == \"LLM\"]) > 0 else None\n",
    "    best_slm = df[df[\"model_type\"] == \"SLM\"].iloc[0] if len(df[df[\"model_type\"] == \"SLM\"]) > 0 else None\n",
    "    \n",
    "    # Verbesserung\n",
    "    improvement = \"N/A\"\n",
    "    if best_llm is not None and best_slm is not None:\n",
    "        imp = (best_slm[\"accuracy\"] - best_llm[\"accuracy\"]) / best_llm[\"accuracy\"] * 100\n",
    "        improvement = f\"+{imp:.1f}%\" if imp > 0 else f\"{imp:.1f}%\"\n",
    "    \n",
    "    html = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"de\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Medical Diagnosis Finetuning - Evaluation Report</title>\n",
    "    <style>\n",
    "        body {{\n",
    "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "            line-height: 1.6;\n",
    "            max-width: 1200px;\n",
    "            margin: 0 auto;\n",
    "            padding: 20px;\n",
    "            background-color: #f5f5f5;\n",
    "        }}\n",
    "        .header {{\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            color: white;\n",
    "            padding: 30px;\n",
    "            border-radius: 10px;\n",
    "            margin-bottom: 30px;\n",
    "        }}\n",
    "        .header h1 {{\n",
    "            margin: 0;\n",
    "            font-size: 2em;\n",
    "        }}\n",
    "        .header p {{\n",
    "            margin: 10px 0 0 0;\n",
    "            opacity: 0.9;\n",
    "        }}\n",
    "        .card {{\n",
    "            background: white;\n",
    "            border-radius: 10px;\n",
    "            padding: 20px;\n",
    "            margin-bottom: 20px;\n",
    "            box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n",
    "        }}\n",
    "        .card h2 {{\n",
    "            color: #333;\n",
    "            border-bottom: 2px solid #667eea;\n",
    "            padding-bottom: 10px;\n",
    "        }}\n",
    "        table {{\n",
    "            width: 100%;\n",
    "            border-collapse: collapse;\n",
    "            margin: 20px 0;\n",
    "        }}\n",
    "        th, td {{\n",
    "            padding: 12px;\n",
    "            text-align: left;\n",
    "            border-bottom: 1px solid #ddd;\n",
    "        }}\n",
    "        th {{\n",
    "            background-color: #667eea;\n",
    "            color: white;\n",
    "        }}\n",
    "        tr:hover {{\n",
    "            background-color: #f5f5f5;\n",
    "        }}\n",
    "        .metric-grid {{\n",
    "            display: grid;\n",
    "            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));\n",
    "            gap: 20px;\n",
    "            margin: 20px 0;\n",
    "        }}\n",
    "        .metric-box {{\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            color: white;\n",
    "            padding: 20px;\n",
    "            border-radius: 10px;\n",
    "            text-align: center;\n",
    "        }}\n",
    "        .metric-box .value {{\n",
    "            font-size: 2em;\n",
    "            font-weight: bold;\n",
    "        }}\n",
    "        .metric-box .label {{\n",
    "            opacity: 0.9;\n",
    "        }}\n",
    "        .llm {{\n",
    "            color: #E74C3C;\n",
    "            font-weight: bold;\n",
    "        }}\n",
    "        .slm {{\n",
    "            color: #27AE60;\n",
    "            font-weight: bold;\n",
    "        }}\n",
    "        .highlight {{\n",
    "            background-color: #e8f5e9;\n",
    "        }}\n",
    "        .conclusion {{\n",
    "            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);\n",
    "            color: white;\n",
    "            padding: 20px;\n",
    "            border-radius: 10px;\n",
    "            margin-top: 30px;\n",
    "        }}\n",
    "        .conclusion h2 {{\n",
    "            color: white;\n",
    "            border-bottom: 2px solid rgba(255,255,255,0.5);\n",
    "        }}\n",
    "        img {{\n",
    "            max-width: 100%;\n",
    "            border-radius: 10px;\n",
    "            margin: 10px 0;\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"header\">\n",
    "        <h1>Medical Diagnosis Finetuning</h1>\n",
    "        <p>Evaluation Report - ICD-10 Klassifikation mit LLMs und SLMs</p>\n",
    "        <p>Generiert: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"card\">\n",
    "        <h2>Key Metrics</h2>\n",
    "        <div class=\"metric-grid\">\n",
    "            <div class=\"metric-box\">\n",
    "                <div class=\"value\">{len(all_results)}</div>\n",
    "                <div class=\"label\">Modelle evaluiert</div>\n",
    "            </div>\n",
    "            <div class=\"metric-box\">\n",
    "                <div class=\"value\">{best_overall['accuracy']:.1%}</div>\n",
    "                <div class=\"label\">Beste Accuracy</div>\n",
    "            </div>\n",
    "            <div class=\"metric-box\">\n",
    "                <div class=\"value\">{best_overall['model_name']}</div>\n",
    "                <div class=\"label\">Bestes Modell</div>\n",
    "            </div>\n",
    "            <div class=\"metric-box\">\n",
    "                <div class=\"value\">{improvement}</div>\n",
    "                <div class=\"label\">SLM vs LLM</div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"card\">\n",
    "        <h2>Ergebnis√ºbersicht</h2>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th>Modell</th>\n",
    "                <th>Typ</th>\n",
    "                <th>Gr√∂√üe</th>\n",
    "                <th>Training</th>\n",
    "                <th>Accuracy</th>\n",
    "                <th>F1</th>\n",
    "            </tr>\n",
    "\"\"\"\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        highlight = 'highlight' if idx == 0 else ''\n",
    "        typ_class = 'llm' if row['model_type'] == 'LLM' else 'slm'\n",
    "        html += f\"\"\"\n",
    "            <tr class=\"{highlight}\">\n",
    "                <td><strong>{row['model_name']}</strong></td>\n",
    "                <td class=\"{typ_class}\">{row['model_type']}</td>\n",
    "                <td>{row['model_size']}</td>\n",
    "                <td>{row['training']}</td>\n",
    "                <td><strong>{row['accuracy']:.2%}</strong></td>\n",
    "                <td>{row['f1']:.4f}</td>\n",
    "            </tr>\n",
    "\"\"\"\n",
    "    \n",
    "    html += \"\"\"\n",
    "        </table>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"card\">\n",
    "        <h2>Visualisierungen</h2>\n",
    "        <p>Die folgenden Plots wurden generiert:</p>\n",
    "        <ul>\n",
    "            <li><strong>main_comparison.png</strong> - Hauptvergleich aller Modelle</li>\n",
    "            <li><strong>efficiency_analysis.png</strong> - Effizienz-Analyse (Gr√∂√üe vs. Performance)</li>\n",
    "            <li><strong>slm_vs_llm_comparison.png</strong> - Detailvergleich SLM vs. LLM</li>\n",
    "        </ul>\n",
    "        <img src=\"main_comparison.png\" alt=\"Hauptvergleich\">\n",
    "    </div>\n",
    "\"\"\"\n",
    "    \n",
    "    # Fazit\n",
    "    if best_slm is not None and best_llm is not None:\n",
    "        if best_slm[\"accuracy\"] > best_llm[\"accuracy\"]:\n",
    "            conclusion = f\"\"\"\n",
    "            <p><strong>Hypothese best√§tigt:</strong> Das finetuned SLM ({best_slm['model_name']}) \n",
    "            √ºbertrifft das beste LLM ({best_llm['model_name']}) um {improvement}.</p>\n",
    "            <p>Dies zeigt, dass <strong>Spezialisierung > Gr√∂√üe</strong> f√ºr dom√§nenspezifische Aufgaben gilt.</p>\n",
    "            \"\"\"\n",
    "        else:\n",
    "            conclusion = f\"\"\"\n",
    "            <p><strong>Hypothese nicht best√§tigt:</strong> Das LLM ({best_llm['model_name']}) \n",
    "            ist weiterhin besser als das finetuned SLM.</p>\n",
    "            <p>M√∂gliche Verbesserungen: Mehr Training, bessere Hyperparameter, gr√∂√üere Datens√§tze.</p>\n",
    "            \"\"\"\n",
    "    else:\n",
    "        conclusion = \"<p>Nicht gen√ºgend Daten f√ºr eine Schlussfolgerung.</p>\"\n",
    "    \n",
    "    html += f\"\"\"\n",
    "    <div class=\"conclusion\">\n",
    "        <h2>üéØ Fazit</h2>\n",
    "        {conclusion}\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"card\">\n",
    "        <h2>Methodologie</h2>\n",
    "        <ul>\n",
    "            <li><strong>Dataset:</strong> MedSynth (Ahmad0067/MedSynth) - Synthetische medizinische Dialoge</li>\n",
    "            <li><strong>Split:</strong> 70% Train, 15% Val, 15% Test</li>\n",
    "            <li><strong>LLM Evaluation:</strong> Zero-Shot mit 4-bit Quantisierung</li>\n",
    "            <li><strong>SLM Training:</strong> LoRA (r=64, alpha=128), 3 Epochs</li>\n",
    "            <li><strong>Metriken:</strong> Exact Match Accuracy, Prefix Match, F1-Score</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    \n",
    "    <footer style=\"text-align: center; padding: 20px; color: #666;\">\n",
    "        <p>Medical Diagnosis Finetuning Pipeline - {datetime.now().year}</p>\n",
    "    </footer>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(html)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Report generieren\n",
    "report_path = generate_html_report(df, llm_results, slm_results, paths.reports_dir / \"evaluation_report.html\")\n",
    "print(f\"HTML-Report generiert: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d1f761",
   "metadata": {},
   "source": [
    "## 5. Finale Zusammenfassung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88f55b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINALE JSON-ZUSAMMENFASSUNG\n",
    "# ============================================================\n",
    "\n",
    "final_summary = {\n",
    "    \"project\": \"Medical Diagnosis Finetuning\",\n",
    "    \"description\": \"ICD-10 Klassifikation mit LLMs und finetuned SLMs\",\n",
    "    \"dataset\": {\n",
    "        \"name\": \"Ahmad0067/MedSynth\",\n",
    "        \"split\": {\"train\": 0.70, \"val\": 0.15, \"test\": 0.15},\n",
    "    },\n",
    "    \"models_evaluated\": len(all_results),\n",
    "    \"best_model\": {\n",
    "        \"name\": df.iloc[0][\"model_name\"],\n",
    "        \"type\": df.iloc[0][\"model_type\"],\n",
    "        \"accuracy\": float(df.iloc[0][\"accuracy\"]),\n",
    "        \"f1\": float(df.iloc[0][\"f1\"]),\n",
    "    },\n",
    "    \"llm_results\": {\n",
    "        \"count\": len(llm_df),\n",
    "        \"best_accuracy\": float(llm_df[\"accuracy\"].max()) if len(llm_df) > 0 else 0,\n",
    "        \"mean_accuracy\": float(llm_df[\"accuracy\"].mean()) if len(llm_df) > 0 else 0,\n",
    "    },\n",
    "    \"slm_results\": {\n",
    "        \"count\": len(slm_df),\n",
    "        \"best_accuracy\": float(slm_df[\"accuracy\"].max()) if len(slm_df) > 0 else 0,\n",
    "        \"mean_accuracy\": float(slm_df[\"accuracy\"].mean()) if len(slm_df) > 0 else 0,\n",
    "    },\n",
    "    \"generated_files\": [\n",
    "        str(paths.reports_dir / \"evaluation_report.html\"),\n",
    "        str(paths.plots_dir / \"main_comparison.png\"),\n",
    "        str(paths.plots_dir / \"efficiency_analysis.png\"),\n",
    "    ],\n",
    "    \"generated_at\": datetime.now().isoformat(),\n",
    "}\n",
    "\n",
    "# Speichern\n",
    "summary_path = paths.reports_dir / \"final_summary.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(final_summary, f, indent=2)\n",
    "\n",
    "print(f\"Finale Zusammenfassung gespeichert: {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d792c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINALE AUSGABE\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(#FINALE ZUSAMMENFASSUNG: Medical Diagnosis Finetuning\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\"\"\n",
    "üéØ Forschungsfrage:\n",
    "   K√∂nnen spezialisierte 3B SLMs durch Finetuning 7-8B LLMs √ºbertreffen?\n",
    "\n",
    "üìä Evaluierte Modelle: {len(all_results)}\n",
    "   LLM (Zero-Shot): {len(llm_df)}\n",
    "   SLM (Finetuned): {len(slm_df)}\n",
    "\n",
    "Rangliste:\n",
    "\"\"\")\n",
    "\n",
    "for i, (idx, row) in enumerate(df.iterrows()):\n",
    "    medal = \"ü•á\" if i == 0 else \"ü•à\" if i == 1 else \"ü•â\" if i == 2 else \"  \"\n",
    "    typ = \"üî¥\" if row[\"model_type\"] == \"LLM\" else \"üü¢\"\n",
    "    print(f\"   {medal} {typ} {row['model_name']}: {row['accuracy']:.2%} (F1: {row['f1']:.4f})\")\n",
    "\n",
    "print(f\"\"\"\n",
    "üìà Schl√ºsselerkenntnisse:\n",
    "\"\"\")\n",
    "\n",
    "if len(llm_df) > 0 and len(slm_df) > 0:\n",
    "    llm_best = llm_df[\"accuracy\"].max()\n",
    "    slm_best = slm_df[\"accuracy\"].max()\n",
    "    \n",
    "    if slm_best > llm_best:\n",
    "        improvement = (slm_best - llm_best) / llm_best * 100\n",
    "        print(f\"   Finetuned SLM √ºbertrifft LLM um {improvement:.1f}%\")\n",
    "        print(f\"   ‚Üí Spezialisierung schl√§gt Gr√∂√üe bei dom√§nenspezifischen Aufgaben\")\n",
    "        print(f\"   ‚Üí 3B Modell mit LoRA besser als 8B Zero-Shot\")\n",
    "    else:\n",
    "        print(f\"   LLM Baseline weiterhin f√ºhrend\")\n",
    "        print(f\"   ‚Üí Mehr Training oder bessere Daten erforderlich\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Generierte Dateien:\n",
    "   - {paths.reports_dir / 'evaluation_report.html'}\n",
    "   - {paths.reports_dir / 'final_summary.json'}\n",
    "   - {paths.plots_dir / 'main_comparison.png'}\n",
    "   - {paths.plots_dir / 'efficiency_analysis.png'}\n",
    "\n",
    "Pipeline abgeschlossen!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb65e0f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Pipeline abgeschlossen!\n",
    "\n",
    "## Zusammenfassung der Notebooks:\n",
    "\n",
    "| Notebook | Inhalt |\n",
    "|----------|--------|\n",
    "| **00** | Projekt√ºbersicht und Konfiguration |\n",
    "| **01** | Datenladung und Exploration |\n",
    "| **02** | Datenverarbeitung und Tokenisierung |\n",
    "| **03** | LLM Evaluation (Zero-Shot Baseline) |\n",
    "| **04** | SLM Training mit LoRA |\n",
    "| **05** | SLM Evaluation (Finetuned) |\n",
    "| **06** | Ergebnisanalyse und Reporting |\n",
    "\n",
    "## N√§chste Schritte:\n",
    "\n",
    "1. **Hyperparameter-Tuning**: LoRA r, alpha, learning rate optimieren\n",
    "2. **Mehr Daten**: Dataset erweitern oder augmentieren\n",
    "3. **Andere Modelle**: Weitere SLMs testen (Phi-3, Gemma, etc.)\n",
    "4. **Deployment**: Bestes Modell in Produktion bringen\n",
    "\n",
    "---\n",
    "\n",
    "**Autor**: Medical Diagnosis Finetuning Pipeline\n",
    "**Datum**: {datetime.now().strftime('%Y-%m-%d')}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
