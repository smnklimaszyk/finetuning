{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 - Custom Interview Testing\n",
    "\n",
    "**Previous:** [06_Results_Analysis_and_Comparison.ipynb](06_Results_Analysis_and_Comparison.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Covers\n",
    "\n",
    "The **interactive playground** - test your models on custom medical conversations!\n",
    "\n",
    "**Key Features:**\n",
    "1. Load custom doctor-patient conversations from JSON\n",
    "2. Test multiple models side-by-side\n",
    "3. Compare predictions with ground truth\n",
    "4. Create your own test cases\n",
    "5. Analyze model behavior on edge cases\n",
    "\n",
    "**Use Cases:**\n",
    "- Validate models on real-world conversations\n",
    "- Test edge cases and rare diagnoses\n",
    "- Demonstrate to stakeholders\n",
    "- Quality assurance before deployment\n",
    "\n",
    "**Custom Interview Format:**\n",
    "```json\n",
    "{\n",
    "  \"name\": \"Case 001 - Upper Respiratory Infection\",\n",
    "  \"expected_icd10\": \"J06.9\",\n",
    "  \"description\": \"Patient with cold symptoms\",\n",
    "  \"messages\": [\n",
    "    {\"role\": \"doctor\", \"message\": \"What brings you here today?\"},\n",
    "    {\"role\": \"patient\", \"message\": \"I have a sore throat and fever.\"}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Critical for GPU memory management\n",
    "os.environ['PYTORCH_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "print(f\"‚úÖ Project Root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "from IPython.display import display, HTML\n",
    "import gc\n",
    "\n",
    "print(\"‚úÖ All libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ CUDA Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  CUDA not available - using CPU (slower)\")\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Loading Custom Interview Files\n",
    "\n",
    "Custom interviews should be placed in `custom_interviews/` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom interviews directory\n",
    "interviews_dir = project_root / \"custom_interviews\"\n",
    "\n",
    "print(f\"Looking for custom interviews in: {interviews_dir}\\n\")\n",
    "\n",
    "# Find all JSON files\n",
    "interview_files = list(interviews_dir.glob(\"*.json\"))\n",
    "\n",
    "if interview_files:\n",
    "    print(f\"‚úÖ Found {len(interview_files)} interview file(s):\\n\")\n",
    "    for file in interview_files:\n",
    "        print(f\"   ‚Ä¢ {file.name}\")\n",
    "else:\n",
    "    print(\"‚ùå No interview files found!\")\n",
    "    print(\"\\nCreate JSON files in custom_interviews/ with format:\")\n",
    "    print('''\n",
    "{\n",
    "  \"name\": \"Case Name\",\n",
    "  \"expected_icd10\": \"J06.9\",\n",
    "  \"description\": \"Short description\",\n",
    "  \"messages\": [\n",
    "    {\"role\": \"doctor\", \"message\": \"...\"},\n",
    "    {\"role\": \"patient\", \"message\": \"...\"}\n",
    "  ]\n",
    "}\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Interview Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_interview(file_path: Path) -> Dict:\n",
    "    \"\"\"\n",
    "    Load a custom interview from JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        interview = json.load(f)\n",
    "    \n",
    "    # Validate format\n",
    "    required_fields = ['name', 'expected_icd10', 'messages']\n",
    "    for field in required_fields:\n",
    "        if field not in interview:\n",
    "            raise ValueError(f\"Missing required field: {field}\")\n",
    "    \n",
    "    return interview\n",
    "\n",
    "# Load all interviews\n",
    "interviews = []\n",
    "for file in interview_files:\n",
    "    try:\n",
    "        interview = load_interview(file)\n",
    "        interviews.append(interview)\n",
    "        print(f\"‚úÖ Loaded: {interview['name']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {file.name}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal interviews loaded: {len(interviews)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Interview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_interview(interview: Dict):\n",
    "    \"\"\"\n",
    "    Pretty print an interview.\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(f\"üìã {interview['name']}\")\n",
    "    print(\"=\"*70)\n",
    "    if 'description' in interview:\n",
    "        print(f\"Description: {interview['description']}\")\n",
    "    print(f\"Expected ICD-10: {interview['expected_icd10']}\")\n",
    "    print(f\"\\nConversation ({len(interview['messages'])} messages):\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for i, msg in enumerate(interview['messages'], 1):\n",
    "        role = msg['role'].upper()\n",
    "        content = msg['message'] if 'message' in msg else msg['content']\n",
    "        print(f\"\\n[{i}] {role}:\")\n",
    "        print(f\"    {content}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Show first interview as example\n",
    "if interviews:\n",
    "    display_interview(interviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Model Loading Utilities\n",
    "\n",
    "Helper functions to load and test models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_diagnosis(interview: Dict, model, tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Predict ICD-10 code from interview.\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        \"You are a medical diagnosis assistant. \"\n",
    "        \"Based on the doctor-patient conversation, predict the ICD-10 diagnosis code. \"\n",
    "        \"Respond with ONLY the code (e.g., 'J06.9'), nothing else.\"\n",
    "    )\n",
    "    \n",
    "    # Format conversation\n",
    "    conversation_text = \"\\n\".join([\n",
    "        f\"{msg['role'].capitalize()}: {msg.get('message', msg.get('content'))}\"\n",
    "        for msg in interview['messages']\n",
    "    ])\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": conversation_text}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize and generate\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=10,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode\n",
    "    generated = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    prediction = tokenizer.decode(generated, skip_special_tokens=True).strip()\n",
    "    \n",
    "    # Extract code (first word)\n",
    "    prediction = prediction.split()[0] if prediction.split() else \"\"\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "print(\"‚úÖ Prediction utilities ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Testing Finetuned Model\n",
    "\n",
    "Load and test the finetuned Llama 3.2 3B model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load finetuned model\n",
    "print(\"Loading finetuned Llama 3.2 3B model...\\n\")\n",
    "\n",
    "base_model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "adapter_path = project_root / \"models\" / \"llama-3.2-3b-medical-lora\" / \"final_model\"\n",
    "\n",
    "if adapter_path.exists():\n",
    "    # Load base model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    # Load LoRA adapters\n",
    "    finetuned_model = PeftModel.from_pretrained(base_model, str(adapter_path))\n",
    "    finetuned_model.eval()\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(\"‚úÖ Finetuned model loaded successfully\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        print(f\"   GPU Memory: {allocated:.2f} GB\")\n",
    "else:\n",
    "    print(f\"‚ùå Adapter not found at {adapter_path}\")\n",
    "    print(\"   Run notebook 04 to train the model first.\")\n",
    "    finetuned_model = None\n",
    "    tokenizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Custom Interviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if finetuned_model and interviews:\n",
    "    print(\"\\nTesting finetuned model on custom interviews...\\n\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for interview in interviews:\n",
    "        print(f\"\\nüìã {interview['name']}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = predict_diagnosis(interview, finetuned_model, tokenizer)\n",
    "        expected = interview['expected_icd10']\n",
    "        \n",
    "        # Check if correct\n",
    "        correct = prediction == expected\n",
    "        \n",
    "        # Store result\n",
    "        results.append({\n",
    "            'interview': interview['name'],\n",
    "            'expected': expected,\n",
    "            'predicted': prediction,\n",
    "            'correct': correct\n",
    "        })\n",
    "        \n",
    "        # Display result\n",
    "        print(f\"Expected:   {expected}\")\n",
    "        print(f\"Predicted:  {prediction}\")\n",
    "        print(f\"Result:     {'‚úÖ CORRECT' if correct else '‚ùå INCORRECT'}\")\n",
    "        \n",
    "        # Show conversation snippet\n",
    "        print(f\"\\nConversation snippet:\")\n",
    "        for msg in interview['messages'][:2]:\n",
    "            content = msg.get('message', msg.get('content', ''))\n",
    "            print(f\"  {msg['role']:8s}: {content[:60]}...\" if len(content) > 60 else f\"  {msg['role']:8s}: {content}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Summary\n",
    "    correct_count = sum(r['correct'] for r in results)\n",
    "    total_count = len(results)\n",
    "    accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìä SUMMARY:\")\n",
    "    print(f\"   Total interviews: {total_count}\")\n",
    "    print(f\"   Correct:          {correct_count}\")\n",
    "    print(f\"   Accuracy:         {accuracy:.1%}\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    if not finetuned_model:\n",
    "        print(\"‚ö†Ô∏è  Finetuned model not loaded\")\n",
    "    if not interviews:\n",
    "        print(\"‚ö†Ô∏è  No interviews to test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Comparing Multiple Models\n",
    "\n",
    "Test both finetuned and zero-shot models side-by-side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_on_interview(interview: Dict, models_dict: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Test multiple models on one interview.\n",
    "    \n",
    "    Args:\n",
    "        interview: Interview dict\n",
    "        models_dict: {name: (model, tokenizer)}\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with results\n",
    "    \"\"\"\n",
    "    expected = interview['expected_icd10']\n",
    "    results = []\n",
    "    \n",
    "    for model_name, (model, tokenizer) in models_dict.items():\n",
    "        prediction = predict_diagnosis(interview, model, tokenizer)\n",
    "        correct = prediction == expected\n",
    "        \n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Prediction': prediction,\n",
    "            'Match': '‚úÖ' if correct else '‚ùå'\n",
    "        })\n",
    "    \n",
    "    # Add expected\n",
    "    results.append({\n",
    "        'Model': 'Ground Truth',\n",
    "        'Prediction': expected,\n",
    "        'Match': '‚Äî'\n",
    "    })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# For demonstration, we only have finetuned model loaded\n",
    "# In practice, you could load multiple models\n",
    "\n",
    "if finetuned_model and interviews:\n",
    "    print(\"\\nDetailed Comparison (first interview):\\n\")\n",
    "    \n",
    "    models_to_test = {\n",
    "        'Llama 3.2 3B (Finetuned)': (finetuned_model, tokenizer)\n",
    "    }\n",
    "    \n",
    "    # Test first interview\n",
    "    interview = interviews[0]\n",
    "    display_interview(interview)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL PREDICTIONS:\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    comparison_df = compare_models_on_interview(interview, models_to_test)\n",
    "    display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Interactive Testing\n",
    "\n",
    "Create your own interview on the fly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom interview programmatically\n",
    "custom_interview = {\n",
    "    \"name\": \"Custom Test Case - Hypertension\",\n",
    "    \"expected_icd10\": \"I10\",\n",
    "    \"description\": \"Patient with high blood pressure\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"doctor\",\n",
    "            \"message\": \"What brings you to the clinic today?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"patient\",\n",
    "            \"message\": \"I've been experiencing headaches and my blood pressure has been high.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"doctor\",\n",
    "            \"message\": \"How high is your blood pressure?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"patient\",\n",
    "            \"message\": \"It's been around 150/95 for the past week.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"doctor\",\n",
    "            \"message\": \"Do you have any family history of hypertension?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"patient\",\n",
    "            \"message\": \"Yes, both my parents have high blood pressure.\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Test it\n",
    "if finetuned_model:\n",
    "    print(\"\\nTesting custom interview:\\n\")\n",
    "    display_interview(custom_interview)\n",
    "    \n",
    "    prediction = predict_diagnosis(custom_interview, finetuned_model, tokenizer)\n",
    "    expected = custom_interview['expected_icd10']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Expected:  {expected}\")\n",
    "    print(f\"Predicted: {prediction}\")\n",
    "    print(f\"Result:    {'‚úÖ CORRECT' if prediction == expected else '‚ùå INCORRECT'}\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Batch Testing on All Interviews\n",
    "\n",
    "Generate a comprehensive report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if finetuned_model and interviews:\n",
    "    # Test all interviews\n",
    "    all_results = []\n",
    "    \n",
    "    for interview in interviews:\n",
    "        prediction = predict_diagnosis(interview, finetuned_model, tokenizer)\n",
    "        expected = interview['expected_icd10']\n",
    "        \n",
    "        all_results.append({\n",
    "            'Interview': interview['name'],\n",
    "            'Expected': expected,\n",
    "            'Predicted': prediction,\n",
    "            'Correct': prediction == expected\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df['Status'] = results_df['Correct'].apply(lambda x: '‚úÖ' if x else '‚ùå')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" \"*25 + \"COMPREHENSIVE TEST REPORT\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    display(results_df[['Interview', 'Expected', 'Predicted', 'Status']])\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY STATISTICS:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    total = len(results_df)\n",
    "    correct = results_df['Correct'].sum()\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    print(f\"\\nTotal Interviews Tested:  {total}\")\n",
    "    print(f\"Correct Predictions:      {correct}\")\n",
    "    print(f\"Incorrect Predictions:    {total - correct}\")\n",
    "    print(f\"Accuracy:                 {accuracy:.1%}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Save results\n",
    "    output_path = project_root / \"outputs\" / \"custom_interview_results.csv\"\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n‚úÖ Results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Creating New Test Cases\n",
    "\n",
    "Template for creating your own interview JSON files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template generator\n",
    "def create_interview_template(name: str, icd10: str, description: str = \"\") -> Dict:\n",
    "    \"\"\"\n",
    "    Create a template for a new interview.\n",
    "    \"\"\"\n",
    "    template = {\n",
    "        \"name\": name,\n",
    "        \"expected_icd10\": icd10,\n",
    "        \"description\": description,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"doctor\",\n",
    "                \"message\": \"What brings you here today?\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"patient\",\n",
    "                \"message\": \"[Patient describes symptoms]\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"doctor\",\n",
    "                \"message\": \"[Doctor asks follow-up questions]\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"patient\",\n",
    "                \"message\": \"[Patient provides more details]\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    return template\n",
    "\n",
    "def save_interview(interview: Dict, filename: str):\n",
    "    \"\"\"\n",
    "    Save interview to JSON file.\n",
    "    \"\"\"\n",
    "    filepath = interviews_dir / filename\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(interview, f, indent=2)\n",
    "    print(f\"‚úÖ Saved interview to: {filepath}\")\n",
    "\n",
    "# Example: Create a new template\n",
    "print(\"\\nExample Interview Template:\\n\")\n",
    "new_template = create_interview_template(\n",
    "    name=\"Case XXX - Condition Name\",\n",
    "    icd10=\"XXX.X\",\n",
    "    description=\"Brief description of the case\"\n",
    ")\n",
    "\n",
    "print(json.dumps(new_template, indent=2))\n",
    "\n",
    "print(\"\\nüí° To create your own interview:\")\n",
    "print(\"   1. Copy the template above\")\n",
    "print(\"   2. Fill in the conversation\")\n",
    "print(\"   3. Save as .json in custom_interviews/\")\n",
    "print(\"   4. Re-run this notebook to test!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory\n",
    "if finetuned_model:\n",
    "    del finetuned_model\n",
    "    del base_model\n",
    "    del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "print(\"‚úÖ Memory freed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Key Takeaways üí°\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Custom Testing is Essential**\n",
    "   - Standard metrics don't capture everything\n",
    "   - Real-world cases reveal edge cases\n",
    "   - Stakeholder demonstrations need concrete examples\n",
    "\n",
    "2. **JSON Format is Flexible**\n",
    "   - Easy to create test cases\n",
    "   - Human-readable and version-controllable\n",
    "   - Can be generated programmatically\n",
    "\n",
    "3. **Model Comparison**\n",
    "   - Side-by-side comparison is powerful\n",
    "   - Shows concrete differences between models\n",
    "   - Helps identify model strengths/weaknesses\n",
    "\n",
    "4. **Interactive Testing**\n",
    "   - Immediate feedback loop\n",
    "   - Can test hypotheses quickly\n",
    "   - Useful for model debugging\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "‚úÖ **Do:**\n",
    "- Create diverse test cases (common + rare conditions)\n",
    "- Include edge cases and ambiguous scenarios\n",
    "- Document why each test case matters\n",
    "- Version control your test cases\n",
    "- Regularly expand your test suite\n",
    "\n",
    "‚ùå **Don't:**\n",
    "- Only test happy paths\n",
    "- Ignore model failures (they're learning opportunities!)\n",
    "- Use production data without proper anonymization\n",
    "- Skip validation of expected ICD codes\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Expand Test Suite**\n",
    "   - Add more diverse medical conditions\n",
    "   - Include multi-diagnosis cases\n",
    "   - Test rare diseases\n",
    "\n",
    "2. **Automate Testing**\n",
    "   - Integrate with CI/CD pipeline\n",
    "   - Set accuracy thresholds\n",
    "   - Monitor regression\n",
    "\n",
    "3. **Gather Real Data**\n",
    "   - Collaborate with medical professionals\n",
    "   - Collect anonymized real conversations\n",
    "   - Validate on clinical data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "- ‚úÖ Loaded custom medical interviews from JSON\n",
    "- ‚úÖ Tested finetuned models on custom cases\n",
    "- ‚úÖ Compared predictions with ground truth\n",
    "- ‚úÖ Created interactive testing framework\n",
    "- ‚úÖ Generated comprehensive test reports\n",
    "- ‚úÖ Learned how to create new test cases\n",
    "\n",
    "**You can now:**\n",
    "- Test models on your own medical conversations\n",
    "- Validate model performance on specific cases\n",
    "- Demonstrate model capabilities to stakeholders\n",
    "- Build confidence before deployment\n",
    "\n",
    "**Related Files:**\n",
    "- `custom_interviews/` - Your test cases directory\n",
    "- `test_custom_interviews.py` - Command-line testing script\n",
    "- `CUSTOM_INTERVIEW_TESTING.md` - Detailed documentation\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've completed the entire notebook series! You now understand:\n",
    "\n",
    "1. ‚úÖ The complete ML pipeline (data ‚Üí training ‚Üí evaluation)\n",
    "2. ‚úÖ How LoRA finetuning works\n",
    "3. ‚úÖ Size vs specialization trade-offs\n",
    "4. ‚úÖ How to evaluate and compare models\n",
    "5. ‚úÖ How to test on custom cases\n",
    "\n",
    "**Ready for production?** Check out the main project files:\n",
    "- `main.py` - Automated pipeline\n",
    "- `GUIDE.md` - Complete project guide\n",
    "- `QUICKSTART.md` - Fast deployment guide\n",
    "\n",
    "üöÄ **Happy model building!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
