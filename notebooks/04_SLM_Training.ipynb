{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d417204",
   "metadata": {},
   "source": [
    "# Notebook 04: SLM Training mit LoRA\n",
    "\n",
    "## Ziel dieses Notebooks\n",
    "\n",
    "In diesem Notebook werden wir:\n",
    "\n",
    "1. **Kleinere SLMs laden** - 3B Parameter Modelle\n",
    "2. **LoRA-Finetuning** - Parameter-effizientes Training\n",
    "3. **Training durchfÃ¼hren** - Mit validierten Hyperparametern\n",
    "4. **Modelle speichern** - Finetuned Versionen fÃ¼r Evaluation\n",
    "\n",
    "---\n",
    "\n",
    "## Theoretischer Hintergrund\n",
    "\n",
    "### Was ist LoRA?\n",
    "\n",
    "**Low-Rank Adaptation (LoRA)** ist eine Methode fÃ¼r parameter-effizientes Finetuning:\n",
    "\n",
    "- **Idee**: Statt alle Gewichte zu trainieren, nur niedrigrangige Updates\n",
    "- **Vorteil**: ~0.1% der Parameter werden trainiert\n",
    "- **Formel**: $W' = W + BA$ wobei $B \\in \\mathbb{R}^{d \\times r}$, $A \\in \\mathbb{R}^{r \\times k}$\n",
    "\n",
    "### LoRA-Hyperparameter\n",
    "\n",
    "| Parameter | Wert | Beschreibung |\n",
    "|-----------|------|--------------|\n",
    "| `r` | 64 | Rang der Low-Rank Matrices |\n",
    "| `alpha` | 128 | Skalierungsfaktor |\n",
    "| `dropout` | 0.1 | Regularisierung |\n",
    "| `target_modules` | QKV + FFN | Welche Layer adaptiert werden |\n",
    "\n",
    "### Unsere SLM-Modelle\n",
    "\n",
    "| Modell | GrÃ¶ÃŸe | Beschreibung |\n",
    "|--------|-------|--------------|\n",
    "| Llama-3.2-3B-Instruct | 3B | Kleinste Llama 3.2 Version |\n",
    "| Qwen2.5-3B-Instruct | 3B | Qwen 2.5 Instruct-Version |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcc319a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports erfolgreich!\n",
      "   PyTorch: 2.10.0+cu128\n",
      "   CUDA verfÃ¼gbar: True\n",
      "   GPU: NVIDIA GeForce RTX 5090\n",
      "   VRAM: 33.6 GB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SETUP: Imports und Umgebung\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict, load_from_disk\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    TaskType,\n",
    ")\n",
    "\n",
    "print(\"Imports erfolgreich!\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")\n",
    "print(f\"   CUDA verfÃ¼gbar: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c73d59",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# KONFIGURATION (Standalone)\n# ============================================================\n\n@dataclass\nclass PathConfig:\n    project_root: Path = field(default_factory=lambda: Path.cwd().parent)\n    data_dir: Path = field(default_factory=lambda: Path.cwd().parent / \"data\")\n    cache_dir: Path = field(default_factory=lambda: Path.cwd().parent / \"data\" / \"cache\")\n    processed_data_dir: Path = field(default_factory=lambda: Path.cwd().parent / \"data\" / \"processed\")\n    models_dir: Path = field(default_factory=lambda: Path.cwd().parent / \"models\")\n    finetuned_models_dir: Path = field(default_factory=lambda: Path.cwd().parent / \"models\" / \"finetuned\")\n    checkpoints_dir: Path = field(default_factory=lambda: Path.cwd().parent / \"models\" / \"checkpoints\")\n    outputs_dir: Path = field(default_factory=lambda: Path.cwd().parent / \"outputs\")\n    logs_dir: Path = field(default_factory=lambda: Path.cwd().parent / \"outputs\" / \"logs\")\n    \n    def create_directories(self):\n        for attr_name in dir(self):\n            attr = getattr(self, attr_name)\n            if isinstance(attr, Path) and not attr_name.startswith('_'):\n                attr.mkdir(parents=True, exist_ok=True)\n\n@dataclass\nclass DataConfig:\n    dataset_name: str = \"Ahmad0067/MedSynth\"\n    dataset_split_seed: int = 42\n    train_ratio: float = 0.80\n    val_ratio: float = 0.1\n    test_ratio: float = 0.1\n    # WICHTIG: MedSynth Dialoge haben durchschnittlich ~1167 Tokens\n    # 512 wÃ¼rde >99% der Dialoge abschneiden und wichtige Informationen verlieren!\n    max_sequence_length: int = 2048  # ErhÃ¶ht von 512\n\n@dataclass  \nclass ModelConfig:\n    \"\"\"SLM Modelle fÃ¼r Finetuning.\"\"\"\n    slm_models: List[Dict] = field(default_factory=lambda: [\n        {\n            \"name\": \"meta-llama/Llama-3.2-3B-Instruct\",\n            \"size\": \"3B\",\n            \"description\": \"3B - Small Llama model\",\n        },\n        {\n            \"name\": \"Qwen/Qwen2.5-3B-Instruct\",\n            \"size\": \"3B\",\n            \"description\": \"3B - Qwen 2.5 model\",\n        },\n    ])\n\n@dataclass\nclass LoRAConfig:\n    \"\"\"LoRA-Hyperparameter.\"\"\"\n    r: int = 64                    # Rang der Low-Rank Matrices\n    lora_alpha: int = 128          # Skalierungsfaktor\n    lora_dropout: float = 0.1      # Dropout\n    target_modules: List[str] = field(default_factory=lambda: [\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention\n        \"gate_proj\", \"up_proj\", \"down_proj\",      # FFN\n    ])\n    bias: str = \"none\"\n    task_type: str = \"CAUSAL_LM\"\n\n@dataclass\nclass TrainingConfig:\n    \"\"\"Training-Hyperparameter.\"\"\"\n    num_train_epochs: int = 3\n    per_device_train_batch_size: int = 8   # Reduziert wegen lÃ¤ngerer Sequenzen\n    per_device_eval_batch_size: int = 8    # Reduziert wegen lÃ¤ngerer Sequenzen\n    gradient_accumulation_steps: int = 4    # ErhÃ¶ht fÃ¼r gleiche effektive Batch-Size\n    learning_rate: float = 2e-4\n    warmup_ratio: float = 0.03\n    weight_decay: float = 0.01\n    lr_scheduler_type: str = \"cosine\"\n    bf16: bool = True\n    fp16: bool = False\n    gradient_checkpointing: bool = True\n    optim: str = \"adamw_torch_fused\"\n    logging_steps: int = 10\n    eval_steps: int = 100\n    save_steps: int = 100\n    save_total_limit: int = 2\n    load_best_model_at_end: bool = True\n    metric_for_best_model: str = \"eval_loss\"\n    greater_is_better: bool = False\n\n@dataclass\nclass Config:\n    paths: PathConfig = field(default_factory=PathConfig)\n    data: DataConfig = field(default_factory=DataConfig)\n    model: ModelConfig = field(default_factory=ModelConfig)\n    lora: LoRAConfig = field(default_factory=LoRAConfig)\n    training: TrainingConfig = field(default_factory=TrainingConfig)\n    \n    def setup(self):\n        self.paths.create_directories()\n        if torch.cuda.is_available():\n            torch.backends.cuda.matmul.allow_tf32 = True\n            torch.backends.cudnn.allow_tf32 = True\n\nconfig = Config()\nconfig.setup()\n\nprint(\"Konfiguration geladen!\")\nprint(f\"   SLM Modelle: {len(config.model.slm_models)}\")\nfor m in config.model.slm_models:\n    print(f\"      - {m['name']} ({m['size']})\")\nprint(f\"\\n   LoRA: r={config.lora.r}, alpha={config.lora.lora_alpha}\")\nprint(f\"   Epochs: {config.training.num_train_epochs}\")\nprint(f\"   Max Sequence Length: {config.data.max_sequence_length}\")\nprint(f\"   Effective Batch Size: {config.training.per_device_train_batch_size * config.training.gradient_accumulation_steps}\")"
  },
  {
   "cell_type": "markdown",
   "id": "571b9d3c",
   "metadata": {},
   "source": [
    "## LoRA-Konfiguration\n",
    "\n",
    "### LoRA Hyperparameter\n",
    "\n",
    "| Parameter | Wert | Bedeutung |\n",
    "|-----------|------|-----------|\n",
    "| **r (Rank)** | 16-64 | KapazitÃ¤t der Adapter. HÃ¶her = mehr ExpressivitÃ¤t, aber mehr Parameter |\n",
    "| **alpha** | 2Ã—r | Scaling-Faktor. Kontrolliert wie stark LoRA das Modell beeinflusst |\n",
    "| **dropout** | 0.05-0.1 | Regularisierung. Verhindert Overfitting |\n",
    "| **target_modules** | [\"q_proj\", \"v_proj\", ...] | Welche Layer modifiziert werden |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccc371f",
   "metadata": {},
   "source": [
    "## 1. Utility-Funktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fce820a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Initial - GPU: 0.00GB alloc, 0.00GB reserved\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# UTILITY-FUNKTIONEN (Standalone)\n",
    "# ============================================================\n",
    "\n",
    "def get_device() -> str:\n",
    "    \"\"\"Bestimmt das beste verfÃ¼gbare Device.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "def aggressive_memory_cleanup():\n",
    "    \"\"\"Aggressive Memory-Cleanup.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "def log_gpu_memory(prefix: str = \"\"):\n",
    "    \"\"\"Loggt GPU Memory Status.\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    msg = f\"GPU: {allocated:.2f}GB alloc, {reserved:.2f}GB reserved\"\n",
    "    if prefix:\n",
    "        msg = f\"{prefix} - {msg}\"\n",
    "    print(msg)\n",
    "\n",
    "print(f\"Device: {get_device()}\")\n",
    "log_gpu_memory(\"Initial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1183b1c",
   "metadata": {},
   "source": [
    "## 2. Data Processor fÃ¼r Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb022d58",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# MEDICAL DIALOG PROCESSOR (Standalone)\n# ============================================================\n\nclass MedicalDialogProcessor:\n    \"\"\"Formatiert Dialoge fÃ¼r Training und Inference.\"\"\"\n    \n    SYSTEM_PROMPT = \"\"\"You are a medical assistance system that supports doctors in making diagnoses. \nYour task is to suggest the appropriate ICD-10 diagnosis code based on a doctor-patient dialogue.\nRespond only with the ICD-10 code, without further explanation.\"\"\"\n    \n    def __init__(self, tokenizer, max_length: int = 2048):\n        \"\"\"\n        Args:\n            tokenizer: HuggingFace Tokenizer\n            max_length: Maximale SequenzlÃ¤nge (Default: 2048 fÃ¼r MedSynth)\n        \"\"\"\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def format_dialog_for_training(self, conversation: str, icd_code: str) -> str:\n        \"\"\"Formatiert Dialog fÃ¼r Training (mit Antwort).\"\"\"\n        user_prompt = f\"\"\"Analyze the following doctor-patient dialogue and determine the appropriate ICD-10 code:\n\n{conversation}\n\nICD-10 Code:\"\"\"\n        \n        if hasattr(self.tokenizer, 'chat_template') and self.tokenizer.chat_template:\n            messages = [\n                {\"role\": \"system\", \"content\": self.SYSTEM_PROMPT},\n                {\"role\": \"user\", \"content\": user_prompt},\n                {\"role\": \"assistant\", \"content\": icd_code},\n            ]\n            try:\n                return self.tokenizer.apply_chat_template(\n                    messages, tokenize=False, add_generation_prompt=False\n                )\n            except:\n                pass\n        \n        return f\"System: {self.SYSTEM_PROMPT}\\n\\nUser: {user_prompt}\\n\\nAssistant: {icd_code}\"\n    \n    def tokenize_function(self, examples: Dict) -> Dict:\n        \"\"\"Tokenisiert Beispiele.\"\"\"\n        dialog_field = 'Dialogue' if 'Dialogue' in examples else 'dialogue'\n        icd_field = 'ICD10' if 'ICD10' in examples else 'icd10'\n        \n        if isinstance(examples[dialog_field], list):\n            texts = [\n                self.format_dialog_for_training(conv, icd)\n                for conv, icd in zip(examples[dialog_field], examples[icd_field])\n            ]\n        else:\n            texts = [self.format_dialog_for_training(\n                examples[dialog_field], examples[icd_field]\n            )]\n        \n        tokenized = self.tokenizer(\n            texts,\n            truncation=True,\n            max_length=self.max_length,\n            padding=False,  # Dynamisches Padding ist effizienter\n            return_tensors=None,\n        )\n        \n        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n        \n        return tokenized\n    \n    def process_dataset(self, dataset: Dataset) -> Dataset:\n        \"\"\"Verarbeitet gesamten Dataset.\"\"\"\n        return dataset.map(\n            self.tokenize_function,\n            batched=True,\n            remove_columns=dataset.column_names,\n            desc=\"Tokenisiere Dataset\",\n        )\n\nprint(\"MedicalDialogProcessor definiert!\")"
  },
  {
   "cell_type": "markdown",
   "id": "366f02d7",
   "metadata": {},
   "source": [
    "## 3. FineTuner-Klasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64540e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuner-Klasse definiert!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FINETUNER-KLASSE (Standalone)\n",
    "# ============================================================\n",
    "\n",
    "class FineTuner:\n",
    "    \"\"\"\n",
    "    LoRA FineTuner fÃ¼r SLMs.\n",
    "    \n",
    "    Features:\n",
    "    - Parameter-effizientes Training\n",
    "    - Gradient Checkpointing\n",
    "    - BF16/FP16 Training\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        config: Config,\n",
    "        output_dir: Optional[Path] = None,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.config = config\n",
    "        self.model_short = model_name.split(\"/\")[-1]\n",
    "        \n",
    "        if output_dir is None:\n",
    "            output_dir = config.paths.finetuned_models_dir / self.model_short\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.processor = None\n",
    "        self.trainer = None\n",
    "    \n",
    "    def load_base_model(self):\n",
    "        \"\"\"LÃ¤dt Base-Modell und Tokenizer.\"\"\"\n",
    "        print(f\"ðŸ“¥ Lade Base-Modell: {self.model_name}\")\n",
    "        \n",
    "        # Tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"right\"  # FÃ¼r Training\n",
    "        \n",
    "        # Modell\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        \n",
    "        # Gradient Checkpointing\n",
    "        if self.config.training.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "            if hasattr(self.model, 'enable_input_require_grads'):\n",
    "                self.model.enable_input_require_grads()\n",
    "        \n",
    "        print(\"Base-Modell geladen!\")\n",
    "        log_gpu_memory(\"Nach Laden\")\n",
    "    \n",
    "    def apply_lora(self):\n",
    "        \"\"\"Wendet LoRA auf Modell an.\"\"\"\n",
    "        print(f\"ðŸ”§ Wende LoRA an (r={self.config.lora.r}, alpha={self.config.lora.lora_alpha})\")\n",
    "        \n",
    "        lora_config = LoraConfig(\n",
    "            r=self.config.lora.r,\n",
    "            lora_alpha=self.config.lora.lora_alpha,\n",
    "            lora_dropout=self.config.lora.lora_dropout,\n",
    "            target_modules=self.config.lora.target_modules,\n",
    "            bias=self.config.lora.bias,\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "        )\n",
    "        \n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        \n",
    "        # Trainable Parameter anzeigen\n",
    "        trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        total = sum(p.numel() for p in self.model.parameters())\n",
    "        \n",
    "        print(f\"LoRA angewendet!\")\n",
    "        print(f\"   Trainable: {trainable:,} ({100*trainable/total:.2f}%)\")\n",
    "        print(f\"   Total: {total:,}\")\n",
    "    \n",
    "    def prepare_data(self, train_dataset: Dataset, val_dataset: Dataset):\n",
    "        \"\"\"Bereitet Daten fÃ¼r Training vor.\"\"\"\n",
    "        print(\"Bereite Daten vor...\")\n",
    "        \n",
    "        self.processor = MedicalDialogProcessor(\n",
    "            self.tokenizer, \n",
    "            max_length=self.config.data.max_sequence_length\n",
    "        )\n",
    "        \n",
    "        self.train_data = self.processor.process_dataset(train_dataset)\n",
    "        self.val_data = self.processor.process_dataset(val_dataset)\n",
    "        \n",
    "        print(f\"Daten vorbereitet!\")\n",
    "        print(f\"   Train: {len(self.train_data):,} samples\")\n",
    "        print(f\"   Val: {len(self.val_data):,} samples\")\n",
    "    \n",
    "    def setup_trainer(self):\n",
    "        \"\"\"Erstellt Trainer.\"\"\"\n",
    "        print(\"Erstelle Trainer...\")\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=str(self.output_dir),\n",
    "            num_train_epochs=self.config.training.num_train_epochs,\n",
    "            per_device_train_batch_size=self.config.training.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=self.config.training.per_device_eval_batch_size,\n",
    "            gradient_accumulation_steps=self.config.training.gradient_accumulation_steps,\n",
    "            learning_rate=self.config.training.learning_rate,\n",
    "            warmup_ratio=self.config.training.warmup_ratio,\n",
    "            weight_decay=self.config.training.weight_decay,\n",
    "            lr_scheduler_type=self.config.training.lr_scheduler_type,\n",
    "            bf16=self.config.training.bf16,\n",
    "            fp16=self.config.training.fp16,\n",
    "            gradient_checkpointing=self.config.training.gradient_checkpointing,\n",
    "            optim=self.config.training.optim,\n",
    "            logging_steps=self.config.training.logging_steps,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=self.config.training.eval_steps,\n",
    "            save_steps=self.config.training.save_steps,\n",
    "            save_total_limit=self.config.training.save_total_limit,\n",
    "            load_best_model_at_end=self.config.training.load_best_model_at_end,\n",
    "            metric_for_best_model=self.config.training.metric_for_best_model,\n",
    "            greater_is_better=self.config.training.greater_is_better,\n",
    "            report_to=\"none\",  # Disable wandb\n",
    "            logging_dir=str(self.config.paths.logs_dir),\n",
    "            remove_unused_columns=False,\n",
    "        )\n",
    "        \n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False,\n",
    "        )\n",
    "        \n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=self.train_data,\n",
    "            eval_dataset=self.val_data,\n",
    "            data_collator=data_collator,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "        )\n",
    "        \n",
    "        print(\"Trainer erstellt!\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Startet Training.\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Starte Training: {self.model_short}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        train_result = self.trainer.train()\n",
    "        \n",
    "        print(f\"\\nTraining abgeschlossen!\")\n",
    "        print(f\"   Steps: {train_result.global_step}\")\n",
    "        print(f\"   Loss: {train_result.training_loss:.4f}\")\n",
    "        \n",
    "        return train_result\n",
    "    \n",
    "    def save_model(self):\n",
    "        \"\"\"Speichert finetuned Modell.\"\"\"\n",
    "        print(f\"Speichere Modell: {self.output_dir}\")\n",
    "        \n",
    "        # LoRA Adapter speichern\n",
    "        self.model.save_pretrained(self.output_dir)\n",
    "        self.tokenizer.save_pretrained(self.output_dir)\n",
    "        \n",
    "        # Merged Model speichern (fÃ¼r einfachere Inference)\n",
    "        merged_dir = self.output_dir / \"merged\"\n",
    "        merged_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Merge LoRA\n",
    "        merged_model = self.model.merge_and_unload()\n",
    "        merged_model.save_pretrained(merged_dir)\n",
    "        self.tokenizer.save_pretrained(merged_dir)\n",
    "        \n",
    "        print(f\"Modell gespeichert!\")\n",
    "        print(f\"   Adapter: {self.output_dir}\")\n",
    "        print(f\"   Merged: {merged_dir}\")\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Cleanup nach Training.\"\"\"\n",
    "        if self.model is not None:\n",
    "            del self.model\n",
    "            self.model = None\n",
    "        if self.trainer is not None:\n",
    "            del self.trainer\n",
    "            self.trainer = None\n",
    "        aggressive_memory_cleanup()\n",
    "        print(\"Cleanup abgeschlossen\")\n",
    "        log_gpu_memory(\"Nach Cleanup\")\n",
    "\n",
    "print(\"FineTuner-Klasse definiert!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee87157f",
   "metadata": {},
   "source": [
    "## Training vorbereiten\n",
    "\n",
    "| Parameter | Bedeutung | Unser Wert |\n",
    "|-----------|-----------|------------|\n",
    "| `num_train_epochs` | DurchlÃ¤ufe durch Datensatz | 3 |\n",
    "| `learning_rate` | SchrittgrÃ¶ÃŸe | 2e-4 |\n",
    "| `per_device_train_batch_size` | Samples pro GPU pro Schritt | 24 |\n",
    "| `gradient_accumulation_steps` | Akkumulierte Batches | 1 |\n",
    "| `warmup_steps` | LR-Warmup | 100 |\n",
    "| `eval_strategy` | Wann evaluieren | \"steps\" |\n",
    "| `save_strategy` | Wann speichern | \"steps\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b22a2a1",
   "metadata": {},
   "source": [
    "## 4. Trainingsdaten laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec327610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade gespeicherte Daten...\n",
      "\n",
      "Daten geladen!\n",
      "   Train: 7,168 samples\n",
      "   Val: 1,536 samples\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TRAININGSDATEN LADEN\n",
    "# ============================================================\n",
    "\n",
    "# Versuche gespeicherte Daten zu laden\n",
    "train_path = config.paths.processed_data_dir / \"train_raw\"\n",
    "val_path = config.paths.processed_data_dir / \"val_raw\"\n",
    "\n",
    "if train_path.exists() and val_path.exists():\n",
    "    print(f\"Lade gespeicherte Daten...\")\n",
    "    train_data = load_from_disk(str(train_path))\n",
    "    val_data = load_from_disk(str(val_path))\n",
    "else:\n",
    "    print(f\"Lade Dataset von HuggingFace und splitte...\")\n",
    "    dataset = load_dataset(config.data.dataset_name, cache_dir=str(config.paths.cache_dir))\n",
    "    \n",
    "    if isinstance(dataset, DatasetDict):\n",
    "        dataset = dataset[\"train\"] if \"train\" in dataset else dataset[list(dataset.keys())[0]]\n",
    "    \n",
    "    # Split\n",
    "    train_test = dataset.train_test_split(test_size=0.30, seed=config.data.dataset_split_seed)\n",
    "    val_test = train_test[\"test\"].train_test_split(test_size=0.50, seed=config.data.dataset_split_seed)\n",
    "    \n",
    "    train_data = train_test[\"train\"]\n",
    "    val_data = val_test[\"train\"]\n",
    "    \n",
    "    # Speichern\n",
    "    train_data.save_to_disk(str(train_path))\n",
    "    val_data.save_to_disk(str(val_path))\n",
    "    print(f\"Daten gespeichert\")\n",
    "\n",
    "print(f\"\\nDaten geladen!\")\n",
    "print(f\"   Train: {len(train_data):,} samples\")\n",
    "print(f\"   Val: {len(val_data):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51342e3d",
   "metadata": {},
   "source": [
    "## 5. Erstes SLM trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0861b0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Modell 1: meta-llama/Llama-3.2-3B-Instruct\n",
      "============================================================\n",
      "ðŸ“¥ Lade Base-Modell: meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 254/254 [00:00<00:00, 400.52it/s, Materializing param=model.norm.weight]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base-Modell geladen!\n",
      "Nach Laden - GPU: 6.43GB alloc, 6.44GB reserved\n",
      "ðŸ”§ Wende LoRA an (r=64, alpha=128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n",
      "`logging_dir` is deprecated and will be removed in v5.2. Please set `TENSORBOARD_LOGGING_DIR` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA angewendet!\n",
      "   Trainable: 97,255,424 (2.94%)\n",
      "   Total: 3,310,005,248\n",
      "Bereite Daten vor...\n",
      "Daten vorbereitet!\n",
      "   Train: 7,168 samples\n",
      "   Val: 1,536 samples\n",
      "Erstelle Trainer...\n",
      "Trainer erstellt!\n",
      "\n",
      "============================================================\n",
      "Starte Training: Llama-3.2-3B-Instruct\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='1077' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  19/1077 00:29 < 30:38, 0.58 it/s, Epoch 0.05/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ERSTES SLM TRAINIEREN\n",
    "# ============================================================\n",
    "\n",
    "training_results = {}\n",
    "\n",
    "# Erstes Modell\n",
    "slm_config_1 = config.model.slm_models[0]\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Modell 1: {slm_config_1['name']}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "finetuner_1 = FineTuner(\n",
    "    model_name=slm_config_1[\"name\"],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Training Pipeline\n",
    "finetuner_1.load_base_model()\n",
    "finetuner_1.apply_lora()\n",
    "finetuner_1.prepare_data(train_data, val_data)\n",
    "finetuner_1.setup_trainer()\n",
    "result_1 = finetuner_1.train()\n",
    "finetuner_1.save_model()\n",
    "\n",
    "# Ergebnis speichern\n",
    "training_results[slm_config_1[\"name\"]] = {\n",
    "    \"model_name\": slm_config_1[\"name\"],\n",
    "    \"model_size\": slm_config_1[\"size\"],\n",
    "    \"training_loss\": result_1.training_loss,\n",
    "    \"global_step\": result_1.global_step,\n",
    "    \"output_dir\": str(finetuner_1.output_dir),\n",
    "}\n",
    "\n",
    "# Cleanup\n",
    "finetuner_1.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9404531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Modell 2: Qwen/Qwen2.5-3B-Instruct\n",
      "============================================================\n",
      "ðŸ“¥ Lade Base-Modell: Qwen/Qwen2.5-3B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 434/434 [00:00<00:00, 644.06it/s, Materializing param=model.norm.weight]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base-Modell geladen!\n",
      "Nach Laden - GPU: 6.17GB alloc, 6.26GB reserved\n",
      "ðŸ”§ Wende LoRA an (r=64, alpha=128)\n",
      "LoRA angewendet!\n",
      "   Trainable: 119,734,272 (3.74%)\n",
      "   Total: 3,205,672,960\n",
      "Bereite Daten vor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n",
      "`logging_dir` is deprecated and will be removed in v5.2. Please set `TENSORBOARD_LOGGING_DIR` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten vorbereitet!\n",
      "   Train: 7,168 samples\n",
      "   Val: 1,536 samples\n",
      "Erstelle Trainer...\n",
      "Trainer erstellt!\n",
      "\n",
      "============================================================\n",
      "Starte Training: Qwen2.5-3B-Instruct\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='900' max='1077' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 900/1077 33:06 < 06:31, 0.45 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.675417</td>\n",
       "      <td>0.684779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.647531</td>\n",
       "      <td>0.654795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.635059</td>\n",
       "      <td>0.638873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.585515</td>\n",
       "      <td>0.633117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.575008</td>\n",
       "      <td>0.625780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.570120</td>\n",
       "      <td>0.618052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.561110</td>\n",
       "      <td>0.609637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.495462</td>\n",
       "      <td>0.620692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.504392</td>\n",
       "      <td>0.619704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training abgeschlossen!\n",
      "   Steps: 900\n",
      "   Loss: 0.6061\n",
      "Speichere Modell: /home/bmw/src/simon/finetuning/models/finetuned/Qwen2.5-3B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modell gespeichert!\n",
      "   Adapter: /home/bmw/src/simon/finetuning/models/finetuned/Qwen2.5-3B-Instruct\n",
      "   Merged: /home/bmw/src/simon/finetuning/models/finetuned/Qwen2.5-3B-Instruct/merged\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'training_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m finetuner_2.save_model()\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Ergebnis speichern\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mtraining_results\u001b[49m[slm_config_2[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]] = {\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel_name\u001b[39m\u001b[33m\"\u001b[39m: slm_config_2[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel_size\u001b[39m\u001b[33m\"\u001b[39m: slm_config_2[\u001b[33m\"\u001b[39m\u001b[33msize\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     28\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtraining_loss\u001b[39m\u001b[33m\"\u001b[39m: result_2.training_loss,\n\u001b[32m     29\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mglobal_step\u001b[39m\u001b[33m\"\u001b[39m: result_2.global_step,\n\u001b[32m     30\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_dir\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(finetuner_2.output_dir),\n\u001b[32m     31\u001b[39m }\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Cleanup\u001b[39;00m\n\u001b[32m     34\u001b[39m finetuner_2.cleanup()\n",
      "\u001b[31mNameError\u001b[39m: name 'training_results' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ZWEITES SLM TRAINIEREN\n",
    "# ============================================================\n",
    "\n",
    "if len(config.model.slm_models) > 1:\n",
    "    slm_config_2 = config.model.slm_models[1]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Modell 2: {slm_config_2['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    finetuner_2 = FineTuner(\n",
    "        model_name=slm_config_2[\"name\"],\n",
    "        config=config,\n",
    "    )\n",
    "    \n",
    "    # Training Pipeline\n",
    "    finetuner_2.load_base_model()\n",
    "    finetuner_2.apply_lora()\n",
    "    finetuner_2.prepare_data(train_data, val_data)\n",
    "    finetuner_2.setup_trainer()\n",
    "    result_2 = finetuner_2.train()\n",
    "    finetuner_2.save_model()\n",
    "    \n",
    "    # Ergebnis speichern\n",
    "    training_results[slm_config_2[\"name\"]] = {\n",
    "        \"model_name\": slm_config_2[\"name\"],\n",
    "        \"model_size\": slm_config_2[\"size\"],\n",
    "        \"training_loss\": result_2.training_loss,\n",
    "        \"global_step\": result_2.global_step,\n",
    "        \"output_dir\": str(finetuner_2.output_dir),\n",
    "    }\n",
    "    \n",
    "    # Cleanup\n",
    "    finetuner_2.cleanup()\n",
    "else:\n",
    "    print(\"Nur ein SLM konfiguriert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58885c38",
   "metadata": {},
   "source": [
    "## 6. Training-Ergebnisse visualisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "babeb177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training-Ergebnisse:\n",
      "============================================================\n",
      "             Modell GrÃ¶ÃŸe   Loss  Steps\n",
      "Qwen2.5-3B-Instruct    3B 0.6061    900\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot gespeichert!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TRAINING-ERGEBNISSE VISUALISIEREN\n",
    "# ============================================================\n",
    "\n",
    "print(\"Training-Ergebnisse:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Modell\": result[\"model_name\"].split(\"/\")[-1],\n",
    "        \"GrÃ¶ÃŸe\": result[\"model_size\"],\n",
    "        \"Loss\": result[\"training_loss\"],\n",
    "        \"Steps\": result[\"global_step\"],\n",
    "    }\n",
    "    for result in training_results.values()\n",
    "])\n",
    "\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Visualisierung\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "models = [r[\"model_name\"].split(\"/\")[-1] for r in training_results.values()]\n",
    "losses = [r[\"training_loss\"] for r in training_results.values()]\n",
    "\n",
    "colors = plt.cm.Greens(np.linspace(0.4, 0.8, len(models)))\n",
    "bars = ax.bar(range(len(models)), losses, color=colors, edgecolor='black')\n",
    "\n",
    "ax.set_xticks(range(len(models)))\n",
    "ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax.set_ylabel('Training Loss')\n",
    "ax.set_title('SLM Training Loss nach LoRA Finetuning')\n",
    "\n",
    "for bar, loss in zip(bars, losses):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{loss:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.paths.outputs_dir / 'plots' / 'slm_training_loss.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Plot gespeichert!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05374be7",
   "metadata": {},
   "source": [
    "## 7. Ergebnisse speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "850fc8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training-Ergebnisse gespeichert: /home/bmw/src/simon/finetuning/outputs/reports/slm_training_results.json\n",
      "Summary gespeichert: /home/bmw/src/simon/finetuning/outputs/reports/slm_training_summary.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ERGEBNISSE SPEICHERN\n",
    "# ============================================================\n",
    "\n",
    "# Training Results\n",
    "results_path = config.paths.outputs_dir / 'reports' / 'slm_training_results.json'\n",
    "results_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(training_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Training-Ergebnisse gespeichert: {results_path}\")\n",
    "\n",
    "# Summary\n",
    "summary = {\n",
    "    \"training_type\": \"LoRA Finetuning\",\n",
    "    \"lora_r\": config.lora.r,\n",
    "    \"lora_alpha\": config.lora.lora_alpha,\n",
    "    \"epochs\": config.training.num_train_epochs,\n",
    "    \"learning_rate\": config.training.learning_rate,\n",
    "    \"models_trained\": len(training_results),\n",
    "    \"trained_at\": datetime.now().isoformat(),\n",
    "}\n",
    "\n",
    "summary_path = config.paths.outputs_dir / 'reports' / 'slm_training_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"Summary gespeichert: {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d22a3656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ZUSAMMENFASSUNG: SLM LoRA Training\n",
      "============================================================\n",
      "\n",
      "Training abgeschlossen!\n",
      "\n",
      "Trainierte Modelle: 1\n",
      "\n",
      "   â€¢ Qwen2.5-3B-Instruct:\n",
      "     Loss: 0.6061\n",
      "     Output: /home/bmw/src/simon/finetuning/models/finetuned/Qwen2.5-3B-Instruct\n",
      "\n",
      "LoRA-Konfiguration:\n",
      "   r=64, alpha=128\n",
      "   Target: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
      "\n",
      "Training-Konfiguration:\n",
      "   Epochs: 3\n",
      "   LR: 0.0002\n",
      "   Batch: 20 x 1\n",
      "\n",
      "Gespeicherte Modelle:\n",
      "\n",
      "   â€¢ /home/bmw/src/simon/finetuning/models/finetuned/Qwen2.5-3B-Instruct\n",
      "\n",
      "NÃ¤chster Schritt: Notebook 05 - SLM Evaluation (Finetuned)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FINALE ZUSAMMENFASSUNG\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ZUSAMMENFASSUNG: SLM LoRA Training\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Training abgeschlossen!\n",
    "\n",
    "Trainierte Modelle: {len(training_results)}\n",
    "\"\"\")\n",
    "\n",
    "for name, result in training_results.items():\n",
    "    short = name.split(\"/\")[-1]\n",
    "    print(f\"   â€¢ {short}:\")\n",
    "    print(f\"     Loss: {result['training_loss']:.4f}\")\n",
    "    print(f\"     Output: {result['output_dir']}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "LoRA-Konfiguration:\n",
    "   r={config.lora.r}, alpha={config.lora.lora_alpha}\n",
    "   Target: {config.lora.target_modules}\n",
    "\n",
    "Training-Konfiguration:\n",
    "   Epochs: {config.training.num_train_epochs}\n",
    "   LR: {config.training.learning_rate}\n",
    "   Batch: {config.training.per_device_train_batch_size} x {config.training.gradient_accumulation_steps}\n",
    "\n",
    "Gespeicherte Modelle:\n",
    "\"\"\")\n",
    "\n",
    "for name, result in training_results.items():\n",
    "    print(f\"   â€¢ {result['output_dir']}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "NÃ¤chster Schritt: Notebook 05 - SLM Evaluation (Finetuned)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical-diagnosis-finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}