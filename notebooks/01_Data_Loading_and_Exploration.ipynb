{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Exploration\n",
    "\n",
    "**Goal:** Understand the MedSynth dataset and prepare it for training\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Loading medical conversation data from HuggingFace\n",
    "2. Dataset structure and statistics\n",
    "3. ICD-10 code distribution\n",
    "4. Train/validation/test splitting strategy\n",
    "5. Data quality checks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Environment setup\n",
    "import os\n",
    "os.environ['PYTORCH_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Add project to path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Dataset\n",
    "\n",
    "### What is MedSynth?\n",
    "\n",
    "**MedSynth** is a synthetic dataset of doctor-patient conversations with ICD-10 diagnosis codes.\n",
    "\n",
    "**Key features:**\n",
    "- ~10,000 conversations\n",
    "- Realistic medical dialogues\n",
    "- ICD-10 codes as labels\n",
    "- Multiple languages (we use English/German)\n",
    "\n",
    "**Why synthetic data?**\n",
    "- ‚úÖ No privacy concerns\n",
    "- ‚úÖ Perfect labels (generated with codes)\n",
    "- ‚úÖ Publicly available\n",
    "- ‚úÖ Diverse medical conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  CUDA not available - TF32 optimization skipped\n",
      "‚ö†Ô∏è  Deterministic mode enabled - may reduce TF32 benefits\n",
      "Dataset name: Ahmad0067/MedSynth\n",
      "Cache directory: /Users/qtf7046/uni/finetuning/data/cache\n"
     ]
    }
   ],
   "source": [
    "from data import create_data_loader\n",
    "from config import get_config\n",
    "\n",
    "# Get configuration\n",
    "config = get_config()\n",
    "config.setup()\n",
    "\n",
    "print(f\"Dataset name: {config.data.dataset_name}\")\n",
    "print(f\"Cache directory: {config.paths.cache_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unset http_proxy\n",
      "Unset https_proxy\n",
      "Unset HTTP_PROXY\n",
      "Unset HTTPS_PROXY\n",
      "Proxy settings cleared. Try running the download cell now.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List of proxy variables to unset\n",
    "proxy_vars = [\n",
    "    'http_proxy',\n",
    "    'https_proxy',\n",
    "    'HTTP_PROXY',\n",
    "    'HTTPS_PROXY'\n",
    "]\n",
    "\n",
    "for var in proxy_vars:\n",
    "    # use pop to remove the variable; the second argument None prevents errors if it doesn't exist\n",
    "    if var in os.environ:\n",
    "        os.environ.pop(var)\n",
    "        print(f\"Unset {var}\")\n",
    "    else:\n",
    "        print(f\"{var} was not set\")\n",
    "\n",
    "print(\"Proxy settings cleared. Try running the download cell now.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_loader = create_data_loader(\n",
    "    dataset_name=config.data.dataset_name,\n",
    "    cache_dir=config.paths.cache_dir,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "dataset = data_loader.load()\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"   Total samples: {len(dataset):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Structure üèóÔ∏è\n",
    "\n",
    "Let's examine what fields are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show dataset features\n",
    "print(\"Dataset Features:\")\n",
    "print(\"=\" * 60)\n",
    "for feature, dtype in dataset.features.items():\n",
    "    print(f\"  {feature:20} {dtype}\")\n",
    "\n",
    "# Show first example\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"First Example:\")\n",
    "print(\"=\" * 60)\n",
    "example = dataset[0]\n",
    "for key, value in example.items():\n",
    "    if isinstance(value, str) and len(value) > 200:\n",
    "        print(f\"{key}: {value[:200]}...\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Fields\n",
    "\n",
    "- **Dialogue**: The doctor-patient conversation (our input)\n",
    "- **ICD10**: The diagnosis code (our label/target)\n",
    "- **ICD10_desc**: Human-readable diagnosis description\n",
    "- **Note**: Additional clinical notes (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Statistics üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset statistics\n",
    "stats = data_loader.get_statistics()\n",
    "\n",
    "print(\"Dataset Statistics:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total samples: {stats['total_samples']:,}\")\n",
    "print(f\"\\nFeatures: {', '.join(stats['features'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas for analysis\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Show basic info\n",
    "print(\"\\nDataFrame Info:\")\n",
    "print(\"=\" * 60)\n",
    "print(df.info())\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nFirst 5 samples:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ICD-10 Code Analysis üîç\n",
    "\n",
    "### What are ICD-10 codes?\n",
    "\n",
    "**ICD-10** (International Classification of Diseases, 10th revision) is a medical classification list:\n",
    "\n",
    "**Structure:**\n",
    "```\n",
    "J06.9\n",
    "‚îÇ‚îÇ‚îÇ ‚îÇ\n",
    "‚îÇ‚îÇ‚îÇ ‚îî‚îÄ Sub-classification (.9 = unspecified)\n",
    "‚îÇ‚îÇ‚îî‚îÄ‚îÄ Category (06 = acute upper respiratory)\n",
    "‚îÇ‚îî‚îÄ‚îÄ‚îÄ Chapter (J = respiratory system)\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ Level\n",
    "```\n",
    "\n",
    "**Example codes:**\n",
    "- A00-B99: Infectious diseases\n",
    "- C00-D48: Neoplasms  \n",
    "- E00-E90: Endocrine, nutritional, metabolic\n",
    "- I00-I99: Circulatory system\n",
    "- J00-J99: Respiratory system\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique ICD codes\n",
    "unique_codes = df['ICD10'].nunique()\n",
    "print(f\"Unique ICD-10 codes: {unique_codes:,}\")\n",
    "\n",
    "# Count samples per code\n",
    "code_counts = df['ICD10'].value_counts()\n",
    "print(f\"\\nMost common codes:\")\n",
    "print(code_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top 20 ICD codes\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "code_counts.head(20).plot(kind='bar', ax=ax, color='steelblue')\n",
    "ax.set_title('Top 20 Most Common ICD-10 Codes', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('ICD-10 Code')\n",
    "ax.set_ylabel('Number of Samples')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDistribution insights:\")\n",
    "print(f\"  Mean samples per code: {code_counts.mean():.1f}\")\n",
    "print(f\"  Median samples per code: {code_counts.median():.0f}\")\n",
    "print(f\"  Most common code: {code_counts.index[0]} ({code_counts.iloc[0]} samples)\")\n",
    "print(f\"  Least common codes: {(code_counts == 1).sum()} codes with only 1 sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ICD chapters (first letter)\n",
    "df['ICD_Chapter'] = df['ICD10'].str[0]\n",
    "chapter_counts = df['ICD_Chapter'].value_counts()\n",
    "\n",
    "# Chapter meanings\n",
    "chapter_names = {\n",
    "    'A': 'A-B: Infectious diseases',\n",
    "    'B': 'A-B: Infectious diseases',\n",
    "    'C': 'C-D: Neoplasms',\n",
    "    'D': 'C-D: Neoplasms',\n",
    "    'E': 'E: Endocrine/Metabolic',\n",
    "    'F': 'F: Mental/Behavioral',\n",
    "    'G': 'G: Nervous system',\n",
    "    'H': 'H: Eye/Ear',\n",
    "    'I': 'I: Circulatory',\n",
    "    'J': 'J: Respiratory',\n",
    "    'K': 'K: Digestive',\n",
    "    'L': 'L: Skin',\n",
    "    'M': 'M: Musculoskeletal',\n",
    "    'N': 'N: Genitourinary',\n",
    "    'O': 'O: Pregnancy',\n",
    "    'P': 'P: Perinatal',\n",
    "    'Q': 'Q: Congenital',\n",
    "    'R': 'R: Symptoms/Signs',\n",
    "    'S': 'S-T: Injury/External',\n",
    "    'T': 'S-T: Injury/External',\n",
    "    'V': 'V-Y: External causes',\n",
    "    'Z': 'Z: Health status'\n",
    "}\n",
    "\n",
    "print(\"\\nICD-10 Chapters Distribution:\")\n",
    "print(\"=\" * 60)\n",
    "for chapter in chapter_counts.index:\n",
    "    count = chapter_counts[chapter]\n",
    "    pct = (count / len(df)) * 100\n",
    "    name = chapter_names.get(chapter, 'Unknown')\n",
    "    print(f\"{chapter}: {name:30} {count:5,} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize chapter distribution\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "chapter_counts.plot(kind='bar', ax=ax, color='coral')\n",
    "ax.set_title('Distribution by ICD-10 Chapter', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('ICD Chapter (First Letter)')\n",
    "ax.set_ylabel('Number of Samples')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dialogue Length Analysis üìè\n",
    "\n",
    "Understanding dialogue lengths is crucial for:\n",
    "- Setting max_sequence_length\n",
    "- Choosing batch sizes\n",
    "- Estimating training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dialogue lengths\n",
    "df['dialogue_length'] = df['Dialogue'].str.len()\n",
    "df['dialogue_words'] = df['Dialogue'].str.split().str.len()\n",
    "\n",
    "print(\"Dialogue Length Statistics:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Characters:\")\n",
    "print(f\"  Min:    {df['dialogue_length'].min():,}\")\n",
    "print(f\"  Max:    {df['dialogue_length'].max():,}\")\n",
    "print(f\"  Mean:   {df['dialogue_length'].mean():,.0f}\")\n",
    "print(f\"  Median: {df['dialogue_length'].median():,.0f}\")\n",
    "print(f\"\\nWords:\")\n",
    "print(f\"  Min:    {df['dialogue_words'].min():,}\")\n",
    "print(f\"  Max:    {df['dialogue_words'].max():,}\")\n",
    "print(f\"  Mean:   {df['dialogue_words'].mean():,.0f}\")\n",
    "print(f\"  Median: {df['dialogue_words'].median():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize length distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Character length\n",
    "axes[0].hist(df['dialogue_length'], bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(df['dialogue_length'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"dialogue_length\"].mean():.0f}')\n",
    "axes[0].axvline(df['dialogue_length'].median(), color='green', linestyle='--', label=f'Median: {df[\"dialogue_length\"].median():.0f}')\n",
    "axes[0].set_title('Distribution of Dialogue Length (Characters)')\n",
    "axes[0].set_xlabel('Length (characters)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Word count\n",
    "axes[1].hist(df['dialogue_words'], bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(df['dialogue_words'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"dialogue_words\"].mean():.0f}')\n",
    "axes[1].axvline(df['dialogue_words'].median(), color='green', linestyle='--', label=f'Median: {df[\"dialogue_words\"].median():.0f}')\n",
    "axes[1].set_title('Distribution of Dialogue Length (Words)')\n",
    "axes[1].set_xlabel('Length (words)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sample Conversations üí¨\n",
    "\n",
    "Let's look at actual examples to understand the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show random samples\n",
    "import random\n",
    "\n",
    "def show_sample(idx):\n",
    "    sample = dataset[idx]\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Sample #{idx}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nICD-10 Code: {sample['ICD10']}\")\n",
    "    print(f\"Description: {sample['ICD10_desc']}\")\n",
    "    print(f\"\\nDialogue:\")\n",
    "    print(\"-\" * 80)\n",
    "    dialogue = sample['Dialogue']\n",
    "    if len(dialogue) > 500:\n",
    "        print(dialogue[:500] + \"...\")\n",
    "        print(f\"\\n[Truncated - full length: {len(dialogue)} characters]\")\n",
    "    else:\n",
    "        print(dialogue)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Show 3 random examples\n",
    "for _ in range(3):\n",
    "    idx = random.randint(0, len(dataset)-1)\n",
    "    show_sample(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train/Val/Test Split üìä\n",
    "\n",
    "### Why do we split?\n",
    "\n",
    "**Three-way split is crucial for machine learning:**\n",
    "\n",
    "1. **Training Set (70%)**: Model learns from this\n",
    "2. **Validation Set (15%)**: Tune hyperparameters, early stopping\n",
    "3. **Test Set (15%)**: Final evaluation (never seen during training!)\n",
    "\n",
    "**Why not just train/test?**\n",
    "- Without validation, you'd tune on test set ‚Üí overfitting\n",
    "- Validation lets you optimize without \"peeking\" at test\n",
    "- Test remains pristine for final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import split_dataset\n",
    "\n",
    "# Split the dataset\n",
    "splits = split_dataset(\n",
    "    dataset,\n",
    "    train_ratio=config.data.train_ratio,\n",
    "    val_ratio=config.data.val_ratio,\n",
    "    test_ratio=config.data.test_ratio,\n",
    "    seed=config.data.dataset_split_seed  # For reproducibility!\n",
    ")\n",
    "\n",
    "train_dataset = splits['train']\n",
    "val_dataset = splits['validation']\n",
    "test_dataset = splits['test']\n",
    "\n",
    "print(\"Dataset Split:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train:      {len(train_dataset):,} samples ({config.data.train_ratio*100:.0f}%)\")\n",
    "print(f\"Validation: {len(val_dataset):,} samples ({config.data.val_ratio*100:.0f}%)\")\n",
    "print(f\"Test:       {len(test_dataset):,} samples ({config.data.test_ratio*100:.0f}%)\")\n",
    "print(f\"\\nTotal:      {len(train_dataset) + len(val_dataset) + len(test_dataset):,} samples\")\n",
    "\n",
    "# Verify no overlap\n",
    "assert len(set(train_dataset['ICD10']) & set(test_dataset['ICD10'])) > 0  # Should share some codes\n",
    "print(\"\\n‚úÖ Split verification passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize split\n",
    "split_sizes = [len(train_dataset), len(val_dataset), len(test_dataset)]\n",
    "split_labels = ['Train\\n(70%)', 'Validation\\n(15%)', 'Test\\n(15%)']\n",
    "colors = ['steelblue', 'coral', 'lightgreen']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.pie(split_sizes, labels=split_labels, autopct='%1.1f%%', colors=colors, \n",
    "       startangle=90, textprops={'fontsize': 12, 'weight': 'bold'})\n",
    "ax.set_title('Dataset Split Distribution', fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Quality Checks ‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values Check:\")\n",
    "print(\"=\" * 60)\n",
    "for col in df.columns:\n",
    "    missing = df[col].isna().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    status = \"‚úÖ\" if missing == 0 else \"‚ö†Ô∏è\"\n",
    "    print(f\"{status} {col:20} {missing:5,} missing ({missing_pct:5.2f}%)\")\n",
    "\n",
    "# Check for empty strings\n",
    "print(\"\\nEmpty String Check:\")\n",
    "print(\"=\" * 60)\n",
    "for col in ['Dialogue', 'ICD10', 'ICD10_desc']:\n",
    "    empty = (df[col].str.len() == 0).sum()\n",
    "    status = \"‚úÖ\" if empty == 0 else \"‚ö†Ô∏è\"\n",
    "    print(f\"{status} {col:20} {empty:5,} empty\")\n",
    "\n",
    "# Check ICD code format\n",
    "print(\"\\nICD-10 Format Check:\")\n",
    "print(\"=\" * 60)\n",
    "import re\n",
    "icd_pattern = r'^[A-Z]\\d{2}(\\.\\d{1,2})?$'\n",
    "valid_codes = df['ICD10'].str.match(icd_pattern).sum()\n",
    "invalid_codes = len(df) - valid_codes\n",
    "print(f\"‚úÖ Valid codes:   {valid_codes:,} ({valid_codes/len(df)*100:.1f}%)\")\n",
    "if invalid_codes > 0:\n",
    "    print(f\"‚ö†Ô∏è  Invalid codes: {invalid_codes:,} ({invalid_codes/len(df)*100:.1f}%)\")\n",
    "    print(f\"\\nExamples of invalid codes:\")\n",
    "    print(df[~df['ICD10'].str.match(icd_pattern)]['ICD10'].head(10).tolist())\n",
    "else:\n",
    "    print(\"‚úÖ All ICD codes are valid!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways üí°\n",
    "\n",
    "### Dataset Summary\n",
    "\n",
    "From this exploration, we learned:\n",
    "\n",
    "1. **Dataset Size**: ~10,000 medical conversations\n",
    "2. **Task**: Predict ICD-10 codes from dialogues\n",
    "3. **Label Distribution**: Some codes are common, many are rare (long-tail)\n",
    "4. **Dialogue Length**: Average ~XXX words, max ~XXX words\n",
    "5. **Data Quality**: Clean data with valid ICD codes\n",
    "\n",
    "### Implications for Training\n",
    "\n",
    "**What we know now:**\n",
    "- Need to handle class imbalance (some codes rare)\n",
    "- Max sequence length of 512 tokens should be sufficient\n",
    "- Train/val/test split ensures fair evaluation\n",
    "- Clean data means less preprocessing needed\n",
    "\n",
    "**Next steps:**\n",
    "1. Tokenization (convert text to numbers)\n",
    "2. Format dialogues with chat templates\n",
    "3. Create batches for training\n",
    "\n",
    "---\n",
    "\n",
    "## Next Notebook üëâ\n",
    "\n",
    "**02_Data_Processing_and_Tokenization.ipynb**\n",
    "\n",
    "Learn how to:\n",
    "- Convert text to token IDs\n",
    "- Apply chat templates\n",
    "- Create training batches\n",
    "- Visualize tokenization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
