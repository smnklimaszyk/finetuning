{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fb25e98",
   "metadata": {},
   "source": [
    "# Medical Diagnosis Finetuning: Projekt-Übersicht\n",
    "\n",
    "## Projekt\n",
    "\n",
    "> **\"Können kleine, spezialisierte Modelle (3B Parameter) große generische Modelle (7-8B Parameter) bei der ICD-10 Klassifikation übertreffen?\"**\n",
    "\n",
    "---\n",
    "\n",
    "### Inhaltsverzeichnis dieser Notebook-Serie\n",
    "\n",
    "| Notebook | Titel | Beschreibung |\n",
    "|----------|-------|--------------|\n",
    "| **00** | Projekt-Übersicht | Forschungsfrage, Architektur, experimentelles Design |\n",
    "| **01** | Daten laden & explorieren | MedSynth Dataset, Statistiken, Qualitätsprüfung |\n",
    "| **02** | Datenverarbeitung & Tokenisierung | Chat-Templates, Tokenisierung, Train/Val/Test Split |\n",
    "| **03** | LLM Evaluation (Zero-Shot) | Große Modelle ohne Training bewerten |\n",
    "| **04** | SLM Training mit LoRA | Kleine Modelle finetunen |\n",
    "| **05** | SLM Evaluation (Finetuned) | Trainierte kleine Modelle bewerten |\n",
    "| **06** | Ergebnisanalyse & Vergleich | Größe vs. Spezialisierung auswerten |\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Forschungsansatz\n",
    "\n",
    "#### 1.1 Das Problem\n",
    "\n",
    "Im Themenfeld der künstlichen Intelligenz stehen Experten foft vor einem fundamentalen Trade-Off:\n",
    "\n",
    "| Ansatz | Vorteile | Nachteile |\n",
    "|--------|----------|-----------|\n",
    "| **Große Modelle (7-8B Parameter)** | Mehr Wissen, bessere allgemeine Performance | Langsam, teuer, hoher Ressourcenbedarf |\n",
    "| **Kleine Modelle (3B Parameter)** | Schnell, günstig, lokal ausführbar | Weniger leistungsfähig ohne Training |\n",
    "\n",
    "#### 1.2 Unsere Hypothese\n",
    "\n",
    "> **Kann Spezialisierung durch Finetuning den Größennachteil kleinerer Modelle bei domänenspezifischen Aufgaben kompensieren?**\n",
    "\n",
    "Anders ausgedrückt: Kann ein 3B-Modell, das auf medizinische Daten trainiert wurde, ein 8B-Modell ohne medizinisches Training übertreffen?\n",
    "\n",
    "| Modelltyp | Größe | Training | Hypothese |\n",
    "|-----------|-------|----------|-----------|\n",
    "| **LLM** (Large Language Model) | 7-8B Parameter | Zero-Shot (kein Training) | Größe = Wissen |\n",
    "| **SLM** (Small Language Model) | 3B Parameter | LoRA Finetuning | Spezialisierung = Effizienz |\n",
    "\n",
    "#### 1.3 Warum ist das relevant?\n",
    "\n",
    "1. **Kosten**: Kleinere Modelle = günstigere Inference (~3x billiger)\n",
    "2. **Latenz**: Weniger Parameter = schnellere Antworten (~2.5x schneller)\n",
    "3. **Datenschutz**: Kleinere Modelle können lokal laufen (On-Premise)\n",
    "4. **Nachhaltigkeit**: Weniger Compute = weniger CO2-Emissionen\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08589321",
   "metadata": {},
   "source": [
    "## 3. Experimentelles Design\n",
    "\n",
    "### 3.2 Die zu vergleichenden Modelle\n",
    "\n",
    "#### Large Language Models - Große, untrainierte Modelle\n",
    "\n",
    "Diese Modelle werden **nicht** finetuned, sondern nur zero-shot evaluiert.\n",
    "Sie repräsentieren den Ansatz \"Größe ohne Spezialisierung\".\n",
    "\n",
    "| Modell | Größe | Beschreibung |\n",
    "|--------|-------|--------------|\n",
    "| Meta-Llama-3.1-8B-Instruct | 8B | Großes Referenzmodell von Meta |\n",
    "| Mistral-7B-Instruct-v0.3 | 7B | Mittleres Referenzmodell von Mistral |\n",
    "\n",
    "#### Small Language Models - Kleine, finetuned Modelle\n",
    "\n",
    "| Modell | Größe | Beschreibung | Training |\n",
    "|--------|-------|--------------|----------|\n",
    "| Llama-3.2-3B-Instruct | 3B | Kompaktes Llama-Modell | LoRA finetuned |\n",
    "| Qwen2.5-3B-Instruct | 3B | Kompaktes Qwen-Modell | LoRA finetuned |\n",
    "\n",
    "### 3.3 Was genau testen wir?\n",
    "\n",
    "```\n",
    "LLMs (Größenvorteil)              SLMs (Spezialisierungsvorteil)\n",
    "├─ Llama 8B (untrainiert)        ├─ Llama 3B (LoRA finetuned)\n",
    "└─ Mistral 7B (untrainiert)      └─ Qwen 3B (LoRA finetuned)\n",
    "        ↓                                 ↓\n",
    "   Zero-Shot                       Domain-Adapted\n",
    "   Inference                       Inference\n",
    "        ↓                                 ↓\n",
    "   ┌─────────────────────────────────────────┐\n",
    "   │     ICD-10 Code Klassifikation          │\n",
    "   │        (Medizinische Diagnose)          │\n",
    "   └─────────────────────────────────────────┘\n",
    "        ↓                                 ↓\n",
    "   Performance                       Performance\n",
    "   Vergleich                         Vergleich\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68b3e7b",
   "metadata": {},
   "source": [
    "## 4. Theoretische Grundlagen\n",
    "\n",
    "### 4.1 Was ist Finetuning?\n",
    "\n",
    "**Finetuning** ist das Anpassen eines vortrainierten Modells auf eine spezifische Aufgabe.\n",
    "\n",
    "```\n",
    "Vortrainiertes Modell     Finetuning        Spezialisiertes Modell\n",
    "(generelles Wissen)    →  (+ Domain-Daten) →  (+ Domänen-Wissen)\n",
    "```\n",
    "\n",
    "**Warum Finetuning statt Training von Grund auf?**\n",
    "\n",
    "- Vortrainierte Modelle haben bereits Sprachverständnis gelernt\n",
    "- Finetuning braucht viel weniger Daten (1.000e vs. Milliarden)\n",
    "- Schneller und günstiger\n",
    "\n",
    "### 4.2 Was ist LoRA (Low-Rank Adaptation)?\n",
    "\n",
    "**LoRA** ist eine **parameter-effiziente** Finetuning-Methode.\n",
    "\n",
    "**Das Problem:** Normale Finetuning-Methoden ändern alle Parameter (Milliarden!).\n",
    "\n",
    "**Die Lösung:** LoRA trainiert nur kleine \"Adapter\"-Matrizen:\n",
    "\n",
    "```\n",
    "Original-Matrix W:    [1000 x 1000] = 1.000.000 Parameter\n",
    "LoRA-Matrizen A, B:   [1000 x 16] + [16 x 1000] = 32.000 Parameter\n",
    "                                                   = 3.2% der Original-Größe!\n",
    "```\n",
    "\n",
    "**Die LoRA-Formel:**\n",
    "\n",
    "$$W' = W + \\Delta W = W + A \\times B$$\n",
    "\n",
    "Wobei:\n",
    "- $W$ = Originale Gewichte (eingefroren, nicht trainiert)\n",
    "- $A$ = Down-Projection Matrix (Input → niedrig-dimensionaler Raum)\n",
    "- $B$ = Up-Projection Matrix (niedrig-dimensionaler Raum → Output)\n",
    "- $r$ = Rank (typisch 8-64, kontrolliert Kapazität)\n",
    "\n",
    "**Vorteile von LoRA:**\n",
    "- Weniger als 1% der Parameter werden trainiert\n",
    "- Deutlich geringerer Speicherbedarf\n",
    "- Schnelleres Training\n",
    "- Originales Modell bleibt unverändert (einfach rückgängig zu machen)\n",
    "\n",
    "### 4.3 Was ist ICD-10?\n",
    "\n",
    "**ICD-10** (International Classification of Diseases, 10. Revision) ist das weltweite Standard-System zur Klassifikation von Krankheiten und Diagnosen.\n",
    "\n",
    "**Aufbau eines ICD-10 Codes:**\n",
    "\n",
    "```\n",
    "J06.9\n",
    "│││ │\n",
    "│││ └── Weitere Spezifikation (.9 = nicht näher bezeichnet)\n",
    "│││\n",
    "││└──── Hauptgruppe innerhalb Kapitel (06 = Akute Infektionen obere Atemwege)\n",
    "││\n",
    "│└───── Kapitel-Buchstabe (J = Atmungssystem)\n",
    "│\n",
    "└────── Hierarchie-Ebene\n",
    "```\n",
    "\n",
    "**Beispiele:**\n",
    "- `J06.9` = Akute Infektion der oberen Atemwege, nicht näher bezeichnet\n",
    "- `I10` = Essentielle Hypertonie (Bluthochdruck)\n",
    "- `G43.9` = Migräne, nicht näher bezeichnet\n",
    "- `E11.9` = Diabetes mellitus Typ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1490a4a8",
   "metadata": {},
   "source": [
    "## 5. Technologien & Dependencies\n",
    "\n",
    "| Framework | Zweck | Warum? |\n",
    "|-----------|-------|--------|\n",
    "| **PyTorch** | Deep Learning | Industry-Standard, flexible |\n",
    "| **HuggingFace Transformers** | Modell-Loading | Einfacher Zugang zu vortrainierten Modellen |\n",
    "| **PEFT/LoRA** | Parameter-effizientes Training | Ermöglicht Training auf Consumer-GPUs |\n",
    "| **BitsAndBytes** | Quantisierung | Reduziert Speicherbedarf um 75% |\n",
    "| **Datasets** | Datenverarbeitung | Effizientes Memory-Management |\n",
    "| **Pydantic** | Konfiguration | Type-safe, validierte Configs |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8287619",
   "metadata": {},
   "source": [
    "## 6. Zentrale Konfiguration\n",
    "\n",
    "Die folgende Zelle enthält **alle Konfigurationsparameter** für das gesamte Projekt. Diese Werte werden in allen nachfolgenden Notebooks verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff1bdb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Konfiguration geladen!\n",
      "   Projekt-Root: /home/bmw/src/simon/finetuning\n",
      "   Dataset: Ahmad0067/MedSynth\n",
      "   CUDA verfügbar: True\n",
      "   GPU: NVIDIA GeForce RTX 5090\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ZENTRALE KONFIGURATION\n",
    "# ============================================================\n",
    "# Diese Konfiguration wird in allen Notebooks verwendet.\n",
    "# ============================================================\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# ============================================================\n",
    "# PFAD-KONFIGURATION\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class PathConfig:\n",
    "    \"\"\"Alle Pfade für das Projekt.\"\"\"\n",
    "    # Basis-Verzeichnis (relativ zum Notebooks-Ordner)\n",
    "    project_root: Path = field(default_factory=lambda: Path.cwd().parent)\n",
    "    \n",
    "    # Daten-Verzeichnisse\n",
    "    data_dir: Path = field(default_factory=lambda: Path.cwd().parent / \"data\")\n",
    "    cache_dir: Path = field(default_factory=lambda: Path.cwd().parent / \"data\" / \"cache\")\n",
    "    \n",
    "    # Modell-Verzeichnisse\n",
    "    models_dir: Path = field(default_factory=lambda: Path.cwd().parent / \"models\")\n",
    "    finetuned_models_dir: Path = field(default_factory=lambda: Path.cwd().parent / \"models\" / \"finetuned\")\n",
    "    \n",
    "    # Output-Verzeichnisse\n",
    "    outputs_dir: Path = field(default_factory=lambda: Path.cwd().parent / \"outputs\")\n",
    "    logs_dir: Path = field(default_factory=lambda: Path.cwd().parent / \"outputs\" / \"logs\")\n",
    "    plots_dir: Path = field(default_factory=lambda: Path.cwd().parent / \"outputs\" / \"plots\")\n",
    "    reports_dir: Path = field(default_factory=lambda: Path.cwd().parent / \"outputs\" / \"reports\")\n",
    "    predictions_cache_dir: Path = field(default_factory=lambda: Path.cwd().parent / \"outputs\" / \"cache\" / \"predictions\")\n",
    "    \n",
    "    def create_directories(self):\n",
    "        \"\"\"Erstellt alle benötigten Verzeichnisse.\"\"\"\n",
    "        for attr_name in dir(self):\n",
    "            attr = getattr(self, attr_name)\n",
    "            if isinstance(attr, Path) and not attr_name.startswith('_'):\n",
    "                attr.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# DATEN-KONFIGURATION\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    \"\"\"Konfiguration für Datenverarbeitung.\"\"\"\n",
    "    # Dataset\n",
    "    dataset_name: str = \"Ahmad0067/MedSynth\"\n",
    "    dataset_split_seed: int = 42\n",
    "    \n",
    "    # Train/Val/Test Split\n",
    "    train_ratio: float = 0.70\n",
    "    val_ratio: float = 0.15\n",
    "    test_ratio: float = 0.15\n",
    "    \n",
    "    # Tokenisierung\n",
    "    max_sequence_length: int = 512\n",
    "    truncation: bool = True\n",
    "    padding: str = \"max_length\"\n",
    "    \n",
    "    # Batch-Verarbeitung\n",
    "    batch_size: int = 128\n",
    "    num_workers: int = 12\n",
    "\n",
    "# ============================================================\n",
    "# MODELL-KONFIGURATION\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Konfiguration für Modelle.\"\"\"\n",
    "    # LLMs (große Modelle für Zero-Shot Baseline)\n",
    "    llm_models: List[dict] = field(default_factory=lambda: [\n",
    "        {\n",
    "            \"name\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "            \"size\": \"8B\",\n",
    "            \"description\": \"8B - Large reference model\",\n",
    "            \"load_in_4bit\": True,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "            \"size\": \"7B\",\n",
    "            \"description\": \"7B - Medium reference model\",\n",
    "            \"load_in_4bit\": True,\n",
    "        },\n",
    "    ])\n",
    "    \n",
    "    # SLMs (kleine Modelle für Finetuning)\n",
    "    slm_models: List[dict] = field(default_factory=lambda: [\n",
    "        {\n",
    "            \"name\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "            \"size\": \"3B\",\n",
    "            \"description\": \"3B - Compact Llama for finetuning\",\n",
    "            \"load_in_4bit\": False,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "            \"size\": \"3B\",\n",
    "            \"description\": \"3B - Compact Qwen for finetuning\",\n",
    "            \"load_in_4bit\": False,\n",
    "        },\n",
    "    ])\n",
    "    \n",
    "    # Generation-Parameter\n",
    "    max_new_tokens: int = 256\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.9\n",
    "    top_k: int = 50\n",
    "    repetition_penalty: float = 1.1\n",
    "\n",
    "# ============================================================\n",
    "# TRAINING-KONFIGURATION\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Konfiguration für LoRA Finetuning.\"\"\"\n",
    "    # Training Hyperparameter\n",
    "    num_epochs: int = 3\n",
    "    learning_rate: float = 2e-4\n",
    "    warmup_steps: int = 100\n",
    "    weight_decay: float = 0.01\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer: str = \"adamw_torch_fused\"\n",
    "    \n",
    "    # Batch Sizes\n",
    "    per_device_train_batch_size: int = 32\n",
    "    per_device_eval_batch_size: int = 64\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    \n",
    "    # Precision\n",
    "    fp16: bool = False\n",
    "    bf16: bool = True\n",
    "    \n",
    "    # Gradient\n",
    "    max_grad_norm: float = 1.0\n",
    "    gradient_checkpointing: bool = False\n",
    "    \n",
    "    # Logging & Evaluation\n",
    "    logging_steps: int = 5\n",
    "    eval_steps: int = 50\n",
    "    save_steps: int = 200\n",
    "    save_total_limit: int = 3\n",
    "    \n",
    "    # Early Stopping\n",
    "    early_stopping_patience: int = 3\n",
    "    early_stopping_threshold: float = 0.001\n",
    "    \n",
    "    # LoRA Konfiguration\n",
    "    use_lora: bool = True\n",
    "    lora_r: int = 64\n",
    "    lora_alpha: int = 128\n",
    "    lora_dropout: float = 0.1\n",
    "    lora_target_modules: List[str] = field(default_factory=lambda: [\n",
    "        \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ])\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed: int = 42\n",
    "\n",
    "# ============================================================\n",
    "# EVALUATION-KONFIGURATION\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class EvaluationConfig:\n",
    "    \"\"\"Konfiguration für Evaluation.\"\"\"\n",
    "    eval_batch_size: int = 48\n",
    "    max_eval_samples: Optional[int] = None\n",
    "    use_prediction_cache: bool = True\n",
    "    generate_plots: bool = True\n",
    "\n",
    "# ============================================================\n",
    "# HAUPT-KONFIGURATION\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Haupt-Konfiguration die alle Sub-Configs vereint.\"\"\"\n",
    "    paths: PathConfig = field(default_factory=PathConfig)\n",
    "    data: DataConfig = field(default_factory=DataConfig)\n",
    "    model: ModelConfig = field(default_factory=ModelConfig)\n",
    "    training: TrainingConfig = field(default_factory=TrainingConfig)\n",
    "    evaluation: EvaluationConfig = field(default_factory=EvaluationConfig)\n",
    "    \n",
    "    def setup(self):\n",
    "        \"\"\"Initialisiert alle Verzeichnisse und Seeds.\"\"\"\n",
    "        self.paths.create_directories()\n",
    "        self._set_seeds()\n",
    "        self._enable_tf32()\n",
    "    \n",
    "    def _set_seeds(self):\n",
    "        \"\"\"Setzt alle Random Seeds.\"\"\"\n",
    "        import random\n",
    "        import numpy as np\n",
    "        \n",
    "        seed = self.training.seed\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    def _enable_tf32(self):\n",
    "        \"\"\"Aktiviert TF32 für NVIDIA GPUs.\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# ============================================================\n",
    "# KONFIGURATION ERSTELLEN\n",
    "# ============================================================\n",
    "def get_config() -> Config:\n",
    "    \"\"\"Factory-Funktion für Konfiguration.\"\"\"\n",
    "    return Config()\n",
    "\n",
    "# Initialisierung\n",
    "config = get_config()\n",
    "config.setup()\n",
    "\n",
    "print(\"Konfiguration geladen!\")\n",
    "print(f\"   Projekt-Root: {config.paths.project_root}\")\n",
    "print(f\"   Dataset: {config.data.dataset_name}\")\n",
    "print(f\"   CUDA verfügbar: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd481cf",
   "metadata": {},
   "source": [
    "### Utility-Funktionen\n",
    "\n",
    "Die folgenden Funktionen werden in allen Notebooks verwendet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "181b3cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility-Funktionen geladen!\n",
      "   Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# UTILITY-FUNKTIONEN\n",
    "# ============================================================\n",
    "# Diese Funktionen werden in allen Notebooks verwendet.\n",
    "# ============================================================\n",
    "\n",
    "import gc\n",
    "import json\n",
    "import hashlib\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "def get_device() -> str:\n",
    "    \"\"\"Bestimmt das beste verfügbare Device.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "def aggressive_memory_cleanup():\n",
    "    \"\"\"Führt aggressive Memory-Cleanup durch.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "def log_gpu_memory(prefix: str = \"\"):\n",
    "    \"\"\"Loggt aktuellen GPU Memory Status.\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return\n",
    "    \n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    \n",
    "    msg = f\"GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\"\n",
    "    if prefix:\n",
    "        msg = f\"{prefix} - {msg}\"\n",
    "    print(msg)\n",
    "\n",
    "def save_json(data: dict, path: Path):\n",
    "    \"\"\"Speichert Dict als JSON.\"\"\"\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(data, f, indent=2, default=str)\n",
    "\n",
    "def load_json(path: Path) -> dict:\n",
    "    \"\"\"Lädt JSON als Dict.\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def generate_cache_key(model_name: str, dataset_size: int, \n",
    "                       generation_config: Optional[Dict[str, Any]] = None) -> str:\n",
    "    \"\"\"Generiert eindeutigen Cache-Key für Predictions.\"\"\"\n",
    "    cache_parts = [model_name.replace(\"/\", \"_\"), str(dataset_size)]\n",
    "    \n",
    "    if generation_config:\n",
    "        config_str = json.dumps(generation_config, sort_keys=True)\n",
    "        cache_parts.append(config_str)\n",
    "    \n",
    "    cache_string = \"|\".join(cache_parts)\n",
    "    cache_hash = hashlib.md5(cache_string.encode()).hexdigest()[:12]\n",
    "    \n",
    "    model_short = model_name.split(\"/\")[-1].replace(\".\", \"_\")\n",
    "    return f\"{model_short}_{cache_hash}\"\n",
    "\n",
    "print(\"Utility-Funktionen geladen!\")\n",
    "print(f\"   Device: {get_device()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b97afe7",
   "metadata": {},
   "source": [
    "## Pipeline-Übersicht\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                        MEDICAL DIAGNOSIS PIPELINE                    │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                      │\n",
    "│  ┌─────────────┐   ┌─────────────┐   ┌─────────────┐                │\n",
    "│  │   MedSynth  │──▶│   Tokenize  │──▶│ Train/Val/  │                │\n",
    "│  │   Dataset   │   │  + Format   │   │ Test Split  │                │\n",
    "│  └─────────────┘   └─────────────┘   └──────┬──────┘                │\n",
    "│                                              │                       │\n",
    "│         ┌────────────────────────────────────┼──────────┐           │\n",
    "│         ▼                                    ▼          ▼           │\n",
    "│  ┌─────────────┐                      ┌─────────────┐               │\n",
    "│  │  LLM (8B)   │                      │  SLM (3B)   │               │\n",
    "│  │  Zero-Shot  │                      │   LoRA FT   │               │\n",
    "│  └──────┬──────┘                      └──────┬──────┘               │\n",
    "│         │                                    │                       │\n",
    "│         ▼                                    ▼                       │\n",
    "│  ┌─────────────┐                      ┌─────────────┐               │\n",
    "│  │  Evaluate   │                      │  Evaluate   │               │\n",
    "│  │  Baseline   │                      │  Finetuned  │               │\n",
    "│  └──────┬──────┘                      └──────┬──────┘               │\n",
    "│         │                                    │                       │\n",
    "│         └────────────────┬───────────────────┘                      │\n",
    "│                          ▼                                          │\n",
    "│                   ┌─────────────┐                                   │\n",
    "│                   │   Compare   │                                   │\n",
    "│                   │  & Report   │                                   │\n",
    "│                   └─────────────┘                                   │\n",
    "│                                                                      │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
