{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Diagnosis Model Finetuning - Project Overview\n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Covers\n",
    "\n",
    "1. **The Research Question** - Why this matters\n",
    "2. **Project Architecture** - How everything fits together\n",
    "3. **Experimental Design** - What we're comparing\n",
    "4. **Quick Setup** - Get started fast\n",
    "5. **Notebook Series** - What each notebook does\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Research Question \n",
    "\n",
    "### The Problem\n",
    "\n",
    "In healthcare AI, we face a fundamental trade-off:\n",
    "\n",
    "| Approach | Pros | Cons |\n",
    "|----------|------|------|\n",
    "| **Large Models** | More knowledge, better general performance | Slow and expensive |\n",
    "| **Small Models (3B parameters)** | Fast, cheap, can run locally | Less capable out-of-the-box |\n",
    "\n",
    "### Our Hypothesis\n",
    "\n",
    "> **Specialization through finetuning can compensate for smaller model size on domain-specific tasks**\n",
    "\n",
    "In other words: A 3B model trained on medical data might outperform an 8B model with no medical training!\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "**If small finetuned models win:**\n",
    "- âœ… Faster inference (2-3x speedup)\n",
    "- âœ… Lower costs (can run on smaller GPUs)\n",
    "- âœ… Better privacy (can run on-premise)\n",
    "- âœ… More sustainable (less energy consumption)\n",
    "\n",
    "**Real-world impact:**\n",
    "- Rural hospitals with limited compute\n",
    "- Privacy-sensitive patient data\n",
    "- Large-scale deployment (thousands of instances)\n",
    "- Embedded medical devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Project Architecture ðŸ—ï¸\n",
    "\n",
    "### High-Level Pipeline\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  1. DATA LOADING                                        â”‚\n",
    "â”‚     â”œâ”€ MedSynth Dataset (10K medical conversations)     â”‚\n",
    "â”‚     â”œâ”€ Split: 70% train, 15% val, 15% test            â”‚\n",
    "â”‚     â””â”€ ICD-10 diagnosis codes as labels                â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  2. DATA PROCESSING                                     â”‚\n",
    "â”‚     â”œâ”€ Tokenization (text â†’ numbers)                   â”‚\n",
    "â”‚     â”œâ”€ Chat template formatting                         â”‚\n",
    "â”‚     â””â”€ Padding & truncation                            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â†“\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â†“                               â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  3a. LLM EVAL    â”‚          â”‚  3b. SLM TRAIN   â”‚\n",
    "â”‚  (Zero-Shot)     â”‚          â”‚  (LoRA Finetune) â”‚\n",
    "â”‚                  â”‚          â”‚                  â”‚\n",
    "â”‚  â€¢ Llama 8B      â”‚          â”‚  â€¢ Llama 3B      â”‚\n",
    "â”‚  â€¢ Mistral 7B    â”‚          â”‚  â€¢ Qwen 3B       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        â†“                               â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  4a. LLM Results â”‚          â”‚  4b. SLM Eval    â”‚\n",
    "â”‚  Accuracy, F1... â”‚          â”‚  Accuracy, F1... â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        â†“                               â†“\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  5. COMPARISON & ANALYSIS                               â”‚\n",
    "â”‚     â”œâ”€ Size vs Specialization                          â”‚\n",
    "â”‚     â”œâ”€ Performance metrics                              â”‚\n",
    "â”‚     â”œâ”€ Visualizations                                   â”‚\n",
    "â”‚     â””â”€ Conclusions                                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experimental Design\n",
    "\n",
    "### Models Being Compared\n",
    "\n",
    "#### LLMs (Large, Untrained)\n",
    "\n",
    "**Purpose:** Baseline performance with size advantage\n",
    "\n",
    "| Model | Size | Training | Use Case |\n",
    "|-------|------|----------|----------|\n",
    "| Meta-Llama-3.1-8B | 8B | Zero-shot | Large reference |\n",
    "| Mistral-7B | 7B | Zero-shot | Medium reference |\n",
    "\n",
    "**Strengths:**\n",
    "- More parameters = more knowledge\n",
    "- No training needed\n",
    "- General-purpose capability\n",
    "\n",
    "**Weaknesses:**\n",
    "- Slow (8-10 tokens/sec)\n",
    "- Memory-intensive\n",
    "- No domain specialization\n",
    "\n",
    "#### SLMs (Small, Finetuned)\n",
    "\n",
    "**Purpose:** Test if specialization beats size\n",
    "\n",
    "| Model | Size | Training | Use Case |\n",
    "|-------|------|----------|----------|\n",
    "| Llama-3.2-3B | 3B | LoRA finetuned | Specialized model #1 |\n",
    "| Qwen2.5-3B | 3B | LoRA finetuned | Specialized model #2 |\n",
    "\n",
    "**Strengths:**\n",
    "- Fast (20-30 tokens/sec)\n",
    "- Memory-efficient\n",
    "- Domain-specialized\n",
    "\n",
    "**Weaknesses:**\n",
    "- Requires training\n",
    "- Fewer parameters\n",
    "- Limited to medical domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: ICD-10 Code Classification\n",
    "\n",
    "**Input:** Doctor-patient conversation\n",
    "\n",
    "```\n",
    "Doctor: What brings you here today?\n",
    "Patient: I have a sore throat and runny nose for 3 days.\n",
    "Doctor: Do you have fever?\n",
    "Patient: Yes, 38.5Â°C yesterday.\n",
    "...\n",
    "```\n",
    "\n",
    "**Output:** ICD-10 diagnosis code\n",
    "\n",
    "```\n",
    "J06.9  (Acute upper respiratory infection, unspecified)\n",
    "```\n",
    "\n",
    "**Why ICD-10?**\n",
    "- International standard for diagnoses\n",
    "- Used worldwide in healthcare systems\n",
    "- Clear ground truth for evaluation\n",
    "- Hierarchical structure (J06.9 â†’ J06 â†’ J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Method: LoRA (Low-Rank Adaptation)\n",
    "\n",
    "**Why LoRA?**\n",
    "\n",
    "Traditional finetuning trains ALL parameters:\n",
    "```\n",
    "3B model = 3,000,000,000 trainable parameters\n",
    "Memory needed: ~12GB VRAM\n",
    "Training time: ~10 hours\n",
    "```\n",
    "\n",
    "LoRA trains only small adapter matrices:\n",
    "```\n",
    "LoRA adapters = ~10,000,000 trainable parameters (0.3%!)\n",
    "Memory needed: ~6GB VRAM\n",
    "Training time: ~2 hours\n",
    "```\n",
    "\n",
    "**How LoRA Works:**\n",
    "\n",
    "```\n",
    "Original weight matrix W:  [4096 x 4096] = 16M parameters âŒ\n",
    "\n",
    "LoRA decomposition:\n",
    "  A matrix: [4096 x 64]  = 262K parameters âœ…\n",
    "  B matrix: [64 x 4096]  = 262K parameters âœ…\n",
    "  Total:                 = 524K parameters (97% reduction!)\n",
    "\n",
    "Updated weight: W' = W + A Ã— B\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- âœ… 97% fewer trainable parameters\n",
    "- âœ… 5x faster training\n",
    "- âœ… 50% less VRAM needed\n",
    "- âœ… Can train on consumer GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quick Setup\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "```bash\n",
    "# GPU with CUDA (recommended: 16GB+ VRAM)\n",
    "nvidia-smi\n",
    "\n",
    "# Python 3.11+\n",
    "python --version\n",
    "\n",
    "# Install dependencies\n",
    "pip install -e \".[dev]\"\n",
    "```\n",
    "\n",
    "### Environment Setup\n",
    "\n",
    "**IMPORTANT:** Set this before running anything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment configured for optimal GPU memory usage\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Critical for memory management on modern GPUs\n",
    "os.environ['PYTORCH_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "print(\"Environment configured for optimal GPU memory usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ CUDA not available - will use CPU (very slow!)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"âŒ CUDA not available - will use CPU (very slow!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: /Users/qtf7046/uni/finetuning\n",
      "\n",
      "Key Directories:\n",
      "  âœ… src/\n",
      "  âœ… data/\n",
      "  âœ… models/\n",
      "  âœ… outputs/\n",
      "  âœ… notebooks/\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Add src to path for imports\n",
    "import sys\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"\\nKey Directories:\")\n",
    "for dir_name in ['src', 'data', 'models', 'outputs', 'notebooks']:\n",
    "    dir_path = project_root / dir_name\n",
    "    exists = \"âœ…\" if dir_path.exists() else \"âŒ\"\n",
    "    print(f\"  {exists} {dir_name}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Notebook Series\n",
    "\n",
    "### Learning Path\n",
    "\n",
    "Follow these notebooks in order to understand the complete pipeline:\n",
    "\n",
    "#### **01_Data_Loading_and_Exploration.ipynb**\n",
    "**What you'll learn:**\n",
    "- How to load medical conversation data\n",
    "- Dataset statistics and distribution\n",
    "- ICD-10 code analysis\n",
    "- Data quality checks\n",
    "\n",
    "**Key concepts:**\n",
    "- HuggingFace datasets\n",
    "- Train/val/test splitting\n",
    "- Data exploration with pandas\n",
    "\n",
    "---\n",
    "\n",
    "#### **02_Data_Processing_and_Tokenization.ipynb**\n",
    "**What you'll learn:**\n",
    "- How text becomes numbers (tokenization)\n",
    "- Chat templates and formatting\n",
    "- Padding and truncation strategies\n",
    "- Processing pipeline\n",
    "\n",
    "**Key concepts:**\n",
    "- Tokenizers\n",
    "- Special tokens\n",
    "- Sequence length management\n",
    "\n",
    "---\n",
    "\n",
    "#### **03_LLM_Evaluation_ZeroShot.ipynb**\n",
    "**What you'll learn:**\n",
    "- Loading large language models\n",
    "- Zero-shot inference\n",
    "- Evaluation metrics (accuracy, F1, etc.)\n",
    "- Performance analysis\n",
    "\n",
    "**Key concepts:**\n",
    "- Model quantization (4-bit)\n",
    "- Batch inference\n",
    "- Metrics computation\n",
    "\n",
    "---\n",
    "\n",
    "#### **04_SLM_Training_LoRA.ipynb**\n",
    "**What you'll learn:**\n",
    "- What is LoRA and why it's efficient\n",
    "- Setting up training\n",
    "- Training loop explained\n",
    "- Monitoring progress\n",
    "- Saving checkpoints\n",
    "\n",
    "**Key concepts:**\n",
    "- Parameter-efficient finetuning\n",
    "- Learning rate scheduling\n",
    "- Early stopping\n",
    "\n",
    "---\n",
    "\n",
    "#### **05_SLM_Evaluation_Finetuned.ipynb**\n",
    "**What you'll learn:**\n",
    "- Loading finetuned models\n",
    "- Evaluating performance\n",
    "- Comparing before/after finetuning\n",
    "- Understanding improvements\n",
    "\n",
    "**Key concepts:**\n",
    "- Model loading from checkpoints\n",
    "- Adapter merging\n",
    "- Performance metrics\n",
    "\n",
    "---\n",
    "\n",
    "#### **06_Results_Analysis_and_Comparison.ipynb**\n",
    "**What you'll learn:**\n",
    "- Statistical analysis of results\n",
    "- Size vs Specialization comparison\n",
    "- Creating publication-quality visualizations\n",
    "- Drawing conclusions\n",
    "\n",
    "**Key concepts:**\n",
    "- Statistical tests\n",
    "- Data visualization\n",
    "- Scientific reporting\n",
    "\n",
    "---\n",
    "\n",
    "#### **07_Custom_Interview_Testing.ipynb**\n",
    "**What you'll learn:**\n",
    "- Interactive model testing\n",
    "- Creating custom test cases\n",
    "- Comparing model predictions\n",
    "- Error analysis\n",
    "\n",
    "**Key concepts:**\n",
    "- Interactive widgets\n",
    "- Real-world testing\n",
    "- Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Expected Results\n",
    "\n",
    "### Performance Metrics We Track\n",
    "\n",
    "**Accuracy Metrics:**\n",
    "- Exact Match Accuracy (full ICD code correct)\n",
    "- Prefix Match (category correct)\n",
    "- Top-K Accuracy (correct code in top predictions)\n",
    "\n",
    "**Statistical Metrics:**\n",
    "- Precision (how many predictions are correct)\n",
    "- Recall (how many actual cases found)\n",
    "- F1 Score (harmonic mean of precision/recall)\n",
    "\n",
    "**Performance Metrics:**\n",
    "- Inference latency (time per prediction)\n",
    "- Throughput (predictions per second)\n",
    "- Memory usage (VRAM required)\n",
    "\n",
    "### Possible Outcomes\n",
    "\n",
    "**Scenario 1: SLMs Win ðŸŽ¯**\n",
    "```\n",
    "Llama 3B (finetuned):  Accuracy: 85%\n",
    "Llama 8B (untrained):  Accuracy: 75%\n",
    "```\n",
    "**Conclusion:** Specialization > Size for medical tasks\n",
    "**Recommendation:** Deploy small finetuned models\n",
    "\n",
    "**Scenario 2: LLMs Win ðŸ“ˆ**\n",
    "```\n",
    "Llama 8B (untrained):  Accuracy: 85%\n",
    "Llama 3B (finetuned):  Accuracy: 75%\n",
    "```\n",
    "**Conclusion:** Size > Specialization, need more parameters\n",
    "**Recommendation:** Use larger models or improve finetuning\n",
    "\n",
    "**Scenario 3: Mixed Results ðŸ¤”**\n",
    "```\n",
    "Common diagnoses:  SLMs better (learned from training data)\n",
    "Rare diagnoses:    LLMs better (general knowledge helps)\n",
    "```\n",
    "**Conclusion:** Different models for different use cases\n",
    "**Recommendation:** Hybrid system or ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways\n",
    "\n",
    "### What Makes This Project Interesting?\n",
    "\n",
    "1. **Practical Research Question**\n",
    "   - Real-world deployment considerations\n",
    "   - Trade-offs everyone faces\n",
    "   - Actionable conclusions\n",
    "\n",
    "2. **Clean Experimental Design**\n",
    "   - Clear comparison groups\n",
    "   - Controlled variables\n",
    "   - Reproducible methodology\n",
    "\n",
    "3. **State-of-the-Art Techniques**\n",
    "   - Parameter-efficient finetuning (LoRA)\n",
    "   - Modern LLMs (Llama 3, Qwen 2.5)\n",
    "   - Best practices (caching, monitoring, etc.)\n",
    "\n",
    "4. **Production-Ready Code**\n",
    "   - Modular architecture\n",
    "   - Proper error handling\n",
    "   - Memory management\n",
    "   - Comprehensive logging\n",
    "\n",
    "### Skills You'll Learn\n",
    "\n",
    "**Machine Learning:**\n",
    "- Language model finetuning\n",
    "- Transfer learning\n",
    "- Evaluation methodologies\n",
    "- Hyperparameter tuning\n",
    "\n",
    "**Engineering:**\n",
    "- GPU optimization\n",
    "- Memory management\n",
    "- Batch processing\n",
    "- Caching strategies\n",
    "\n",
    "**Research:**\n",
    "- Experimental design\n",
    "- Statistical analysis\n",
    "- Scientific visualization\n",
    "- Result interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "### Ready to Start?\n",
    "\n",
    "1. **Make sure environment is set up:**\n",
    "   ```bash\n",
    "   python -c \"import torch; print('CUDA:', torch.cuda.is_available())\"\n",
    "   ```\n",
    "\n",
    "2. **Run the setup cell above** to configure paths\n",
    "\n",
    "3. **Open notebook 01:**\n",
    "   ```\n",
    "   01_Data_Loading_and_Exploration.ipynb\n",
    "   ```\n",
    "\n",
    "4. **Follow along step-by-step!**\n",
    "\n",
    "### Questions?\n",
    "\n",
    "- **Documentation:** See `GUIDE.md` for full project documentation\n",
    "- **Quick Start:** See `QUICKSTART.md` for fast setup\n",
    "- **Architecture:** See `ARCHITECTURE_REDESIGN.md` for design decisions\n",
    "\n",
    "---\n",
    "\n",
    "**Let's dive in!** ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
