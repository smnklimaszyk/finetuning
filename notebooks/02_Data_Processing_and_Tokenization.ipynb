{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Data Processing and Tokenization\n",
    "\n",
    "**Previous:** [01_Data_Loading_and_Exploration.ipynb](01_Data_Loading_and_Exploration.ipynb)  \n",
    "**Next:** [03_LLM_Evaluation_ZeroShot.ipynb](03_LLM_Evaluation_ZeroShot.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Covers\n",
    "\n",
    "In this notebook, we'll explore one of the most critical steps in NLP: **tokenization**.\n",
    "\n",
    "**Key Questions We'll Answer:**\n",
    "1. How does text become numbers that models can understand?\n",
    "2. What are tokens, and how do tokenizers work?\n",
    "3. How do we format doctor-patient conversations for instruction-tuned models?\n",
    "4. What are padding and truncation, and why do we need them?\n",
    "5. How do we create efficient batches for training?\n",
    "\n",
    "**Why This Matters:**\n",
    "- Models don't understand text - they work with numbers\n",
    "- Poor tokenization = poor model performance\n",
    "- Chat templates ensure models interpret conversations correctly\n",
    "- Proper batching makes training 10-100x faster\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Critical for GPU memory management\n",
    "os.environ['PYTORCH_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "print(f\"‚úÖ Project Root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict\n",
    "from collections import Counter\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. What is Tokenization? üî§ ‚Üí üî¢\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Language models are neural networks that perform mathematical operations. They need **numbers**, not **text**.\n",
    "\n",
    "**Tokenization** is the process of converting text into numerical tokens that models can process.\n",
    "\n",
    "### The Process\n",
    "\n",
    "```\n",
    "Text Input:\n",
    "\"I have a sore throat.\"\n",
    "\n",
    "        ‚Üì Tokenization\n",
    "\n",
    "Tokens (pieces):\n",
    "[\"I\", \" have\", \" a\", \" sore\", \" throat\", \".\"]\n",
    "\n",
    "        ‚Üì Convert to IDs\n",
    "\n",
    "Token IDs (numbers):\n",
    "[40, 423, 264, 36366, 28691, 13]\n",
    "\n",
    "        ‚Üì Model processes\n",
    "\n",
    "Embeddings (vectors):\n",
    "[[0.23, -0.45, ...], [0.67, 0.12, ...], ...]\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**Token**: A piece of text (can be a word, subword, or character)\n",
    "- Modern tokenizers use **subword tokenization** (BPE, WordPiece, SentencePiece)\n",
    "- Common words: 1 token (`\"hello\"` ‚Üí `[\"hello\"]`)\n",
    "- Rare words: Multiple tokens (`\"pneumonoultramicroscopicsilicovolcanoconiosis\"` ‚Üí 15+ tokens!)\n",
    "\n",
    "**Vocabulary**: Set of all possible tokens the model knows\n",
    "- Llama 3: ~128,000 tokens\n",
    "- Qwen 2.5: ~151,000 tokens\n",
    "- Trade-off: Larger vocab = more memory, smaller vocab = longer sequences\n",
    "\n",
    "**Special Tokens**: Tokens with special meaning\n",
    "- `<|begin_of_text|>` (BOS): Start of text\n",
    "- `<|end_of_text|>` (EOS): End of text  \n",
    "- `<|pad|>` (PAD): Padding for batching\n",
    "- `<|assistant|>`, `<|user|>`: Role indicators for chat models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading a Tokenizer\n",
    "\n",
    "Each model family has its own tokenizer trained on specific data. We must use the **exact tokenizer** that matches the model.\n",
    "\n",
    "Let's load the tokenizer for Llama 3.2 3B (one of our SLMs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer for Llama 3.2 3B\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"‚úÖ Loaded tokenizer for: {model_name}\")\n",
    "print(f\"\\nVocabulary Size: {len(tokenizer):,} tokens\")\n",
    "print(f\"Model Max Length: {tokenizer.model_max_length:,} tokens\")\n",
    "print(f\"\\nSpecial Tokens:\")\n",
    "print(f\"  BOS (Begin): {tokenizer.bos_token} ‚Üí ID {tokenizer.bos_token_id}\")\n",
    "print(f\"  EOS (End):   {tokenizer.eos_token} ‚Üí ID {tokenizer.eos_token_id}\")\n",
    "print(f\"  PAD (Pad):   {tokenizer.pad_token} ‚Üí ID {tokenizer.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Padding Token\n",
    "\n",
    "Some tokenizers (like Llama) don't have a default padding token. We need to set one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set padding token if not already set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"‚úÖ Set pad_token to eos_token: {tokenizer.pad_token}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Pad token already set: {tokenizer.pad_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Tokenization Examples\n",
    "\n",
    "Let's see tokenization in action with medical examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tokenization(text: str, tokenizer):\n",
    "    \"\"\"\n",
    "    Shows how text gets tokenized step-by-step.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Original Text:\")\n",
    "    print(f\"  '{text}'\")\n",
    "    print(f\"\\nLength: {len(text)} characters\")\n",
    "    \n",
    "    # Tokenize\n",
    "    encoded = tokenizer(text, return_tensors=\"pt\")\n",
    "    token_ids = encoded['input_ids'][0].tolist()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Tokens ({len(tokens)} total):\")\n",
    "    for i, (token, token_id) in enumerate(zip(tokens, token_ids)):\n",
    "        # Clean up token display (Llama uses ƒ† for spaces)\n",
    "        display_token = token.replace('ƒ†', '‚ñÅ')  # Use ‚ñÅ to show spaces\n",
    "        print(f\"  [{i:2d}] {display_token:20s} ‚Üí ID: {token_id:6d}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Token IDs: {token_ids}\")\n",
    "    print(f\"\\nCompression Ratio: {len(text)} chars ‚Üí {len(tokens)} tokens ({len(text)/len(tokens):.1f}x)\")\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Simple Medical Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tokenization(\"I have a sore throat.\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Medical Terminology\n",
    "\n",
    "Watch how complex medical terms get split into multiple tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tokenization(\"Patient presents with acute pharyngitis.\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: ICD-10 Codes\n",
    "\n",
    "ICD codes are crucial for our task - let's see how they tokenize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tokenization(\"Diagnosis: J06.9 (Acute upper respiratory infection)\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Notice:\n",
    "1. **Common words** (\"I\", \"have\", \"a\") are usually single tokens\n",
    "2. **Medical terms** (\"pharyngitis\") may split into subwords (\"phar\" + \"yng\" + \"itis\")\n",
    "3. **ICD codes** (\"J06.9\") typically split into multiple tokens (\"J\", \"06\", \".\", \"9\")\n",
    "4. **Spaces** are part of tokens (shown as ‚ñÅ)\n",
    "\n",
    "This is why specialized medical tokenizers can help - they treat medical terms as single tokens!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chat Templates üí¨\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Our data is **multi-turn conversations** between doctor and patient. How do we represent this?\n",
    "\n",
    "```python\n",
    "# Raw data structure:\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\"role\": \"doctor\", \"content\": \"What brings you here?\"},\n",
    "        {\"role\": \"patient\", \"content\": \"I have a fever.\"}\n",
    "    ],\n",
    "    \"diagnosis\": \"J06.9\"\n",
    "}\n",
    "```\n",
    "\n",
    "We need to convert this into a format the model understands!\n",
    "\n",
    "### Chat Templates\n",
    "\n",
    "Modern instruction-tuned models use **chat templates** - special formatting that indicates roles.\n",
    "\n",
    "**Llama 3 Chat Template:**\n",
    "```\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a medical diagnosis assistant.<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Doctor: What brings you here?\n",
    "Patient: I have a fever.<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "J06.9<|eot_id|>\n",
    "```\n",
    "\n",
    "The template tells the model:\n",
    "- Where each message starts/ends\n",
    "- Who is speaking (system/user/assistant)\n",
    "- Where to generate the response\n",
    "\n",
    "### Applying Chat Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example conversation\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a medical diagnosis assistant. Predict the ICD-10 code based on the doctor-patient conversation.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Doctor: What brings you here today?\\nPatient: I have a sore throat and fever for 3 days.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"J06.9\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Apply chat template\n",
    "formatted_text = tokenizer.apply_chat_template(\n",
    "    conversation,\n",
    "    tokenize=False,  # Get text first (not token IDs)\n",
    "    add_generation_prompt=False\n",
    ")\n",
    "\n",
    "print(\"Raw Conversation:\")\n",
    "for msg in conversation:\n",
    "    print(f\"  [{msg['role']:9s}] {msg['content'][:50]}...\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Formatted with Chat Template:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(formatted_text)\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Template + Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now tokenize the formatted text\n",
    "encoded = tokenizer.apply_chat_template(\n",
    "    conversation,\n",
    "    tokenize=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(f\"Token IDs Shape: {encoded.shape}\")\n",
    "print(f\"Total Tokens: {encoded.shape[1]}\")\n",
    "print(f\"\\nFirst 20 Token IDs: {encoded[0][:20].tolist()}\")\n",
    "\n",
    "# Decode to verify\n",
    "decoded = tokenizer.decode(encoded[0])\n",
    "print(f\"\\nDecoded (verify it matches):\")\n",
    "print(decoded[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Processing Our Dataset\n",
    "\n",
    "Now let's apply this to our actual medical conversation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (same as notebook 01)\n",
    "print(\"Loading MedSynth dataset...\")\n",
    "dataset = load_dataset(\n",
    "    \"samhog/medsynth-diagnosis-icd10-10k\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "# Take a subset for faster processing in this demo\n",
    "dataset = dataset.select(range(100))\n",
    "print(f\"‚úÖ Loaded {len(dataset)} examples\")\n",
    "\n",
    "# Look at first example\n",
    "example = dataset[0]\n",
    "print(f\"\\nFirst Example:\")\n",
    "print(f\"  Diagnosis: {example['diagnosis']}\")\n",
    "print(f\"  Messages: {len(example['messages'])} turns\")\n",
    "print(f\"\\nFirst 2 messages:\")\n",
    "for msg in example['messages'][:2]:\n",
    "    print(f\"  [{msg['role']:10s}] {msg['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Formatting Function\n",
    "\n",
    "This function converts our dataset format to the chat template format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_conversation(example: Dict, tokenizer) -> Dict:\n",
    "    \"\"\"\n",
    "    Formats a conversation example for instruction tuning.\n",
    "    \n",
    "    Input format:\n",
    "    {\n",
    "        \"messages\": [{\"role\": \"doctor\", \"content\": \"...\"}, ...],\n",
    "        \"diagnosis\": \"J06.9\"\n",
    "    }\n",
    "    \n",
    "    Output format:\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"...\"},\n",
    "        {\"role\": \"user\", \"content\": \"Doctor: ...\\nPatient: ...\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"J06.9\"}\n",
    "    ]\n",
    "    \"\"\"\n",
    "    # System prompt\n",
    "    system_prompt = (\n",
    "        \"You are a medical diagnosis assistant. \"\n",
    "        \"Based on the doctor-patient conversation, predict the ICD-10 diagnosis code.\"\n",
    "    )\n",
    "    \n",
    "    # Format conversation turns\n",
    "    conversation_text = \"\\n\".join([\n",
    "        f\"{msg['role'].capitalize()}: {msg['content']}\"\n",
    "        for msg in example['messages']\n",
    "    ])\n",
    "    \n",
    "    # Build chat format\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": conversation_text},\n",
    "        {\"role\": \"assistant\", \"content\": example['diagnosis']}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template and tokenize\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        chat,\n",
    "        tokenize=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": formatted[0],\n",
    "        \"labels\": formatted[0].clone(),  # For training, labels = input_ids\n",
    "        \"diagnosis\": example['diagnosis']\n",
    "    }\n",
    "\n",
    "# Test on first example\n",
    "formatted_example = format_conversation(example, tokenizer)\n",
    "print(f\"‚úÖ Formatted Example:\")\n",
    "print(f\"  Input IDs Shape: {formatted_example['input_ids'].shape}\")\n",
    "print(f\"  Labels Shape: {formatted_example['labels'].shape}\")\n",
    "print(f\"  Diagnosis: {formatted_example['diagnosis']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Formatting\n",
    "\n",
    "Let's decode the tokens to verify the formatting is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = tokenizer.decode(formatted_example['input_ids'])\n",
    "print(\"Decoded Formatted Text:\")\n",
    "print(\"=\"*70)\n",
    "print(decoded_text)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Padding and Truncation\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Neural networks require **fixed-size inputs**. But our conversations have different lengths!\n",
    "\n",
    "```\n",
    "Conversation 1: \"I have a cold.\"              ‚Üí 15 tokens\n",
    "Conversation 2: \"I have chest pain...\" (long) ‚Üí 247 tokens\n",
    "Conversation 3: \"Fever.\"                      ‚Üí 8 tokens\n",
    "```\n",
    "\n",
    "We can't feed these directly to a model - they must all be the same length.\n",
    "\n",
    "### Solution: Padding\n",
    "\n",
    "**Padding** adds special `<|pad|>` tokens to make all sequences the same length:\n",
    "\n",
    "```\n",
    "Target length: 512 tokens\n",
    "\n",
    "Conversation 1: [tokens...] + [PAD] * 497  ‚Üí 512 tokens\n",
    "Conversation 2: [tokens...] + [PAD] * 265  ‚Üí 512 tokens\n",
    "Conversation 3: [tokens...] + [PAD] * 504  ‚Üí 512 tokens\n",
    "```\n",
    "\n",
    "### Solution: Truncation\n",
    "\n",
    "**Truncation** cuts off tokens that exceed the maximum length:\n",
    "\n",
    "```\n",
    "Very long conversation: 1024 tokens\n",
    "Max length: 512 tokens\n",
    "\n",
    "Truncated: [first 512 tokens]  # Last 512 tokens discarded\n",
    "```\n",
    "\n",
    "### Analyzing Sequence Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all examples to get token lengths\n",
    "print(\"Analyzing token lengths across dataset...\\n\")\n",
    "\n",
    "token_lengths = []\n",
    "for example in dataset:\n",
    "    formatted = format_conversation(example, tokenizer)\n",
    "    # Count non-padding tokens\n",
    "    non_pad_tokens = (formatted['input_ids'] != tokenizer.pad_token_id).sum().item()\n",
    "    token_lengths.append(non_pad_tokens)\n",
    "\n",
    "# Statistics\n",
    "token_lengths = np.array(token_lengths)\n",
    "print(f\"Token Length Statistics:\")\n",
    "print(f\"  Min:     {token_lengths.min():4d} tokens\")\n",
    "print(f\"  Max:     {token_lengths.max():4d} tokens\")\n",
    "print(f\"  Mean:    {token_lengths.mean():6.1f} tokens\")\n",
    "print(f\"  Median:  {np.median(token_lengths):6.1f} tokens\")\n",
    "print(f\"  Std Dev: {token_lengths.std():6.1f} tokens\")\n",
    "print(f\"\\nPercentiles:\")\n",
    "for p in [50, 75, 90, 95, 99]:\n",
    "    print(f\"  {p:2d}%: {np.percentile(token_lengths, p):6.1f} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "ax1.hist(token_lengths, bins=30, edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(token_lengths.mean(), color='red', linestyle='--', label=f'Mean: {token_lengths.mean():.0f}')\n",
    "ax1.axvline(np.median(token_lengths), color='green', linestyle='--', label=f'Median: {np.median(token_lengths):.0f}')\n",
    "ax1.set_xlabel('Number of Tokens')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Distribution of Tokenized Sequence Lengths')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "ax2.boxplot(token_lengths, vert=True)\n",
    "ax2.set_ylabel('Number of Tokens')\n",
    "ax2.set_title('Token Length Box Plot')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° Insight: Most conversations are between {np.percentile(token_lengths, 25):.0f}-{np.percentile(token_lengths, 75):.0f} tokens\")\n",
    "print(f\"           A max_length of 512 tokens captures {(token_lengths <= 512).sum() / len(token_lengths) * 100:.1f}% of conversations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Max Length\n",
    "\n",
    "Based on the distribution, we choose `max_length=512` because:\n",
    "1. Captures most conversations without truncation\n",
    "2. Balances between memory efficiency and information retention\n",
    "3. Standard size for many LLMs (powers of 2 are efficient)\n",
    "\n",
    "**Trade-offs:**\n",
    "- **Smaller** (256): Faster training, but truncates more data\n",
    "- **Larger** (1024): Preserves more data, but 2x memory and slower\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Batching for Efficiency\n",
    "\n",
    "### Why Batching?\n",
    "\n",
    "Processing one example at a time is **extremely slow**:\n",
    "\n",
    "```\n",
    "Sequential processing (batch_size=1):\n",
    "  Example 1: 50ms\n",
    "  Example 2: 50ms\n",
    "  ...\n",
    "  Total for 1000 examples: 50,000ms = 50 seconds\n",
    "\n",
    "Batched processing (batch_size=32):\n",
    "  Batch 1 (32 examples): 100ms\n",
    "  Batch 2 (32 examples): 100ms\n",
    "  ...\n",
    "  Total for 1000 examples: ~3,200ms = 3.2 seconds\n",
    "```\n",
    "\n",
    "**15x speedup!** üöÄ\n",
    "\n",
    "### How Batching Works\n",
    "\n",
    "A **batch** is a group of examples processed together:\n",
    "\n",
    "```python\n",
    "# Single example\n",
    "input_ids: [512]          # Shape: (sequence_length,)\n",
    "\n",
    "# Batch of 32 examples  \n",
    "input_ids: [32, 512]      # Shape: (batch_size, sequence_length)\n",
    "```\n",
    "\n",
    "GPUs are designed for **parallel processing** - they excel at matrix operations on batches!\n",
    "\n",
    "### Creating Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process first 8 examples into a batch\n",
    "batch_size = 8\n",
    "batch_examples = dataset.select(range(batch_size))\n",
    "\n",
    "# Format each example\n",
    "formatted_batch = [format_conversation(ex, tokenizer) for ex in batch_examples]\n",
    "\n",
    "# Stack into tensors\n",
    "batch_input_ids = torch.stack([ex['input_ids'] for ex in formatted_batch])\n",
    "batch_labels = torch.stack([ex['labels'] for ex in formatted_batch])\n",
    "batch_diagnoses = [ex['diagnosis'] for ex in formatted_batch]\n",
    "\n",
    "print(f\"Batch Created:\")\n",
    "print(f\"  Input IDs Shape: {batch_input_ids.shape}\")\n",
    "print(f\"  Labels Shape:    {batch_labels.shape}\")\n",
    "print(f\"  Batch Size:      {batch_input_ids.shape[0]}\")\n",
    "print(f\"  Sequence Length: {batch_input_ids.shape[1]}\")\n",
    "print(f\"\\nDiagnoses in batch: {batch_diagnoses}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Padding in a Batch\n",
    "\n",
    "Let's see where the padding tokens are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mask: True where token is NOT padding\n",
    "attention_mask = (batch_input_ids != tokenizer.pad_token_id).int()\n",
    "\n",
    "# Count real tokens per example\n",
    "real_tokens = attention_mask.sum(dim=1)\n",
    "\n",
    "print(\"Tokens per example in batch:\")\n",
    "for i, (n_tokens, diagnosis) in enumerate(zip(real_tokens, batch_diagnoses)):\n",
    "    padding = batch_input_ids.shape[1] - n_tokens.item()\n",
    "    print(f\"  Example {i}: {n_tokens:3d} real tokens + {padding:3d} padding = 512 total | Diagnosis: {diagnosis}\")\n",
    "\n",
    "# Visualize as heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(attention_mask.numpy(), aspect='auto', cmap='RdYlGn', interpolation='nearest')\n",
    "plt.colorbar(label='Token Type', ticks=[0, 1])\n",
    "plt.clim(0, 1)\n",
    "plt.xlabel('Token Position')\n",
    "plt.ylabel('Example in Batch')\n",
    "plt.title('Batch Attention Mask (Green=Real Token, Red=Padding)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° Notice: Different examples have different amounts of padding!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. The Complete Processing Pipeline\n",
    "\n",
    "Let's put everything together into a reusable data processing class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalDataProcessor:\n",
    "    \"\"\"\n",
    "    Complete data processing pipeline for medical diagnosis.\n",
    "    \n",
    "    Handles:\n",
    "    1. Loading tokenizer\n",
    "    2. Formatting conversations with chat templates\n",
    "    3. Tokenization\n",
    "    4. Padding/truncation\n",
    "    5. Batching\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, max_length: int = 512):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        print(f\"‚úÖ Initialized MedicalDataProcessor\")\n",
    "        print(f\"   Model: {model_name}\")\n",
    "        print(f\"   Max Length: {max_length}\")\n",
    "        print(f\"   Vocab Size: {len(self.tokenizer):,}\")\n",
    "    \n",
    "    def format_example(self, example: Dict) -> Dict:\n",
    "        \"\"\"Format a single example.\"\"\"\n",
    "        system_prompt = (\n",
    "            \"You are a medical diagnosis assistant. \"\n",
    "            \"Based on the doctor-patient conversation, predict the ICD-10 diagnosis code.\"\n",
    "        )\n",
    "        \n",
    "        conversation_text = \"\\n\".join([\n",
    "            f\"{msg['role'].capitalize()}: {msg['content']}\"\n",
    "            for msg in example['messages']\n",
    "        ])\n",
    "        \n",
    "        chat = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": conversation_text},\n",
    "            {\"role\": \"assistant\", \"content\": example['diagnosis']}\n",
    "        ]\n",
    "        \n",
    "        formatted = self.tokenizer.apply_chat_template(\n",
    "            chat,\n",
    "            tokenize=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": formatted[0],\n",
    "            \"labels\": formatted[0].clone(),\n",
    "            \"diagnosis\": example['diagnosis']\n",
    "        }\n",
    "    \n",
    "    def process_dataset(self, dataset, batch_size: int = 32):\n",
    "        \"\"\"Process entire dataset into batches.\"\"\"\n",
    "        print(f\"\\nProcessing {len(dataset)} examples...\")\n",
    "        \n",
    "        # Format all examples\n",
    "        formatted = [self.format_example(ex) for ex in dataset]\n",
    "        \n",
    "        # Create batches\n",
    "        batches = []\n",
    "        for i in range(0, len(formatted), batch_size):\n",
    "            batch_data = formatted[i:i+batch_size]\n",
    "            batch = {\n",
    "                'input_ids': torch.stack([ex['input_ids'] for ex in batch_data]),\n",
    "                'labels': torch.stack([ex['labels'] for ex in batch_data]),\n",
    "                'diagnoses': [ex['diagnosis'] for ex in batch_data]\n",
    "            }\n",
    "            batches.append(batch)\n",
    "        \n",
    "        print(f\"‚úÖ Created {len(batches)} batches of size {batch_size}\")\n",
    "        return batches\n",
    "\n",
    "# Test the processor\n",
    "processor = MedicalDataProcessor(model_name=\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "batches = processor.process_dataset(dataset, batch_size=16)\n",
    "\n",
    "print(f\"\\nFirst Batch:\")\n",
    "print(f\"  Input IDs: {batches[0]['input_ids'].shape}\")\n",
    "print(f\"  Labels:    {batches[0]['labels'].shape}\")\n",
    "print(f\"  Diagnoses: {batches[0]['diagnoses'][:3]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Key Takeaways üí°\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Tokenization Basics**\n",
    "   - Text ‚Üí Tokens ‚Üí Token IDs\n",
    "   - Subword tokenization handles rare words\n",
    "   - Each model has its own vocabulary\n",
    "\n",
    "2. **Chat Templates**\n",
    "   - Format multi-turn conversations for instruction models\n",
    "   - Include special tokens for roles (system/user/assistant)\n",
    "   - Critical for model to understand conversation structure\n",
    "\n",
    "3. **Padding & Truncation**\n",
    "   - Neural networks need fixed-size inputs\n",
    "   - Padding fills short sequences\n",
    "   - Truncation cuts long sequences\n",
    "   - Choose `max_length` based on data distribution\n",
    "\n",
    "4. **Batching**\n",
    "   - Process multiple examples in parallel\n",
    "   - 10-100x speedup on GPUs\n",
    "   - Larger batches = faster, but more memory\n",
    "\n",
    "### Why This Matters for Our Project\n",
    "\n",
    "**For Training:**\n",
    "- Proper tokenization ensures model learns meaningful patterns\n",
    "- Chat templates teach model to generate diagnoses after conversations\n",
    "- Batching makes training feasible (hours instead of days)\n",
    "\n",
    "**For Evaluation:**\n",
    "- Consistent formatting between training and inference\n",
    "- Same tokenizer must be used for same model\n",
    "- Efficient batching speeds up evaluation\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "‚ùå **Wrong tokenizer**: Model won't understand inputs  \n",
    "‚ùå **No chat template**: Model confused about conversation structure  \n",
    "‚ùå **Wrong max_length**: Either truncate too much or waste memory  \n",
    "‚ùå **No padding token**: Batching fails  \n",
    "‚ùå **Batch size too large**: CUDA out of memory  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. What's Next? üëâ\n",
    "\n",
    "Now that we understand how to process our data, we're ready to:\n",
    "\n",
    "1. **Evaluate Large Language Models (LLMs)** - Test 7-8B models zero-shot\n",
    "   - Load models with quantization\n",
    "   - Run inference on test set\n",
    "   - Measure baseline performance\n",
    "\n",
    "2. **Train Small Language Models (SLMs)** - Finetune 3B models with LoRA\n",
    "   - Set up LoRA adapters\n",
    "   - Run training loop\n",
    "   - Monitor progress\n",
    "\n",
    "3. **Compare Results** - Does specialization beat size?\n",
    "   - Accuracy metrics\n",
    "   - Speed comparison\n",
    "   - Memory usage\n",
    "\n",
    "**Next Notebook:** [03_LLM_Evaluation_ZeroShot.ipynb](03_LLM_Evaluation_ZeroShot.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "- ‚úÖ What tokenization is and why it's needed\n",
    "- ‚úÖ How tokenizers work (vocabulary, subwords, special tokens)\n",
    "- ‚úÖ Chat templates for multi-turn conversations\n",
    "- ‚úÖ Padding and truncation strategies\n",
    "- ‚úÖ Batching for GPU efficiency\n",
    "- ‚úÖ Complete data processing pipeline\n",
    "\n",
    "**Key Files in Project:**\n",
    "- `src/data_processing/processor.py` - Similar to our `MedicalDataProcessor`\n",
    "- `src/config/base_config.py` - Contains `max_length`, `batch_size` settings\n",
    "\n",
    "---\n",
    "\n",
    "**Continue to:** [03_LLM_Evaluation_ZeroShot.ipynb](03_LLM_Evaluation_ZeroShot.ipynb) üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
