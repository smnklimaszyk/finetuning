# üöÄ RTX 5090 Optimization - Quick Reference

## What Changed

### üîß Configuration Changes

| Parameter | Before | After | Reason |
|-----------|--------|-------|--------|
| `slm_load_in_8bit` | `True` | `False` | Native BF16 is faster |
| `slm_load_in_4bit` | `False` | `False` | No quantization needed |
| `attn_implementation` | *(not set)* | `"flash_attention_2"` | 2-3x attention speedup |
| `optimizer` | `"adamw"` | `"adamw_torch_fused"` | 10-15% faster updates |
| `per_device_train_batch_size` | `16` | `32` | 2x throughput |
| `per_device_eval_batch_size` | `32` | `64` | 2x eval speed |
| `gradient_accumulation_steps` | `2` | `1` | Less overhead |
| `warmup_steps` | `200` | `100` | Faster convergence |
| `num_workers` | `16` | `12` | Prevent CPU thrashing |
| `torch_compile` | *(not set)* | `True` | 20-40% graph optimization |
| TF32 | *(not set)* | **Auto-enabled** | 10-20% hardware boost |

---

## üí™ Expected Results

- **Training Speed**: 3-5x faster
- **Time per Epoch**: ~6-8h ‚Üí ~1.5-2h
- **GPU Utilization**: 75-90%
- **VRAM Usage**: ~28-30GB / 32GB
- **Quality**: Same or better (no quantization)

---

## ‚ö° Action Items

### 1. Install Flash Attention 2
```bash
pip install flash-attn --no-build-isolation
```

### 2. Update Model Loading Code
```python
model = AutoModelForCausalLM.from_pretrained(
    config.model.slm_name,
    torch_dtype=torch.bfloat16,
    device_map="cuda",
    attn_implementation="flash_attention_2",  # ADD THIS
)
```

### 3. Enable torch.compile
```python
if config.training.torch_compile:
    model = torch.compile(model)  # ADD THIS
```

### 4. TF32 Auto-Enabled
```python
config = get_config()
config.setup()  # Automatically enables TF32
```

---

## üõ°Ô∏è Fallback Plan (If OOM)

```python
# Reduce batch size
per_device_train_batch_size = 24
gradient_accumulation_steps = 2
```

---

## üìä Monitor This

```bash
# GPU utilization (should be 75-90%)
watch -n 1 nvidia-smi

# Training logs (should show ~2-3.5 steps/sec)
tail -f outputs/logs/training.log
```

---

## ‚úÖ Validation Checklist

- [ ] Flash Attention installed
- [ ] Model loads with `attn_implementation="flash_attention_2"`
- [ ] `torch.compile(model)` applied
- [ ] GPU utilization 75-90%
- [ ] Training speed 2-3x faster
- [ ] No OOM errors
- [ ] Eval metrics comparable or better

---

**Optimization Stack**: Native BF16 + Flash Attn 2 + Fused AdamW + torch.compile + TF32 + Max Batches

**Result**: ~4x faster training with same quality üöÄ
