{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - SLM Training with LoRA\n",
    "\n",
    "**Previous:** [03_LLM_Evaluation_ZeroShot.ipynb](03_LLM_Evaluation_ZeroShot.ipynb)  \n",
    "**Next:** [05_SLM_Evaluation_Finetuned.ipynb](05_SLM_Evaluation_Finetuned.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Covers\n",
    "\n",
    "This is the **heart of our project** - training small language models (3B parameters) to become medical diagnosis specialists!\n",
    "\n",
    "**Key Questions:**\n",
    "1. What is LoRA and why is it revolutionary?\n",
    "2. How does LoRA reduce training costs by 95%+?\n",
    "3. How do we set up and configure LoRA?\n",
    "4. What happens during the training loop?\n",
    "5. How do we monitor and interpret training progress?\n",
    "\n",
    "**Models We'll Train:**\n",
    "- **Llama 3.2 3B** (Meta's efficient small model)\n",
    "- **Qwen 2.5 3B** (Alibaba's competitive alternative)\n",
    "\n",
    "**Why This Matters:**\n",
    "- Transforms general models into medical specialists\n",
    "- Tests if specialization can beat size\n",
    "- Practical for real-world deployment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Critical for GPU memory management\n",
    "os.environ['PYTORCH_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "print(f\"‚úÖ Project Root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ CUDA Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  CUDA not available - training will be VERY slow on CPU!\")\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Understanding LoRA üéØ\n",
    "\n",
    "### The Traditional Finetuning Problem\n",
    "\n",
    "**Full Finetuning:**\n",
    "```\n",
    "3B parameter model:\n",
    "  ‚Ä¢ 3,000,000,000 parameters to update\n",
    "  ‚Ä¢ Requires ~12 GB VRAM (model weights)\n",
    "  ‚Ä¢ Requires ~24 GB VRAM (optimizer states, gradients)\n",
    "  ‚Ä¢ Total: ~36-40 GB VRAM needed\n",
    "  ‚Ä¢ Training time: ~8-12 hours on consumer GPU\n",
    "```\n",
    "\n",
    "**Problems:**\n",
    "- ‚ùå Most GPUs don't have 40 GB VRAM\n",
    "- ‚ùå Very slow (must update billions of parameters)\n",
    "- ‚ùå Expensive (need powerful hardware)\n",
    "- ‚ùå Risk of catastrophic forgetting\n",
    "\n",
    "### LoRA: Low-Rank Adaptation\n",
    "\n",
    "**Key Insight:** Most weight updates during finetuning are **low-rank**!\n",
    "\n",
    "Instead of updating the entire weight matrix `W`, we learn a small update:\n",
    "\n",
    "```\n",
    "Traditional Update:\n",
    "  W_new = W_original + ŒîW\n",
    "  \n",
    "  Where ŒîW is [4096 √ó 4096] = 16,777,216 parameters ‚ùå\n",
    "\n",
    "LoRA Update:\n",
    "  W_new = W_original + A √ó B\n",
    "  \n",
    "  Where:\n",
    "    A is [4096 √ó rank] = 4096 √ó 64 = 262,144 parameters\n",
    "    B is [rank √ó 4096] = 64 √ó 4096 = 262,144 parameters\n",
    "    Total: 524,288 parameters (97% reduction!) ‚úÖ\n",
    "```\n",
    "\n",
    "### Visual Explanation\n",
    "\n",
    "```\n",
    "Original Weight Matrix W [4096 √ó 4096]:\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                         ‚îÇ\n",
    "‚îÇ     16M parameters      ‚îÇ\n",
    "‚îÇ     (frozen ‚ùÑÔ∏è)          ‚îÇ\n",
    "‚îÇ                         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "LoRA Decomposition:\n",
    "        Matrix A              Matrix B\n",
    "        [4096√ó64]            [64√ó4096]\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ    ‚îÇ              ‚îÇ                 ‚îÇ\n",
    "‚îÇ A  ‚îÇ      √ó       ‚îÇ       B         ‚îÇ\n",
    "‚îÇ    ‚îÇ              ‚îÇ                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "262K params         262K params\n",
    "(trainable üî•)      (trainable üî•)\n",
    "\n",
    "Final Update:\n",
    "  W' = W + A √ó B\n",
    "```\n",
    "\n",
    "### The Mathematics\n",
    "\n",
    "**Forward Pass:**\n",
    "```\n",
    "Output = (W + A √ó B) √ó Input\n",
    "       = W √ó Input + A √ó B √ó Input\n",
    "       = Original_Output + LoRA_Adaptation\n",
    "```\n",
    "\n",
    "**Key Parameters:**\n",
    "- **rank (r)**: Dimension of low-rank matrices (typically 8, 16, 32, or 64)\n",
    "  - Lower rank: Fewer parameters, faster training, may limit capacity\n",
    "  - Higher rank: More parameters, slower training, more expressive\n",
    "  \n",
    "- **alpha (Œ±)**: Scaling factor for LoRA updates\n",
    "  - Update is scaled by Œ±/r\n",
    "  - Typically set to rank or 2√órank\n",
    "  \n",
    "- **target_modules**: Which layers to apply LoRA to\n",
    "  - Usually attention layers (q_proj, k_proj, v_proj, o_proj)\n",
    "  - Can include MLP layers (gate_proj, up_proj, down_proj)\n",
    "\n",
    "### LoRA Benefits\n",
    "\n",
    "**Memory:**\n",
    "```\n",
    "Full Finetuning:  40 GB VRAM\n",
    "LoRA:             6-8 GB VRAM  (5-7x reduction!)\n",
    "```\n",
    "\n",
    "**Speed:**\n",
    "```\n",
    "Full Finetuning:  12 hours\n",
    "LoRA:             2-3 hours    (4-6x faster!)\n",
    "```\n",
    "\n",
    "**Storage:**\n",
    "```\n",
    "Full Model:       12 GB\n",
    "LoRA Adapters:    100-200 MB   (60-120x smaller!)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the Dataset\n",
    "\n",
    "First, let's load and split our medical conversation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"Loading MedSynth dataset...\")\n",
    "dataset = load_dataset(\"samhog/medsynth-diagnosis-icd10-10k\", split=\"train\")\n",
    "\n",
    "# Split into train/val/test (70/15/15)\n",
    "train_test_split = dataset.train_test_split(test_size=0.3, seed=42)\n",
    "val_test_split = train_test_split['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "train_dataset = train_test_split['train']\n",
    "val_dataset = val_test_split['train']\n",
    "test_dataset = val_test_split['test']\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset Split:\")\n",
    "print(f\"   Train: {len(train_dataset):,} examples\")\n",
    "print(f\"   Val:   {len(val_dataset):,} examples\")\n",
    "print(f\"   Test:  {len(test_dataset):,} examples\")\n",
    "\n",
    "# Show example\n",
    "example = train_dataset[0]\n",
    "print(f\"\\nExample Training Case:\")\n",
    "print(f\"  Diagnosis: {example['diagnosis']}\")\n",
    "print(f\"  Messages: {len(example['messages'])} turns\")\n",
    "for msg in example['messages'][:2]:\n",
    "    print(f\"    {msg['role']:8s}: {msg['content'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Loading the Base Model\n",
    "\n",
    "We'll load Llama 3.2 3B with 4-bit quantization to save memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to finetune\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "print(f\"Loading base model: {model_name}\")\n",
    "print(\"This may take 1-2 minutes...\\n\")\n",
    "\n",
    "# Quantization config (same as LLM evaluation)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # Important for causal LM training\n",
    "print(f\"‚úÖ Tokenizer loaded\")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(f\"‚úÖ Base model loaded and quantized\")\n",
    "\n",
    "# Check memory\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    print(f\"\\nGPU Memory: {allocated:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Model for LoRA Training\n",
    "\n",
    "4-bit models need special preparation before training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(\"‚úÖ Model prepared for k-bit training\")\n",
    "print(\"\\n   What this does:\")\n",
    "print(\"   ‚Ä¢ Enables gradient checkpointing (saves memory)\")\n",
    "print(\"   ‚Ä¢ Casts layernorm to float32 (stability)\")\n",
    "print(\"   ‚Ä¢ Enables input embedding gradients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Configuring LoRA\n",
    "\n",
    "Now the crucial step - configuring LoRA parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=64,                          # Rank of LoRA matrices (higher = more capacity)\n",
    "    lora_alpha=128,                # Scaling factor (typically 2√órank)\n",
    "    target_modules=[               # Which layers to apply LoRA to\n",
    "        \"q_proj\",                  # Query projection (attention)\n",
    "        \"k_proj\",                  # Key projection (attention)\n",
    "        \"v_proj\",                  # Value projection (attention)\n",
    "        \"o_proj\",                  # Output projection (attention)\n",
    "        \"gate_proj\",               # Gate projection (MLP)\n",
    "        \"up_proj\",                 # Up projection (MLP)\n",
    "        \"down_proj\"                # Down projection (MLP)\n",
    "    ],\n",
    "    lora_dropout=0.05,             # Dropout for regularization\n",
    "    bias=\"none\",                   # Don't train bias terms\n",
    "    task_type=TaskType.CAUSAL_LM   # Task type (causal language modeling)\n",
    ")\n",
    "\n",
    "print(\"LoRA Configuration:\")\n",
    "print(f\"  Rank (r):          {lora_config.r}\")\n",
    "print(f\"  Alpha (Œ±):         {lora_config.lora_alpha}\")\n",
    "print(f\"  Scaling (Œ±/r):     {lora_config.lora_alpha / lora_config.r}\")\n",
    "print(f\"  Target Modules:    {len(lora_config.target_modules)} types\")\n",
    "print(f\"  Dropout:           {lora_config.lora_dropout}\")\n",
    "print(f\"\\n  Expected trainable params: ~0.3% of total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply LoRA to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA configuration to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"‚úÖ LoRA applied to model\\n\")\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Detailed breakdown\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"\\nDetailed Breakdown:\")\n",
    "print(f\"  Total parameters:     {total:,}\")\n",
    "print(f\"  Trainable parameters: {trainable:,}\")\n",
    "print(f\"  Percentage trainable: {trainable/total*100:.2f}%\")\n",
    "print(f\"\\n  Memory saved: ~{(total - trainable) / total * 100:.0f}% reduction in optimizer memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Architecture\n",
    "\n",
    "Let's inspect which modules got LoRA adapters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count LoRA modules\n",
    "lora_modules = [name for name, module in model.named_modules() if 'lora' in name.lower()]\n",
    "\n",
    "print(f\"LoRA Modules Added: {len(lora_modules)}\\n\")\n",
    "\n",
    "# Show first 10 as examples\n",
    "print(\"Example LoRA modules:\")\n",
    "for name in lora_modules[:10]:\n",
    "    print(f\"  ‚Ä¢ {name}\")\n",
    "\n",
    "if len(lora_modules) > 10:\n",
    "    print(f\"  ... and {len(lora_modules) - 10} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Data Preprocessing\n",
    "\n",
    "Format our conversations for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_training(example: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Format a conversation example for training.\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        \"You are a medical diagnosis assistant. \"\n",
    "        \"Based on the doctor-patient conversation, predict the ICD-10 diagnosis code.\"\n",
    "    )\n",
    "    \n",
    "    # Format conversation\n",
    "    conversation_text = \"\\n\".join([\n",
    "        f\"{msg['role'].capitalize()}: {msg['content']}\"\n",
    "        for msg in example['messages']\n",
    "    ])\n",
    "    \n",
    "    # Build chat\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": conversation_text},\n",
    "        {\"role\": \"assistant\", \"content\": example['diagnosis']}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": formatted[0],\n",
    "        \"labels\": formatted[0].clone()  # For causal LM, labels = input_ids\n",
    "    }\n",
    "\n",
    "print(\"Preprocessing datasets...\\n\")\n",
    "\n",
    "# Process datasets\n",
    "train_formatted = [format_for_training(ex) for ex in tqdm(train_dataset, desc=\"Train\")]\n",
    "val_formatted = [format_for_training(ex) for ex in tqdm(val_dataset, desc=\"Val\")]\n",
    "\n",
    "print(f\"\\n‚úÖ Preprocessing complete\")\n",
    "print(f\"   Train: {len(train_formatted)} examples\")\n",
    "print(f\"   Val:   {len(val_formatted)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create PyTorch Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MedicalDataset(Dataset):\n",
    "    \"\"\"Simple dataset wrapper for formatted examples.\"\"\"\n",
    "    \n",
    "    def __init__(self, formatted_examples: List[Dict]):\n",
    "        self.examples = formatted_examples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "train_torch_dataset = MedicalDataset(train_formatted)\n",
    "val_torch_dataset = MedicalDataset(val_formatted)\n",
    "\n",
    "print(f\"‚úÖ PyTorch datasets created\")\n",
    "print(f\"   Train size: {len(train_torch_dataset)}\")\n",
    "print(f\"   Val size:   {len(val_torch_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Training Configuration\n",
    "\n",
    "Set up training hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory for checkpoints\n",
    "output_dir = project_root / \"models\" / \"llama-3.2-3b-medical-lora\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    # Output\n",
    "    output_dir=str(output_dir),\n",
    "    \n",
    "    # Training schedule\n",
    "    num_train_epochs=3,                    # Number of passes through dataset\n",
    "    per_device_train_batch_size=8,        # Batch size per GPU\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,         # Effective batch size = 8 √ó 4 = 32\n",
    "    \n",
    "    # Learning rate\n",
    "    learning_rate=2e-4,                    # LoRA typically uses higher LR than full finetuning\n",
    "    lr_scheduler_type=\"cosine\",            # Cosine decay schedule\n",
    "    warmup_ratio=0.03,                     # 3% warmup\n",
    "    \n",
    "    # Optimization\n",
    "    optim=\"adamw_torch\",                   # AdamW optimizer\n",
    "    weight_decay=0.01,                     # L2 regularization\n",
    "    max_grad_norm=1.0,                     # Gradient clipping\n",
    "    \n",
    "    # Precision\n",
    "    bf16=True,                             # Use bfloat16 (faster on modern GPUs)\n",
    "    \n",
    "    # Evaluation\n",
    "    evaluation_strategy=\"steps\",           # Evaluate periodically\n",
    "    eval_steps=100,                        # Evaluate every 100 steps\n",
    "    \n",
    "    # Saving\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,                        # Save checkpoint every 200 steps\n",
    "    save_total_limit=3,                    # Keep only 3 best checkpoints\n",
    "    load_best_model_at_end=True,           # Load best checkpoint at end\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=10,                      # Log every 10 steps\n",
    "    logging_dir=str(output_dir / \"logs\"),\n",
    "    report_to=[],                          # Disable wandb/tensorboard for demo\n",
    "    \n",
    "    # Memory optimization\n",
    "    gradient_checkpointing=True,           # Trade compute for memory\n",
    "    \n",
    "    # Misc\n",
    "    seed=42,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"\\nSchedule:\")\n",
    "print(f\"  Epochs:                 {training_args.num_train_epochs}\")\n",
    "print(f\"  Steps per epoch:        ~{len(train_torch_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")\n",
    "print(f\"  Total training steps:   ~{len(train_torch_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")\n",
    "\n",
    "print(f\"\\nBatch Size:\")\n",
    "print(f\"  Per device:             {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation:  {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size:   {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "\n",
    "print(f\"\\nLearning Rate:\")\n",
    "print(f\"  Initial LR:             {training_args.learning_rate}\")\n",
    "print(f\"  Scheduler:              {training_args.lr_scheduler_type}\")\n",
    "print(f\"  Warmup:                 {training_args.warmup_ratio*100:.0f}% of steps\")\n",
    "\n",
    "print(f\"\\nOutput:\")\n",
    "print(f\"  Checkpoint dir:         {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Training Loop\n",
    "\n",
    "Now let's train! This uses HuggingFace Trainer for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for dynamic padding\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Not masked language modeling (we're doing causal LM)\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_torch_dataset,\n",
    "    eval_dataset=val_torch_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer created\")\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Starting Training...\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training!\n",
    "# This will take 2-3 hours depending on GPU\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"‚úÖ Training Complete!\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Print training summary\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Total time:           {train_result.metrics['train_runtime']:.0f} seconds\")\n",
    "print(f\"  Samples per second:   {train_result.metrics['train_samples_per_second']:.2f}\")\n",
    "print(f\"  Final loss:           {train_result.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Training Metrics\n",
    "\n",
    "**Loss:** How \"wrong\" the model's predictions are\n",
    "- Lower is better\n",
    "- Should decrease over training\n",
    "- Typical range: 0.5-2.0 for well-trained models\n",
    "\n",
    "**Learning Rate Schedule:**\n",
    "```\n",
    "LR\n",
    "‚îÇ     \n",
    "‚îÇ   ‚ï±‚îÄ‚îÄ‚ï≤\n",
    "‚îÇ  ‚ï±    ‚ï≤___\n",
    "‚îÇ ‚ï±         ‚ï≤___\n",
    "‚îÇ‚ï±              ‚ï≤____\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Steps\n",
    "  Warmup    Cosine Decay\n",
    "```\n",
    "\n",
    "**Gradient Accumulation:**\n",
    "- Batch size 8, accumulate 4 steps\n",
    "- Effective batch size: 32\n",
    "- Updates every 4 forward passes\n",
    "- Allows larger effective batch size with limited memory\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizing Training Progress\n",
    "\n",
    "Let's plot the training metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training logs\n",
    "import json\n",
    "\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Extract metrics\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "learning_rates = []\n",
    "steps = []\n",
    "\n",
    "for entry in log_history:\n",
    "    if 'loss' in entry:  # Training step\n",
    "        steps.append(entry['step'])\n",
    "        train_losses.append(entry['loss'])\n",
    "        learning_rates.append(entry.get('learning_rate', None))\n",
    "    if 'eval_loss' in entry:  # Evaluation step\n",
    "        eval_losses.append((entry['step'], entry['eval_loss']))\n",
    "\n",
    "print(f\"Training logs: {len(train_losses)} training steps, {len(eval_losses)} evaluation steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Training loss\n",
    "ax1.plot(steps, train_losses, label='Training Loss', color='#3498db', linewidth=2)\n",
    "if eval_losses:\n",
    "    eval_steps, eval_vals = zip(*eval_losses)\n",
    "    ax1.plot(eval_steps, eval_vals, label='Validation Loss', color='#e74c3c', linewidth=2, marker='o')\n",
    "ax1.set_xlabel('Training Steps')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Progress: Loss Over Time')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "lr_steps = [s for s, lr in zip(steps, learning_rates) if lr is not None]\n",
    "lr_vals = [lr for lr in learning_rates if lr is not None]\n",
    "ax2.plot(lr_steps, lr_vals, label='Learning Rate', color='#2ecc71', linewidth=2)\n",
    "ax2.set_xlabel('Training Steps')\n",
    "ax2.set_ylabel('Learning Rate')\n",
    "ax2.set_title('Learning Rate Schedule')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° What to look for:\")\n",
    "print(\"   ‚Ä¢ Loss should decrease steadily\")\n",
    "print(\"   ‚Ä¢ Validation loss should track training loss\")\n",
    "print(\"   ‚Ä¢ LR should warm up then decay\")\n",
    "print(\"   ‚Ä¢ If validation >> training, model is overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Saving the Model\n",
    "\n",
    "Save the LoRA adapters (not the full model!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_dir = output_dir / \"final_model\"\n",
    "\n",
    "# Save LoRA adapters\n",
    "model.save_pretrained(final_model_dir)\n",
    "tokenizer.save_pretrained(final_model_dir)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {final_model_dir}\")\n",
    "\n",
    "# Check file sizes\n",
    "import os\n",
    "\n",
    "total_size = 0\n",
    "for root, dirs, files in os.walk(final_model_dir):\n",
    "    for file in files:\n",
    "        filepath = os.path.join(root, file)\n",
    "        total_size += os.path.getsize(filepath)\n",
    "\n",
    "print(f\"\\nModel Files:\")\n",
    "for file in os.listdir(final_model_dir):\n",
    "    filepath = final_model_dir / file\n",
    "    if filepath.is_file():\n",
    "        size = filepath.stat().st_size / 1024**2  # MB\n",
    "        print(f\"  ‚Ä¢ {file:30s} {size:8.1f} MB\")\n",
    "\n",
    "print(f\"\\n  Total size: {total_size / 1024**2:.1f} MB\")\n",
    "print(f\"\\n  Compare to full model: ~12,000 MB\")\n",
    "print(f\"  Space saved: {(1 - total_size / (12000 * 1024**2)) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Saved?\n",
    "\n",
    "**LoRA adapters only:**\n",
    "- `adapter_model.safetensors` - LoRA weights (A and B matrices)\n",
    "- `adapter_config.json` - LoRA configuration\n",
    "- Tokenizer files\n",
    "\n",
    "**NOT saved:**\n",
    "- Base model weights (these stay frozen)\n",
    "- You load base model + adapters later\n",
    "\n",
    "**Loading later:**\n",
    "```python\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "# Load LoRA adapters\n",
    "model = PeftModel.from_pretrained(base_model, \"path/to/adapters\")\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Quick Test\n",
    "\n",
    "Let's test the finetuned model on a validation example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on validation example\n",
    "test_example = val_dataset[0]\n",
    "\n",
    "# Format for inference\n",
    "system_prompt = (\n",
    "    \"You are a medical diagnosis assistant. \"\n",
    "    \"Based on the doctor-patient conversation below, predict ONLY the ICD-10 diagnosis code. \"\n",
    "    \"Respond with just the code (e.g., 'J06.9'), nothing else.\"\n",
    ")\n",
    "\n",
    "conversation_text = \"\\n\".join([\n",
    "    f\"{msg['role'].capitalize()}: {msg['content']}\"\n",
    "    for msg in test_example['messages']\n",
    "])\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": conversation_text}\n",
    "]\n",
    "\n",
    "formatted = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Tokenize and generate\n",
    "inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=10,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "# Decode\n",
    "generated = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "prediction = tokenizer.decode(generated, skip_special_tokens=True).strip()\n",
    "\n",
    "print(\"Test Prediction:\")\n",
    "print(f\"\\nConversation snippet:\")\n",
    "for msg in test_example['messages'][:2]:\n",
    "    print(f\"  {msg['role']:8s}: {msg['content'][:60]}...\")\n",
    "\n",
    "print(f\"\\nGround Truth: {test_example['diagnosis']}\")\n",
    "print(f\"Prediction:   {prediction}\")\n",
    "print(f\"\\nMatch: {'‚úÖ CORRECT!' if prediction.split()[0] == test_example['diagnosis'] else '‚ùå Incorrect'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Key Takeaways üí°\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **LoRA is Revolutionary**\n",
    "   - 97% fewer trainable parameters\n",
    "   - 5-7x less VRAM needed\n",
    "   - 4-6x faster training\n",
    "   - 99%+ smaller checkpoint files\n",
    "\n",
    "2. **How LoRA Works**\n",
    "   - Low-rank decomposition: `W' = W + A √ó B`\n",
    "   - Freeze original weights, train small adapters\n",
    "   - Apply to attention and MLP layers\n",
    "\n",
    "3. **Training Process**\n",
    "   - Load base model with quantization\n",
    "   - Apply LoRA configuration\n",
    "   - Format data with chat templates\n",
    "   - Train with HuggingFace Trainer\n",
    "   - Monitor loss and learning rate\n",
    "   - Save adapters (not full model)\n",
    "\n",
    "4. **Key Hyperparameters**\n",
    "   - **rank (r)**: Controls adapter capacity (64 is good default)\n",
    "   - **alpha**: Scaling factor (typically 2√órank)\n",
    "   - **learning_rate**: Higher for LoRA (2e-4) than full finetuning (5e-5)\n",
    "   - **batch_size**: Effective size matters (use gradient accumulation)\n",
    "\n",
    "### Training Best Practices\n",
    "\n",
    "‚úÖ **Do:**\n",
    "- Use 4-bit quantization to save memory\n",
    "- Apply LoRA to attention + MLP layers\n",
    "- Use cosine LR schedule with warmup\n",
    "- Monitor both training and validation loss\n",
    "- Save checkpoints regularly\n",
    "\n",
    "‚ùå **Don't:**\n",
    "- Set rank too low (<16) or too high (>128)\n",
    "- Use tiny batch sizes without accumulation\n",
    "- Skip gradient clipping\n",
    "- Ignore validation loss (overfitting!)\n",
    "- Train for too many epochs (3-5 usually enough)\n",
    "\n",
    "### Expected Results\n",
    "\n",
    "**Training Time:**\n",
    "- RTX 5090 (31GB): ~2-3 hours\n",
    "- RTX 4090 (24GB): ~3-4 hours  \n",
    "- RTX 3090 (24GB): ~4-6 hours\n",
    "\n",
    "**Final Loss:**\n",
    "- Training loss: ~0.5-1.0\n",
    "- Validation loss: ~0.6-1.2\n",
    "- If validation >> training: reduce epochs or add dropout\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Memory Cleanup\n",
    "\n",
    "Free GPU memory before moving to next notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del model\n",
    "del tokenizer\n",
    "del trainer\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "print(\"‚úÖ Memory freed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. What's Next? üëâ\n",
    "\n",
    "We've successfully finetuned a 3B model! Now:\n",
    "\n",
    "1. **Evaluate Finetuned Model** - How much did performance improve?\n",
    "   - Compare with zero-shot LLM baseline\n",
    "   - Measure exact accuracy, F1, etc.\n",
    "   - Analyze error patterns\n",
    "\n",
    "2. **Compare Results** - Does specialization beat size?\n",
    "   - Finetuned 3B vs Untrained 8B\n",
    "   - Performance vs Speed vs Memory\n",
    "   - Visualize trade-offs\n",
    "\n",
    "3. **Test on Custom Cases** - Real-world testing\n",
    "   - Your own medical conversations\n",
    "   - Interactive comparison\n",
    "\n",
    "**Next Notebook:** [05_SLM_Evaluation_Finetuned.ipynb](05_SLM_Evaluation_Finetuned.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "- ‚úÖ Understood LoRA and low-rank adaptation\n",
    "- ‚úÖ Configured LoRA with optimal hyperparameters\n",
    "- ‚úÖ Prepared model for k-bit training\n",
    "- ‚úÖ Preprocessed medical conversation data\n",
    "- ‚úÖ Trained with HuggingFace Trainer\n",
    "- ‚úÖ Monitored training progress\n",
    "- ‚úÖ Saved LoRA adapters\n",
    "- ‚úÖ Tested finetuned model\n",
    "\n",
    "**Key Files in Project:**\n",
    "- `src/training/trainer.py` - Training logic and LoRA setup\n",
    "- `src/config/base_config.py` - LoRA and training hyperparameters\n",
    "- `models/*/` - Saved LoRA adapters and checkpoints\n",
    "\n",
    "---\n",
    "\n",
    "**Continue to:** [05_SLM_Evaluation_Finetuned.ipynb](05_SLM_Evaluation_Finetuned.ipynb) üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
