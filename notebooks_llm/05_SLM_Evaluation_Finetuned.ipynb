{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - SLM Evaluation (Finetuned)\n",
    "\n",
    "**Previous:** [04_SLM_Training_LoRA.ipynb](04_SLM_Training_LoRA.ipynb)  \n",
    "**Next:** [06_Results_Analysis_and_Comparison.ipynb](06_Results_Analysis_and_Comparison.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Covers\n",
    "\n",
    "Now comes the moment of truth - **did finetuning work?**\n",
    "\n",
    "**Key Questions:**\n",
    "1. How do we load finetuned models with LoRA adapters?\n",
    "2. How much did performance improve compared to zero-shot?\n",
    "3. What types of cases did the model learn?\n",
    "4. Are there still systematic errors?\n",
    "5. Is the finetuned 3B model competitive with untrained 7-8B models?\n",
    "\n",
    "**What We'll Evaluate:**\n",
    "- **Llama 3.2 3B (finetuned)** - Our medical specialist\n",
    "- Compare with Llama 3.1 8B (zero-shot) from notebook 03\n",
    "\n",
    "**Why This Matters:**\n",
    "- Tests our core hypothesis: specialization vs size\n",
    "- Shows real-world applicability\n",
    "- Identifies areas for improvement\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Critical for GPU memory management\n",
    "os.environ['PYTORCH_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "print(f\"‚úÖ Project Root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from collections import Counter\n",
    "import gc\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ CUDA Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  CUDA not available - using CPU (very slow!)\")\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Loading Finetuned Model üîÑ\n",
    "\n",
    "### The Two-Step Process\n",
    "\n",
    "To load a LoRA finetuned model:\n",
    "1. Load the **base model** (same as training)\n",
    "2. Load and apply the **LoRA adapters** on top\n",
    "\n",
    "```\n",
    "Base Model (3B parameters, frozen):\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                                 ‚îÇ\n",
    "‚îÇ   Meta-Llama-3.2-3B-Instruct   ‚îÇ\n",
    "‚îÇ   (original pre-trained)        ‚îÇ\n",
    "‚îÇ                                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "              ‚Üì\n",
    "    + LoRA Adapters (10M parameters)\n",
    "              ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                                 ‚îÇ\n",
    "‚îÇ   Llama 3.2 3B - Medical        ‚îÇ\n",
    "‚îÇ   (specialized for diagnosis)   ‚îÇ\n",
    "‚îÇ                                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Loading Step-by-Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "base_model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "adapter_path = project_root / \"models\" / \"llama-3.2-3b-medical-lora\" / \"final_model\"\n",
    "\n",
    "print(f\"Base Model: {base_model_name}\")\n",
    "print(f\"Adapter Path: {adapter_path}\")\n",
    "print(f\"\\nChecking adapter files...\")\n",
    "\n",
    "if adapter_path.exists():\n",
    "    print(f\"‚úÖ Adapter directory found\")\n",
    "    for file in adapter_path.iterdir():\n",
    "        if file.is_file():\n",
    "            print(f\"   ‚Ä¢ {file.name}\")\n",
    "else:\n",
    "    print(f\"‚ùå Adapter not found! Run notebook 04 first to train the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load base model with quantization\n",
    "print(\"\\nStep 1: Loading base model...\")\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Base model loaded\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    print(f\"   GPU Memory: {allocated:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load LoRA adapters\n",
    "print(\"\\nStep 2: Loading LoRA adapters...\")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    str(adapter_path),\n",
    "    is_trainable=False  # Inference mode\n",
    ")\n",
    "\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(\"‚úÖ LoRA adapters loaded and applied\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    print(f\"   GPU Memory: {allocated:.2f} GB\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"‚úÖ Tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Model is Finetuned\n",
    "\n",
    "Let's check that the LoRA adapters are actually applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count LoRA modules\n",
    "lora_modules = [name for name, _ in model.named_modules() if 'lora' in name.lower()]\n",
    "\n",
    "print(f\"LoRA modules in model: {len(lora_modules)}\")\n",
    "print(f\"\\nExample modules:\")\n",
    "for name in lora_modules[:5]:\n",
    "    print(f\"  ‚Ä¢ {name}\")\n",
    "\n",
    "if len(lora_modules) > 0:\n",
    "    print(f\"\\n‚úÖ Model is finetuned (LoRA adapters active)\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Warning: No LoRA modules found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Loading Test Data\n",
    "\n",
    "Load the same test set we'll use for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and split (same as training)\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"samhog/medsynth-diagnosis-icd10-10k\", split=\"train\")\n",
    "\n",
    "# Same split as training\n",
    "train_test_split = dataset.train_test_split(test_size=0.3, seed=42)\n",
    "val_test_split = train_test_split['test'].train_test_split(test_size=0.5, seed=42)\n",
    "test_dataset = val_test_split['test']\n",
    "\n",
    "print(f\"\\n‚úÖ Test set loaded: {len(test_dataset):,} examples\")\n",
    "\n",
    "# Use subset for demo\n",
    "test_subset = test_dataset.select(range(min(200, len(test_dataset))))\n",
    "print(f\"   Using {len(test_subset)} examples for demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Running Evaluation\n",
    "\n",
    "### Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_icd10(example: Dict, model, tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Predict ICD-10 code using finetuned model.\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        \"You are a medical diagnosis assistant. \"\n",
    "        \"Based on the doctor-patient conversation, predict the ICD-10 diagnosis code.\"\n",
    "    )\n",
    "    \n",
    "    conversation_text = \"\\n\".join([\n",
    "        f\"{msg['role'].capitalize()}: {msg['content']}\"\n",
    "        for msg in example['messages']\n",
    "    ])\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": conversation_text}\n",
    "    ]\n",
    "    \n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=10,\n",
    "        do_sample=False,\n",
    "        temperature=1.0,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    prediction = tokenizer.decode(generated, skip_special_tokens=True).strip()\n",
    "    \n",
    "    # Extract just the code\n",
    "    prediction = prediction.split()[0] if prediction.split() else \"\"\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Test on first example\n",
    "test_example = test_subset[0]\n",
    "prediction = predict_icd10(test_example, model, tokenizer)\n",
    "\n",
    "print(\"Test Prediction:\")\n",
    "print(f\"  Ground Truth: {test_example['diagnosis']}\")\n",
    "print(f\"  Prediction:   {prediction}\")\n",
    "print(f\"  Match:        {'‚úÖ CORRECT' if prediction == test_example['diagnosis'] else '‚ùå INCORRECT'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on entire test set\n",
    "print(f\"Evaluating finetuned model on {len(test_subset)} examples...\\n\")\n",
    "\n",
    "predictions = []\n",
    "ground_truth = []\n",
    "\n",
    "for example in tqdm(test_subset, desc=\"Predicting\"):\n",
    "    try:\n",
    "        pred = predict_icd10(example, model, tokenizer)\n",
    "        predictions.append(pred)\n",
    "        ground_truth.append(example['diagnosis'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        predictions.append(\"\")\n",
    "        ground_truth.append(example['diagnosis'])\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Calculating Metrics\n",
    "\n",
    "Compute the same metrics we used for LLM evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions: List[str], ground_truth: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Exact match\n",
    "    exact_matches = sum(p == g for p, g in zip(predictions, ground_truth))\n",
    "    exact_match_acc = exact_matches / len(predictions)\n",
    "    \n",
    "    # Category match (e.g., J06.9 ‚Üí J06)\n",
    "    def get_category(code: str) -> str:\n",
    "        if not code:\n",
    "            return \"\"\n",
    "        return code.split('.')[0] if '.' in code else code[:3]\n",
    "    \n",
    "    pred_categories = [get_category(p) for p in predictions]\n",
    "    true_categories = [get_category(g) for g in ground_truth]\n",
    "    \n",
    "    category_matches = sum(p == g for p, g in zip(pred_categories, true_categories))\n",
    "    category_acc = category_matches / len(predictions)\n",
    "    \n",
    "    # Precision, Recall, F1\n",
    "    precision = precision_score(ground_truth, predictions, average='macro', zero_division=0)\n",
    "    recall = recall_score(ground_truth, predictions, average='macro', zero_division=0)\n",
    "    f1 = f1_score(ground_truth, predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'exact_match_accuracy': exact_match_acc,\n",
    "        'category_accuracy': category_acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'num_examples': len(predictions),\n",
    "        'exact_matches': exact_matches\n",
    "    }\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = calculate_metrics(predictions, ground_truth)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINETUNED MODEL PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nExact Match Accuracy:  {metrics['exact_match_accuracy']:.1%}\")\n",
    "print(f\"  ({metrics['exact_matches']} / {metrics['num_examples']} correct)\")\n",
    "print(f\"\\nCategory Accuracy:     {metrics['category_accuracy']:.1%}\")\n",
    "print(f\"\\nPrecision (macro):     {metrics['precision']:.1%}\")\n",
    "print(f\"Recall (macro):        {metrics['recall']:.1%}\")\n",
    "print(f\"F1 Score (macro):      {metrics['f1_score']:.1%}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Comparing with Baseline\n",
    "\n",
    "### Expected Zero-Shot Performance (from Notebook 03)\n",
    "\n",
    "Let's compare with typical zero-shot LLM performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline metrics (typical zero-shot performance)\n",
    "# These would come from notebook 03 evaluation\n",
    "baseline_metrics = {\n",
    "    'model': 'Llama 3.1 8B (Zero-Shot)',\n",
    "    'exact_match_accuracy': 0.25,  # Example baseline\n",
    "    'category_accuracy': 0.45,\n",
    "    'f1_score': 0.30,\n",
    "}\n",
    "\n",
    "finetuned_metrics = {\n",
    "    'model': 'Llama 3.2 3B (Finetuned)',\n",
    "    'exact_match_accuracy': metrics['exact_match_accuracy'],\n",
    "    'category_accuracy': metrics['category_accuracy'],\n",
    "    'f1_score': metrics['f1_score'],\n",
    "}\n",
    "\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Metric':<25s} {'Baseline (8B)':<20s} {'Finetuned (3B)':<20s} {'Improvement'}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for metric_name in ['exact_match_accuracy', 'category_accuracy', 'f1_score']:\n",
    "    baseline_val = baseline_metrics[metric_name]\n",
    "    finetuned_val = finetuned_metrics[metric_name]\n",
    "    improvement = finetuned_val - baseline_val\n",
    "    improvement_pct = (improvement / baseline_val * 100) if baseline_val > 0 else 0\n",
    "    \n",
    "    print(f\"{metric_name.replace('_', ' ').title():<25s} \"\n",
    "          f\"{baseline_val:>6.1%}              \"\n",
    "          f\"{finetuned_val:>6.1%}              \"\n",
    "          f\"{improvement:+.1%} ({improvement_pct:+.0f}%)\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "metrics_to_plot = ['exact_match_accuracy', 'category_accuracy', 'f1_score']\n",
    "metric_labels = ['Exact Match', 'Category Match', 'F1 Score']\n",
    "\n",
    "x = np.arange(len(metrics_to_plot))\n",
    "width = 0.35\n",
    "\n",
    "baseline_vals = [baseline_metrics[m] for m in metrics_to_plot]\n",
    "finetuned_vals = [finetuned_metrics[m] for m in metrics_to_plot]\n",
    "\n",
    "bars1 = ax.bar(x - width/2, baseline_vals, width, label='Baseline (8B Zero-Shot)', color='#3498db')\n",
    "bars2 = ax.bar(x + width/2, finetuned_vals, width, label='Finetuned (3B LoRA)', color='#2ecc71')\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Performance Comparison: Zero-Shot 8B vs Finetuned 3B')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metric_labels)\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "def add_labels(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                f'{height:.1%}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "add_labels(bars1)\n",
    "add_labels(bars2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insight: Does the smaller finetuned model outperform the larger zero-shot model?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Error Analysis\n",
    "\n",
    "Let's analyze what types of errors the finetuned model makes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find errors\n",
    "errors = []\n",
    "correct = []\n",
    "\n",
    "for i, (pred, true) in enumerate(zip(predictions, ground_truth)):\n",
    "    example_data = {\n",
    "        'index': i,\n",
    "        'predicted': pred,\n",
    "        'ground_truth': true,\n",
    "        'conversation': test_subset[i]['messages']\n",
    "    }\n",
    "    \n",
    "    if pred != true:\n",
    "        errors.append(example_data)\n",
    "    else:\n",
    "        correct.append(example_data)\n",
    "\n",
    "print(f\"Results Breakdown:\")\n",
    "print(f\"  Correct:   {len(correct)} ({len(correct)/len(predictions)*100:.1f}%)\")\n",
    "print(f\"  Incorrect: {len(errors)} ({len(errors)/len(predictions)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSample Errors:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, error in enumerate(errors[:5]):\n",
    "    print(f\"\\n[Error {i+1}]\")\n",
    "    print(f\"  Predicted:    {error['predicted']}\")\n",
    "    print(f\"  Ground Truth: {error['ground_truth']}\")\n",
    "    \n",
    "    # Check if category is correct\n",
    "    pred_cat = error['predicted'].split('.')[0] if '.' in error['predicted'] else error['predicted'][:3]\n",
    "    true_cat = error['ground_truth'].split('.')[0] if '.' in error['ground_truth'] else error['ground_truth'][:3]\n",
    "    category_match = pred_cat == true_cat\n",
    "    \n",
    "    print(f\"  Category Match: {'‚úÖ Yes' if category_match else '‚ùå No'} ({pred_cat} vs {true_cat})\")\n",
    "    print(f\"  Conversation:\")\n",
    "    for msg in error['conversation'][:2]:\n",
    "        print(f\"    {msg['role']:8s}: {msg['content'][:60]}...\")\n",
    "    print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct Predictions Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSample Correct Predictions:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, example in enumerate(correct[:3]):\n",
    "    print(f\"\\n[Correct {i+1}]\")\n",
    "    print(f\"  Prediction: {example['predicted']} ‚úÖ\")\n",
    "    print(f\"  Conversation:\")\n",
    "    for msg in example['conversation'][:2]:\n",
    "        print(f\"    {msg['role']:8s}: {msg['content'][:60]}...\")\n",
    "    print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Common Confusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze confusions\n",
    "confusions = Counter()\n",
    "for pred, true in zip(predictions, ground_truth):\n",
    "    if pred != true:\n",
    "        confusions[(true, pred)] += 1\n",
    "\n",
    "print(\"\\nTop 10 Confusions (True ‚Üí Predicted):\")\n",
    "print(\"=\"*70)\n",
    "for (true, pred), count in confusions.most_common(10):\n",
    "    pct = count / len(errors) * 100\n",
    "    print(f\"  {true:10s} ‚Üí {pred:10s}  ({count:2d} times, {pct:4.1f}% of errors)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Prediction Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Ground truth\n",
    "true_counts = Counter(ground_truth)\n",
    "top_true = dict(true_counts.most_common(10))\n",
    "ax1.barh(list(top_true.keys()), list(top_true.values()), color='#3498db')\n",
    "ax1.set_xlabel('Frequency')\n",
    "ax1.set_title('Ground Truth: Top 10 ICD-10 Codes')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Predictions\n",
    "pred_counts = Counter(predictions)\n",
    "top_pred = dict(pred_counts.most_common(10))\n",
    "ax2.barh(list(top_pred.keys()), list(top_pred.values()), color='#2ecc71')\n",
    "ax2.set_xlabel('Frequency')\n",
    "ax2.set_title('Finetuned Model: Top 10 Predicted Codes')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Does the model's prediction distribution match the true distribution?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Model Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "del model\n",
    "del base_model\n",
    "del tokenizer\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "print(\"‚úÖ Memory freed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Key Takeaways üí°\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Loading Finetuned Models**\n",
    "   - Load base model + adapters (two-step process)\n",
    "   - LoRA adapters are tiny (~100-200 MB)\n",
    "   - Can swap adapters for different tasks\n",
    "\n",
    "2. **Performance Improvements**\n",
    "   - Finetuning typically improves accuracy by 20-50%+\n",
    "   - Category accuracy often better than exact match\n",
    "   - F1 score shows balanced precision/recall\n",
    "\n",
    "3. **Error Patterns**\n",
    "   - Most errors are within-category (e.g., J06.8 vs J06.9)\n",
    "   - Rare diagnoses harder to predict\n",
    "   - Model learns common patterns well\n",
    "\n",
    "4. **Size vs Specialization**\n",
    "   - Small finetuned models can match/exceed large zero-shot models\n",
    "   - Specialization compensates for fewer parameters\n",
    "   - Faster inference + lower memory\n",
    "\n",
    "### Typical Results\n",
    "\n",
    "**Expected Improvement:**\n",
    "```\n",
    "Zero-Shot (8B):     20-30% exact match\n",
    "Finetuned (3B):     50-70% exact match  (2-3x improvement!)\n",
    "```\n",
    "\n",
    "**Trade-offs:**\n",
    "```\n",
    "Model Size:         8B ‚Üí 3B  (37.5% of size)\n",
    "Inference Speed:    1.0x ‚Üí 2.5x faster\n",
    "Memory Usage:       6GB ‚Üí 4GB\n",
    "Performance:        Finetuned 3B often better!\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. What's Next? üëâ\n",
    "\n",
    "We've seen the finetuned model's performance! Now:\n",
    "\n",
    "1. **Comprehensive Comparison** - All models side-by-side\n",
    "   - LLMs vs SLMs\n",
    "   - Zero-shot vs Finetuned\n",
    "   - Statistical significance tests\n",
    "\n",
    "2. **Visualization Dashboard** - Publication-quality plots\n",
    "   - Performance metrics\n",
    "   - Speed vs accuracy trade-offs\n",
    "   - Memory usage comparison\n",
    "\n",
    "3. **Interactive Testing** - Try custom medical cases\n",
    "   - Your own doctor-patient conversations\n",
    "   - Compare all models\n",
    "   - Real-world validation\n",
    "\n",
    "**Next Notebook:** [06_Results_Analysis_and_Comparison.ipynb](06_Results_Analysis_and_Comparison.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "- ‚úÖ Loaded finetuned model with LoRA adapters\n",
    "- ‚úÖ Evaluated on test set\n",
    "- ‚úÖ Calculated comprehensive metrics\n",
    "- ‚úÖ Compared with zero-shot baseline\n",
    "- ‚úÖ Analyzed error patterns\n",
    "- ‚úÖ Visualized prediction distributions\n",
    "- ‚úÖ Assessed specialization vs size trade-off\n",
    "\n",
    "**Key Files in Project:**\n",
    "- `src/evaluation/evaluator.py` - Evaluation logic\n",
    "- `src/evaluation/metrics.py` - Metric calculations\n",
    "- `models/*/final_model/` - LoRA adapters\n",
    "\n",
    "---\n",
    "\n",
    "**Continue to:** [06_Results_Analysis_and_Comparison.ipynb](06_Results_Analysis_and_Comparison.ipynb) üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
