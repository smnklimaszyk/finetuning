{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - LLM Evaluation (Zero-Shot)\n",
    "\n",
    "**Previous:** [02_Data_Processing_and_Tokenization.ipynb](02_Data_Processing_and_Tokenization.ipynb)  \n",
    "**Next:** [04_SLM_Training_LoRA.ipynb](04_SLM_Training_LoRA.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Covers\n",
    "\n",
    "In this notebook, we evaluate **Large Language Models (LLMs)** on our medical diagnosis task **without any training**.\n",
    "\n",
    "**Key Questions:**\n",
    "1. What is zero-shot evaluation?\n",
    "2. How do we load large models efficiently (7-8B parameters)?\n",
    "3. What is model quantization and why do we need it?\n",
    "4. How do we run inference and extract predictions?\n",
    "5. How do we measure performance (accuracy, F1, etc.)?\n",
    "\n",
    "**Models We'll Evaluate:**\n",
    "- **Llama 3.1 8B** (Meta's largest instruction-tuned model)\n",
    "- **Mistral 7B** (Popular open-source alternative)\n",
    "\n",
    "**Why This Matters:**\n",
    "- Establishes baseline performance\n",
    "- Tests if large models can solve medical tasks out-of-the-box\n",
    "- Provides comparison point for our finetuned small models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Critical for GPU memory management\n",
    "os.environ['PYTORCH_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "print(f\"‚úÖ Project Root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import gc\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ CUDA Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  CUDA not available - using CPU (very slow!)\")\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Understanding Zero-Shot Evaluation üéØ\n",
    "\n",
    "### What is Zero-Shot?\n",
    "\n",
    "**Zero-shot** means the model has **never seen** examples of this specific task during training.\n",
    "\n",
    "```\n",
    "Traditional ML:\n",
    "  1. Collect labeled data for task X\n",
    "  2. Train model on task X\n",
    "  3. Test model on task X\n",
    "  ‚Üí Model learned from task X examples\n",
    "\n",
    "Zero-Shot Learning:\n",
    "  1. Model pre-trained on general text\n",
    "  2. NO training on task X\n",
    "  3. Test model on task X using instructions only\n",
    "  ‚Üí Model relies on general knowledge\n",
    "```\n",
    "\n",
    "### Why Zero-Shot?\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ No training needed (saves time and compute)\n",
    "- ‚úÖ Works on new tasks immediately\n",
    "- ‚úÖ Generalizes across domains\n",
    "\n",
    "**Disadvantages:**\n",
    "- ‚ùå Often lower performance than finetuned models\n",
    "- ‚ùå May not understand task-specific nuances\n",
    "- ‚ùå Sensitive to prompt wording\n",
    "\n",
    "### Our Experimental Setup\n",
    "\n",
    "We'll evaluate LLMs in zero-shot mode to answer:\n",
    "> **Can large models (7-8B) with no medical training compete with small finetuned models (3B)?**\n",
    "\n",
    "**Prediction:**\n",
    "- LLMs have more parameters ‚Üí more knowledge\n",
    "- But no medical specialization\n",
    "- Will they be good enough?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Quantization üóúÔ∏è\n",
    "\n",
    "### The Memory Problem\n",
    "\n",
    "**Problem:** Large models are HUGE!\n",
    "\n",
    "```\n",
    "Llama 3.1 8B in full precision (float32):\n",
    "  8 billion parameters √ó 4 bytes = 32 GB\n",
    "  + activations, optimizer states = ~40-50 GB total!\n",
    "```\n",
    "\n",
    "Most consumer GPUs: 16-24 GB VRAM ‚Üí **Won't fit!**\n",
    "\n",
    "### Solution: Quantization\n",
    "\n",
    "**Quantization** reduces the precision of model weights:\n",
    "\n",
    "```\n",
    "Float32 (32-bit):  3.14159265358979... (very precise)\n",
    "  ‚Üì Quantize to 8-bit\n",
    "Int8 (8-bit):      3 (less precise, 4x smaller!)\n",
    "  ‚Üì Quantize to 4-bit  \n",
    "Int4 (4-bit):      3 (least precise, 8x smaller!)\n",
    "```\n",
    "\n",
    "**Trade-offs:**\n",
    "\n",
    "| Precision | Memory | Speed | Accuracy |\n",
    "|-----------|--------|-------|----------|\n",
    "| float32   | 32 GB  | 1.0x  | 100%     |\n",
    "| float16   | 16 GB  | 1.5x  | 99.9%    |\n",
    "| int8      | 8 GB   | 2.0x  | 99.5%    |\n",
    "| int4      | 4 GB   | 3.0x  | 98-99%   |\n",
    "\n",
    "**We'll use 4-bit quantization (NF4):**\n",
    "- Llama 8B: 32 GB ‚Üí **4-6 GB** (fits on most GPUs!)\n",
    "- Minimal accuracy loss (~1-2%)\n",
    "- Faster inference\n",
    "\n",
    "### BitsAndBytes NF4\n",
    "\n",
    "**NF4 (4-bit NormalFloat)** is a special quantization format:\n",
    "- Optimized for weights that follow normal distribution\n",
    "- Better than standard int4\n",
    "- Developed by Tim Dettmers (University of Washington)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading a Large Language Model\n",
    "\n",
    "Let's load Llama 3.1 8B with 4-bit quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for 4-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # Enable 4-bit loading\n",
    "    bnb_4bit_quant_type=\"nf4\",            # Use NormalFloat4\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Compute in bfloat16\n",
    "    bnb_4bit_use_double_quant=True        # Double quantization for even more compression\n",
    ")\n",
    "\n",
    "print(\"Quantization Config:\")\n",
    "print(f\"  Quantization Type: NF4 (4-bit)\")\n",
    "print(f\"  Compute Dtype: bfloat16\")\n",
    "print(f\"  Double Quantization: Enabled\")\n",
    "print(f\"\\n  Expected Memory: ~4-6 GB (vs ~32 GB in float32)\")\n",
    "print(f\"  Expected Speedup: ~2-3x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to load\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "print(\"This may take 1-2 minutes...\\n\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "print(f\"‚úÖ Tokenizer loaded\")\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",                    # Automatically distribute across GPUs\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"‚úÖ Model loaded and quantized\")\n",
    "\n",
    "# Check memory usage\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    print(f\"\\nGPU Memory:\")\n",
    "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"  Reserved:  {reserved:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model Statistics:\")\n",
    "print(f\"  Total Parameters:     {total_params:,}\")\n",
    "print(f\"  Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"  Model Size:           ~{total_params / 1e9:.1f}B parameters\")\n",
    "print(f\"\\n  Architecture: {model.config.model_type}\")\n",
    "print(f\"  Hidden Size:  {model.config.hidden_size}\")\n",
    "print(f\"  Num Layers:   {model.config.num_hidden_layers}\")\n",
    "print(f\"  Num Heads:    {model.config.num_attention_heads}\")\n",
    "print(f\"  Vocab Size:   {model.config.vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Loading Test Data\n",
    "\n",
    "Let's load the test set we'll evaluate on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"Loading MedSynth dataset...\")\n",
    "dataset = load_dataset(\"samhog/medsynth-diagnosis-icd10-10k\", split=\"train\")\n",
    "\n",
    "# Split into train/val/test (70/15/15)\n",
    "train_test_split = dataset.train_test_split(test_size=0.3, seed=42)\n",
    "val_test_split = train_test_split['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "train_dataset = train_test_split['train']\n",
    "val_dataset = val_test_split['train']\n",
    "test_dataset = val_test_split['test']\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset Split:\")\n",
    "print(f\"   Train: {len(train_dataset):,} examples\")\n",
    "print(f\"   Val:   {len(val_dataset):,} examples\")\n",
    "print(f\"   Test:  {len(test_dataset):,} examples\")\n",
    "\n",
    "# For this demo, use a subset of test set\n",
    "test_subset = test_dataset.select(range(min(100, len(test_dataset))))\n",
    "print(f\"\\n   Using {len(test_subset)} test examples for demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Running Zero-Shot Inference\n",
    "\n",
    "### The Inference Process\n",
    "\n",
    "```\n",
    "1. Format conversation with chat template\n",
    "   \"Doctor: What brings you here?\\nPatient: I have a fever.\"\n",
    "   \n",
    "2. Tokenize input\n",
    "   [128000, 128006, ...] (token IDs)\n",
    "   \n",
    "3. Generate prediction\n",
    "   Model outputs: [9805, 2705, 13, 24] (new token IDs)\n",
    "   \n",
    "4. Decode prediction\n",
    "   \"J06.9\"\n",
    "   \n",
    "5. Extract ICD-10 code\n",
    "   \"J06.9\" ‚Üí cleaned and validated\n",
    "```\n",
    "\n",
    "### Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_conversation_for_inference(example: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Format a medical conversation for zero-shot inference.\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        \"You are a medical diagnosis assistant. \"\n",
    "        \"Based on the doctor-patient conversation below, predict ONLY the ICD-10 diagnosis code. \"\n",
    "        \"Respond with just the code (e.g., 'J06.9'), nothing else.\"\n",
    "    )\n",
    "    \n",
    "    # Format conversation\n",
    "    conversation_text = \"\\n\".join([\n",
    "        f\"{msg['role'].capitalize()}: {msg['content']}\"\n",
    "        for msg in example['messages']\n",
    "    ])\n",
    "    \n",
    "    # Build chat messages (no assistant response - we want model to generate it)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": conversation_text}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True  # Add assistant prompt for generation\n",
    "    )\n",
    "    \n",
    "    return formatted\n",
    "\n",
    "# Test formatting\n",
    "test_example = test_subset[0]\n",
    "formatted_input = format_conversation_for_inference(test_example)\n",
    "\n",
    "print(\"Example Formatted Input:\")\n",
    "print(\"=\"*70)\n",
    "print(formatted_input[:500] + \"...\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nExpected Output: {test_example['diagnosis']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # Disable gradient computation for inference\n",
    "def predict_icd10(example: Dict, model, tokenizer, max_new_tokens: int = 10) -> str:\n",
    "    \"\"\"\n",
    "    Predict ICD-10 code for a conversation.\n",
    "    \n",
    "    Args:\n",
    "        example: Dataset example with 'messages' field\n",
    "        model: Language model\n",
    "        tokenizer: Tokenizer\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "    \n",
    "    Returns:\n",
    "        Predicted ICD-10 code (string)\n",
    "    \"\"\"\n",
    "    # Format conversation\n",
    "    formatted = format_conversation_for_inference(example)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,           # Deterministic (greedy decoding)\n",
    "        temperature=1.0,\n",
    "        top_p=1.0,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode only the new tokens (not the input)\n",
    "    generated_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    prediction = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Clean prediction (extract just the code)\n",
    "    prediction = prediction.strip()\n",
    "    \n",
    "    # Sometimes models add extra text - extract first word\n",
    "    prediction = prediction.split()[0] if prediction.split() else \"\"\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Test prediction\n",
    "print(\"Testing prediction on first example...\\n\")\n",
    "prediction = predict_icd10(test_example, model, tokenizer)\n",
    "\n",
    "print(f\"Ground Truth: {test_example['diagnosis']}\")\n",
    "print(f\"Prediction:   {prediction}\")\n",
    "print(f\"Match:        {'‚úÖ CORRECT' if prediction == test_example['diagnosis'] else '‚ùå INCORRECT'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Evaluation\n",
    "\n",
    "Now let's evaluate the entire test subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dataset, model, tokenizer) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate model on entire dataset.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with predictions and ground truth\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    \n",
    "    print(f\"Evaluating on {len(dataset)} examples...\\n\")\n",
    "    \n",
    "    for example in tqdm(dataset, desc=\"Predicting\"):\n",
    "        try:\n",
    "            pred = predict_icd10(example, model, tokenizer)\n",
    "            predictions.append(pred)\n",
    "            ground_truth.append(example['diagnosis'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error on example: {e}\")\n",
    "            predictions.append(\"\")  # Empty prediction on error\n",
    "            ground_truth.append(example['diagnosis'])\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'ground_truth': ground_truth\n",
    "    }\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluate_model(test_subset, model, tokenizer)\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluation complete!\")\n",
    "print(f\"   Predictions: {len(results['predictions'])}\")\n",
    "print(f\"   Ground Truth: {len(results['ground_truth'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Calculating Metrics üìä\n",
    "\n",
    "### Metrics We'll Use\n",
    "\n",
    "**Exact Match Accuracy:**\n",
    "```\n",
    "Accuracy = (Correct Predictions) / (Total Predictions)\n",
    "```\n",
    "Example: Predicted \"J06.9\", Ground Truth \"J06.9\" ‚Üí ‚úÖ Correct\n",
    "\n",
    "**Prefix Match Accuracy:**\n",
    "```\n",
    "Category correct = Does predicted category match?\n",
    "```\n",
    "Example: Predicted \"J06.8\", Ground Truth \"J06.9\" ‚Üí ‚úÖ Same category (J06)\n",
    "\n",
    "**Precision, Recall, F1:**\n",
    "- **Precision**: Of predictions for code X, how many were correct?\n",
    "- **Recall**: Of all actual code X cases, how many did we find?\n",
    "- **F1**: Harmonic mean of precision and recall\n",
    "\n",
    "### Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions: List[str], ground_truth: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate comprehensive evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Exact match accuracy\n",
    "    exact_matches = sum(p == g for p, g in zip(predictions, ground_truth))\n",
    "    exact_match_acc = exact_matches / len(predictions)\n",
    "    \n",
    "    # Prefix match (category level, e.g., J06.9 ‚Üí J06)\n",
    "    def get_category(code: str) -> str:\n",
    "        \"\"\"Extract category from ICD-10 code.\"\"\"\n",
    "        if not code:\n",
    "            return \"\"\n",
    "        # Extract letter + first digits (e.g., J06.9 ‚Üí J06)\n",
    "        parts = code.split('.')\n",
    "        return parts[0] if parts else \"\"\n",
    "    \n",
    "    pred_categories = [get_category(p) for p in predictions]\n",
    "    true_categories = [get_category(g) for g in ground_truth]\n",
    "    \n",
    "    category_matches = sum(p == g for p, g in zip(pred_categories, true_categories))\n",
    "    category_acc = category_matches / len(predictions)\n",
    "    \n",
    "    # Precision, Recall, F1 (macro-averaged across codes)\n",
    "    precision = precision_score(ground_truth, predictions, average='macro', zero_division=0)\n",
    "    recall = recall_score(ground_truth, predictions, average='macro', zero_division=0)\n",
    "    f1 = f1_score(ground_truth, predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'exact_match_accuracy': exact_match_acc,\n",
    "        'category_accuracy': category_acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'num_examples': len(predictions)\n",
    "    }\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = calculate_metrics(results['predictions'], results['ground_truth'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nExact Match Accuracy:  {metrics['exact_match_accuracy']:.1%}\")\n",
    "print(f\"Category Accuracy:     {metrics['category_accuracy']:.1%}\")\n",
    "print(f\"\\nPrecision (macro):     {metrics['precision']:.1%}\")\n",
    "print(f\"Recall (macro):        {metrics['recall']:.1%}\")\n",
    "print(f\"F1 Score (macro):      {metrics['f1_score']:.1%}\")\n",
    "print(f\"\\nExamples Evaluated:    {metrics['num_examples']}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis\n",
    "\n",
    "Let's look at some incorrect predictions to understand model behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find errors\n",
    "errors = []\n",
    "for i, (pred, true) in enumerate(zip(results['predictions'], results['ground_truth'])):\n",
    "    if pred != true:\n",
    "        errors.append({\n",
    "            'index': i,\n",
    "            'predicted': pred,\n",
    "            'ground_truth': true,\n",
    "            'conversation': test_subset[i]['messages']\n",
    "        })\n",
    "\n",
    "print(f\"\\nErrors: {len(errors)} / {len(results['predictions'])} ({len(errors)/len(results['predictions'])*100:.1f}%)\\n\")\n",
    "\n",
    "# Show first 5 errors\n",
    "print(\"First 5 Errors:\")\n",
    "print(\"=\"*70)\n",
    "for i, error in enumerate(errors[:5]):\n",
    "    print(f\"\\n[Error {i+1}]\")\n",
    "    print(f\"  Predicted:    {error['predicted']}\")\n",
    "    print(f\"  Ground Truth: {error['ground_truth']}\")\n",
    "    print(f\"  Conversation snippet:\")\n",
    "    for msg in error['conversation'][:2]:  # First 2 messages\n",
    "        print(f\"    {msg['role']:8s}: {msg['content'][:60]}...\")\n",
    "    print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Analysis\n",
    "\n",
    "Which codes does the model confuse most often?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most common confusions\n",
    "from collections import Counter\n",
    "\n",
    "confusions = Counter()\n",
    "for pred, true in zip(results['predictions'], results['ground_truth']):\n",
    "    if pred != true:\n",
    "        confusions[(true, pred)] += 1\n",
    "\n",
    "print(\"\\nTop 10 Confusions (True ‚Üí Predicted):\")\n",
    "print(\"=\"*70)\n",
    "for (true, pred), count in confusions.most_common(10):\n",
    "    print(f\"  {true:10s} ‚Üí {pred:10s}  ({count} times)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Visualizing Results\n",
    "\n",
    "### Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar plot of metrics\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "metric_names = ['Exact Match', 'Category', 'Precision', 'Recall', 'F1']\n",
    "metric_values = [\n",
    "    metrics['exact_match_accuracy'],\n",
    "    metrics['category_accuracy'],\n",
    "    metrics['precision'],\n",
    "    metrics['recall'],\n",
    "    metrics['f1_score']\n",
    "]\n",
    "\n",
    "bars = ax.bar(metric_names, metric_values, color=['#2ecc71', '#3498db', '#9b59b6', '#e74c3c', '#f39c12'])\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title(f'Zero-Shot Performance: {model_name.split(\"/\")[1]}')\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='50% baseline')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, metric_values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "            f'{value:.1%}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distribution of predicted vs ground truth codes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Ground truth distribution\n",
    "true_counts = Counter(results['ground_truth'])\n",
    "top_true = dict(true_counts.most_common(10))\n",
    "ax1.barh(list(top_true.keys()), list(top_true.values()), color='#3498db')\n",
    "ax1.set_xlabel('Frequency')\n",
    "ax1.set_title('Ground Truth: Top 10 ICD-10 Codes')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Predicted distribution  \n",
    "pred_counts = Counter(results['predictions'])\n",
    "top_pred = dict(pred_counts.most_common(10))\n",
    "ax2.barh(list(top_pred.keys()), list(top_pred.values()), color='#e74c3c')\n",
    "ax2.set_xlabel('Frequency')\n",
    "ax2.set_title('Predicted: Top 10 ICD-10 Codes')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Insight: Compare the distributions - does the model over/under-predict certain codes?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Model Cleanup\n",
    "\n",
    "Free GPU memory before loading another model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up model from memory\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "# Aggressive cleanup\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "print(\"‚úÖ Model unloaded and memory freed\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    print(f\"   GPU Memory Allocated: {allocated:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Key Takeaways üí°\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Zero-Shot Evaluation**\n",
    "   - Tests model's general knowledge without task-specific training\n",
    "   - Establishes baseline performance\n",
    "   - Fast to run (no training needed)\n",
    "\n",
    "2. **4-bit Quantization**\n",
    "   - Reduces memory by 8x (32 GB ‚Üí 4 GB)\n",
    "   - Minimal accuracy loss (~1-2%)\n",
    "   - Essential for running large models on consumer GPUs\n",
    "\n",
    "3. **Inference Process**\n",
    "   - Format conversation ‚Üí Tokenize ‚Üí Generate ‚Üí Decode ‚Üí Extract\n",
    "   - Careful prompt engineering matters\n",
    "   - Need to clean/validate outputs\n",
    "\n",
    "4. **Evaluation Metrics**\n",
    "   - Exact match: Most strict (full code must match)\n",
    "   - Category match: More lenient (J06.9 vs J06.8)\n",
    "   - F1/Precision/Recall: Account for class imbalance\n",
    "\n",
    "### Expected Results\n",
    "\n",
    "**Typical Zero-Shot Performance:**\n",
    "- Exact Match: 10-30% (depends on model)\n",
    "- Category Match: 30-50%\n",
    "- F1 Score: 15-35%\n",
    "\n",
    "**Why Not Higher?**\n",
    "- No medical specialization\n",
    "- Limited context about ICD-10 codes\n",
    "- May confuse similar conditions\n",
    "\n",
    "**This is why we finetune!** üéØ\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "‚ùå **CUDA OOM**: Reduce batch size or use smaller model  \n",
    "‚ùå **Slow inference**: Check quantization is enabled  \n",
    "‚ùå **Wrong format**: Verify chat template is correct  \n",
    "‚ùå **Empty predictions**: Model may not understand task - adjust prompt  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. What's Next? üëâ\n",
    "\n",
    "Now we have baseline LLM performance! Next steps:\n",
    "\n",
    "1. **Train Small Models** - Finetune 3B models with LoRA\n",
    "   - Can specialization beat size?\n",
    "   - How much improvement from finetuning?\n",
    "\n",
    "2. **Evaluate Finetuned Models** - Compare with baseline\n",
    "   - Same metrics as zero-shot\n",
    "   - Direct comparison\n",
    "\n",
    "3. **Analyze Trade-offs** - Size vs Specialization\n",
    "   - Performance\n",
    "   - Speed\n",
    "   - Memory\n",
    "\n",
    "**Next Notebook:** [04_SLM_Training_LoRA.ipynb](04_SLM_Training_LoRA.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "- ‚úÖ Understood zero-shot evaluation\n",
    "- ‚úÖ Learned about 4-bit quantization (NF4)\n",
    "- ‚úÖ Loaded large models efficiently (8B ‚Üí 4GB)\n",
    "- ‚úÖ Ran inference on medical conversations\n",
    "- ‚úÖ Calculated comprehensive metrics\n",
    "- ‚úÖ Analyzed errors and confusions\n",
    "- ‚úÖ Visualized results\n",
    "\n",
    "**Key Files in Project:**\n",
    "- `src/models/llm_model.py` - LLM wrapper with quantization\n",
    "- `src/evaluation/metrics.py` - Metric calculation functions\n",
    "- `src/config/base_config.py` - Model configs and quantization settings\n",
    "\n",
    "---\n",
    "\n",
    "**Continue to:** [04_SLM_Training_LoRA.ipynb](04_SLM_Training_LoRA.ipynb) üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
