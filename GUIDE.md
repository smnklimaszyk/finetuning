# üè• Medical Diagnosis Model Finetuning

## Datenschutzkonformes Finetuning eines Small Language Models f√ºr medizinische Diagnoseunterst√ºtzung

---

# üìñ VOLLST√ÑNDIGER ENTWICKLER-GUIDE

Dieser Guide f√ºhrt dich Schritt f√ºr Schritt durch die komplette Entwicklung und Nutzung dieses MLOps-Projekts.
Er ist f√ºr Entwickler geschrieben, die ein tiefes Verst√§ndnis f√ºr Machine Learning Engineering entwickeln m√∂chten.

---

## üìã Inhaltsverzeichnis

1. [Projekt√ºbersicht & Architektur](#1-projekt√ºbersicht--architektur)
2. [Theoretische Grundlagen](#2-theoretische-grundlagen)
3. [Projektstruktur Erkl√§rt](#3-projektstruktur-erkl√§rt)
4. [Konfigurationssystem](#4-konfigurationssystem)
5. [Datenverarbeitungs-Pipeline](#5-datenverarbeitungs-pipeline)
6. [Modell-Architektur](#6-modell-architektur)
7. [Training mit LoRA](#7-training-mit-lora)
8. [Evaluation & Metriken](#8-evaluation--metriken)
9. [Experiment-Workflow](#9-experiment-workflow)
10. [Best Practices & Troubleshooting](#10-best-practices--troubleshooting)

---

## 1. Projekt√ºbersicht & Architektur

### 1.1 Was macht dieses Projekt?

Dieses Projekt entwickelt ein **spezialisiertes KI-Modell** zur Unterst√ºtzung von √Ñrzten bei der Diagnosestellung.
Basierend auf Arzt-Patienten-Dialogen schl√§gt das Modell passende **ICD-10 Diagnose-Codes** vor.

**Der Workflow:**

```
Arzt-Patienten-Dialog ‚Üí KI-Modell ‚Üí ICD-10 Code Vorschlag
```

### 1.2 Experimentelles Design: Gr√∂√üe vs. Spezialisierung

**Zentrale Forschungsfrage:**
> K√∂nnen spezialisierte kleine Modelle (3B) gr√∂√üere untrainierte Modelle (7-8B) schlagen?

Wir vergleichen zwei fundamentale Ans√§tze:

| Ansatz | Beschreibung | Modelle | Vorteil | Nachteil |
|--------|--------------|---------|---------|----------|
| **LLMs (Large, Untrained)** | Gro√üe Modelle (7-8B) ohne Finetuning | Llama 8B, Mistral 7B | Mehr Parameter = mehr Wissen | Langsamer, mehr Ressourcen |
| **SLMs (Small, Finetuned)** | Kleine Modelle (3B) mit LoRA Finetuning | Llama 3B, Qwen 3B | Schneller, spezialisiert, effizient | Trainingsaufwand |

**Was wir testen:**
- **Size Advantage**: Haben 7-8B Modelle genug Wissen f√ºr medizinische Diagnosen?
- **Specialization Advantage**: Kann Finetuning den Gr√∂√üennachteil kompensieren?
- **Deployment Trade-offs**: Performance vs. Effizienz

### 1.3 Architektur-√úbersicht

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    EXPERIMENTAL PIPELINE                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  PHASE 1: LLM Evaluation (Zero-Shot)                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ  Llama 8B  ‚Üí  Zero-Shot Predict  ‚Üí  Evaluate        ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  Mistral 7B ‚Üí Zero-Shot Predict  ‚Üí  Evaluate        ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ                            ‚Üì                                    ‚îÇ
‚îÇ         (Memory Cleanup & GPU Reset)                           ‚îÇ
‚îÇ                            ‚Üì                                    ‚îÇ
‚îÇ  PHASE 2: SLM Finetuning + Evaluation                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ  Llama 3B  ‚Üí  LoRA Train  ‚Üí  Evaluate Finetuned    ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  Qwen 3B   ‚Üí  LoRA Train  ‚Üí  Evaluate Finetuned    ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ                            ‚Üì                                    ‚îÇ
‚îÇ  RESULT: Compare Size vs. Specialization                      ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 2. Experimentelles Design (NEU!)

### 2.1 Die zentrale Forschungsfrage

**Kann Spezialisierung Gr√∂√üe schlagen?**

In diesem Projekt testen wir eine fundamentale Hypothese im Machine Learning:

> K√∂nnen kleine, spezialisierte Modelle (3B Parameter mit Finetuning)
> gr√∂√üere, generelle Modelle (7-8B Parameter ohne Finetuning)
> bei spezifischen Aufgaben √ºbertreffen?

### 2.2 Warum ist das wichtig?

**Praktische Relevanz:**
- **Kosten:** Kleinere Modelle = weniger GPU-Kosten
- **Latenz:** 3B Modelle sind 2-3x schneller als 8B Modelle
- **Datenschutz:** Kleine Modelle k√∂nnen lokal laufen (kein Cloud-Upload)
- **Energie:** Weniger Parameter = weniger Stromverbrauch

**Wissenschaftliche Relevanz:**
- Versteht man die Trade-offs zwischen Gr√∂√üe und Spezialisierung?
- Wann lohnt sich Finetuning?
- Gibt es einen "Sweet Spot" zwischen Gr√∂√üe und Effizienz?

### 2.3 Experimenteller Aufbau

```
LLMs (Size Advantage)           SLMs (Specialization Advantage)
‚îú‚îÄ Llama 8B (untrained)        ‚îú‚îÄ Llama 3B (LoRA finetuned)
‚îî‚îÄ Mistral 7B (untrained)      ‚îî‚îÄ Qwen 3B (LoRA finetuned)

        ‚Üì                               ‚Üì
   Zero-Shot                     Domain-Adapted
   Inference                     Inference
        ‚Üì                               ‚Üì
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ     ICD-10 Code Classification      ‚îÇ
   ‚îÇ        (Medical Diagnosis)          ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì                               ‚Üì
   Performance                     Performance
   Comparison                      Comparison
```

### 2.4 Erwartete Ergebnisse

**M√∂gliches Szenario 1: SLMs gewinnen**
- Finetuning kompensiert Gr√∂√üennachteil
- ‚Üí Empfehlung: Nutze kleine finetuned Modelle in Produktion

**M√∂gliches Szenario 2: LLMs gewinnen**
- Allgemeines Wissen wichtiger als Spezialisierung
- ‚Üí Empfehlung: Nutze gr√∂√üere Modelle auch ohne Finetuning

**M√∂gliches Szenario 3: Nuanciert**
- SLMs besser bei h√§ufigen Diagnosen
- LLMs besser bei seltenen/komplexen F√§llen
- ‚Üí Empfehlung: Hybrid-System je nach Use Case

---

## 3. Theoretische Grundlagen

### 3.1 Was ist Finetuning?

**Finetuning** ist das Anpassen eines vortrainierten Modells auf eine spezifische Aufgabe.

```
Vortrainiertes Modell     Finetuning        Spezialisiertes Modell
(generelles Wissen)    ‚Üí  (+ Domain-Daten) ‚Üí  (+ Dom√§nen-Wissen)
```

**Warum Finetuning statt Training von Grund auf?**

- Vortrainierte Modelle haben bereits Sprachverst√§ndnis gelernt
- Finetuning braucht viel weniger Daten (1000e vs. Milliarden)
- Schneller und g√ºnstiger

### 3.2 Was ist LoRA?

**LoRA (Low-Rank Adaptation)** ist eine effiziente Finetuning-Methode.

**Das Problem:** Normale Finetuning-Methoden √§ndern alle Parameter (Milliarden!).

**Die L√∂sung:** LoRA trainiert nur kleine "Adapter"-Matrizen:

```
Original-Matrix W:    [1000 x 1000] = 1.000.000 Parameter
LoRA-Matrizen A, B:   [1000 x 16] + [16 x 1000] = 32.000 Parameter
                                                   = 3.2% der Original-Gr√∂√üe!
```

**LoRA-Formel:**

```
W' = W + ŒîW = W + A √ó B
```

Wobei:

- `W` = Originale Gewichte (eingefroren, nicht trainiert)
- `A` = Down-Projection (Input ‚Üí niedrig-dimensionaler Raum)
- `B` = Up-Projection (niedrig-dimensionaler Raum ‚Üí Output)
- `r` = Rank (typisch 8-64, kontrolliert Kapazit√§t)

### 3.3 ICD-10 Klassifikation

**ICD-10** (International Classification of Diseases) ist das weltweite Standard-System f√ºr Diagnosen.

**Aufbau:**

```
J06.9
‚îÇ‚îÇ‚îÇ ‚îÇ
‚îÇ‚îÇ‚îÇ ‚îî‚îÄ‚îÄ Weitere Spezifikation (.9 = nicht n√§her bezeichnet)
‚îÇ‚îÇ‚îÇ
‚îÇ‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚îÄ Hauptgruppe innerhalb Kapitel (06 = Akute Infektionen obere Atemwege)
‚îÇ‚îÇ
‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Kapitel-Buchstabe (J = Atmungssystem)
‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Hierarchie-Ebene
```

**Beispiele:**

- `J06.9` = Akute Infektion der oberen Atemwege, nicht n√§her bezeichnet
- `I10` = Essentielle Hypertonie (Bluthochdruck)
- `G43.9` = Migr√§ne, nicht n√§her bezeichnet

---

## 4. Projektstruktur Erkl√§rt

### 4.1 Verzeichnisstruktur

```
finetuning/
‚îú‚îÄ‚îÄ src/                    # üì¶ Python Packages (src-layout)
‚îÇ   ‚îú‚îÄ‚îÄ config/            # üîß Konfiguration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ base_config.py # Alle Hyperparameter und Settings
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ data/              # üìä Datenverarbeitung
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py # L√§dt Daten von HuggingFace
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_processor.py  # Tokenisierung und Formatierung
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models/            # ü§ñ Modell-Wrapper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_model.py  # Abstrakte Basisklasse
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_model.py   # Large Language Model Wrapper
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ slm_model.py   # Small Language Model Wrapper
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ training/          # üèãÔ∏è Training
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ trainer.py     # Finetuning mit LoRA
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ evaluation/        # üìà Auswertung
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.py     # Metriken-Berechnung
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ visualization.py  # Plots und Reports
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/             # üõ†Ô∏è Hilfsfunktionen
‚îÇ       ‚îî‚îÄ‚îÄ __init__.py    # Logging, Helpers
‚îÇ
‚îú‚îÄ‚îÄ tests/                 # ‚úÖ Unit Tests
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ test_pipeline.py
‚îÇ
‚îú‚îÄ‚îÄ data/                  # üìä Daten-Outputs (raw, processed, cache)
‚îú‚îÄ‚îÄ models/                # üíæ Modell-Checkpoints und Finetuned Models
‚îú‚îÄ‚îÄ notebooks/             # üìì Jupyter Notebooks
‚îú‚îÄ‚îÄ experiments/           # üß™ Experiment Tracking (MLflow)
‚îú‚îÄ‚îÄ outputs/               # üìÅ Outputs (logs, metrics, plots, reports)
‚îÇ
‚îú‚îÄ‚îÄ main.py               # üöÄ Haupt-Pipeline
‚îú‚îÄ‚îÄ pyproject.toml        # üì¶ Dependencies
‚îú‚îÄ‚îÄ .gitignore           # Git-Ignore
‚îî‚îÄ‚îÄ GUIDE.md             # üìñ Dieser Guide
```

### 3.2 Warum src-layout?

Diese Struktur folgt **MLOps Best Practices** und verwendet das **src-layout**:

1. **Saubere Trennung:** Code (src/) vs. Daten/Outputs (Projekt-Root)
2. **Build-Tool Kompatibilit√§t:** setuptools, pip, uv funktionieren problemlos
3. **Keine versehentlichen Imports:** Nur installierte Packages sind importierbar
4. **Separation of Concerns:** Jedes Modul hat eine klare Verantwortung
5. **Testbarkeit:** Klare Interfaces erm√∂glichen Unit Tests
6. **Reproduzierbarkeit:** Experimente sind nachvollziehbar

---

## 4. Konfigurationssystem

### 4.1 Datei: `src/config/base_config.py`

Das Konfigurationssystem nutzt **Pydantic** f√ºr type-safe Konfigurationen.

**Warum Pydantic?**

- Automatische Typvalidierung
- Defaults und Overrides
- Serialisierung (JSON speichern/laden)
- IDE-Unterst√ºtzung (Autocomplete)

### 4.2 Wichtige Konfigurationsklassen

#### DataConfig

```python
class DataConfig(BaseModel):
    dataset_name: str = "Ahmad0067/MedSynth"  # HuggingFace Dataset
    train_ratio: float = 0.7   # 70% f√ºr Training
    val_ratio: float = 0.15    # 15% f√ºr Validation
    test_ratio: float = 0.15   # 15% f√ºr finale Tests
    max_sequence_length: int = 512  # Max Token-L√§nge
    batch_size: int = 8
```

**Erkl√§rung der Split-Ratios:**

- **Training (70%):** Das Modell lernt von diesen Daten
- **Validation (15%):** Zum Tunen von Hyperparametern und Early Stopping
- **Test (15%):** Finale Evaluation - NIEMALS w√§hrend Training nutzen!

#### TrainingConfig

```python
class TrainingConfig(BaseModel):
    # Wichtigste Hyperparameter
    num_epochs: int = 3              # Durchl√§ufe durch Datensatz
    learning_rate: float = 2e-5      # Schrittgr√∂√üe beim Lernen
    warmup_steps: int = 500          # Langsamer Start
    weight_decay: float = 0.01       # L2-Regularisierung

    # LoRA-Konfiguration
    use_lora: bool = True
    lora_r: int = 16                 # Rank (8-64 typisch)
    lora_alpha: int = 32             # Scaling-Faktor
    lora_dropout: float = 0.05       # Regularisierung
```

### 4.3 Hyperparameter-Erkl√§rungen

| Parameter               | Typischer Wert | Bedeutung                          | Effekt wenn zu hoch            | Effekt wenn zu niedrig |
| ----------------------- | -------------- | ---------------------------------- | ------------------------------ | ---------------------- |
| `learning_rate`         | 1e-5 bis 5e-5  | Wie stark werden Weights angepasst | Instabiles Training, Divergenz | Zu langsames Lernen    |
| `num_epochs`            | 1-5            | Anzahl Durchl√§ufe                  | Overfitting                    | Underfitting           |
| `warmup_steps`          | 500-2000       | Schritte zum Hochfahren der LR     | Zu langsamer Start             | Instabiler Anfang      |
| `weight_decay`          | 0.01-0.1       | L2-Regularisierung                 | Zu starke Regularisierung      | Overfitting            |
| `lora_r`                | 8-64           | LoRA Rank/Kapazit√§t                | Mehr Memory, evtl. Overfitting | Zu wenig Kapazit√§t     |
| `lora_alpha`            | 2\*r           | LoRA Scaling                       | St√§rkere Adaptation            | Schw√§chere Adaptation  |
| `batch_size`            | 4-32           | Samples pro Schritt                | Memory-Fehler                  | Langsam, instabil      |
| `gradient_accumulation` | 1-8            | Simuliert gr√∂√üere Batches          | Langsamer                      | Weniger stabil         |

**Die "effektive Batch-Gr√∂√üe":**

```
Effektive Batch Size = batch_size √ó gradient_accumulation_steps √ó num_gpus

Beispiel: 4 √ó 4 √ó 1 = 16 effektive Batch Size
```

---

## 5. Datenverarbeitungs-Pipeline

### 5.1 Datei: `data/data_loader.py`

Diese Datei l√§dt den MedSynth-Datensatz von HuggingFace.

**MedSynth-Datensatz:**

- Synthetische Arzt-Patienten-Dialoge
- ICD-10 Diagnose-Codes
- Ca. 50.000 Beispiele

**Wichtige Methoden:**

```python
class MedSynthDataLoader:
    def load(self) -> Dataset:
        """L√§dt Dataset von HuggingFace Hub."""

    def get_statistics(self) -> Dict:
        """Berechnet Statistiken (L√§ngen, Verteilungen)."""

    def validate_dataset(self) -> bool:
        """Pr√ºft ob Dataset erwartete Struktur hat."""
```

### 5.2 Datei: `data/data_processor.py`

Hier werden die Rohdaten in ein f√ºr das Modell verst√§ndliches Format gebracht.

**Der Verarbeitungsprozess:**

```
Roher Dialog           Formatierung           Tokenisierung
"Patient: ..." ‚Üí  "[SYSTEM] Du bist..." ‚Üí [101, 234, 567, ...]
                  "[USER] Dialog..."       (Token IDs)
                  "[ASSISTANT] J06.9"
```

**Wichtige Konzepte:**

#### Tokenisierung

Tokenisierung wandelt Text in Zahlen um:

```
"Ich habe Kopfschmerzen"
‚Üí ["Ich", "habe", "Kopf", "##schmerzen"]  (Subword Tokenization)
‚Üí [1234, 5678, 9012, 3456]                 (Token IDs)
```

#### Chat-Templates

Moderne Modelle erwarten spezielle Formatierungen:

```python
# Phi-3 Format
<|system|>
Du bist ein medizinisches Assistenzsystem.
<|end|>
<|user|>
Analysiere diesen Dialog...
<|end|>
<|assistant|>
J06.9
<|end|>
```

### 5.3 Train/Val/Test Split

```python
def split_dataset(dataset, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
    """
    Warum 3 Splits?

    Training (70%): Modell lernt von diesen Daten
    ‚Üì
    Validation (15%): Pr√ºft Fortschritt W√ÑHREND Training
                      - Hyperparameter-Tuning
                      - Early Stopping
    ‚Üì
    Test (15%): Finale Evaluation NACH Training
                - Nur EINMAL nutzen!
                - Niemals f√ºr Entscheidungen w√§hrend Training
    """
```

---

## 6. Modell-Architektur

### 6.1 Datei: `models/base_model.py`

Definiert die abstrakte Schnittstelle f√ºr alle Modelle.

**Design Pattern: Strategy Pattern**

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  BaseModel   ‚îÇ  (Abstract)
                    ‚îÇ  - predict() ‚îÇ
                    ‚îÇ  - load()    ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üë
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚Üì                           ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   LLMModel   ‚îÇ           ‚îÇ   SLMModel   ‚îÇ
    ‚îÇ   (Gro√ü)     ‚îÇ           ‚îÇ   (Klein)    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Vorteile:**

- Einheitliche Schnittstelle f√ºr alle Modelle
- Einfacher Austausch von Modellen
- Konsistente Evaluation

### 6.2 Datei: `models/llm_model.py`

Wrapper f√ºr Large Language Models.

**Wichtige Features:**

#### 4-bit Quantisierung

```python
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,           # Aktiviert 4-bit Quantisierung
    bnb_4bit_compute_dtype=torch.float16,  # Rechentyp
    bnb_4bit_use_double_quant=True,  # Nested Quantization
    bnb_4bit_quant_type="nf4"    # Normal Float 4-bit
)
```

**Was ist Quantisierung?**

```
Original (float32):    32 bit pro Parameter  ‚Üí 100% Memory
Half Precision (fp16): 16 bit pro Parameter  ‚Üí 50% Memory
4-bit Quantisierung:   4 bit pro Parameter   ‚Üí 12.5% Memory
```

**Trade-off:** Weniger Memory, minimal schlechtere Qualit√§t.

#### Generation-Parameter

```python
outputs = model.generate(
    max_new_tokens=256,      # Max Ausgabel√§nge
    temperature=0.7,         # Kreativit√§t (0=deterministisch, 1=kreativ)
    top_p=0.9,              # Nucleus Sampling
    top_k=50,               # Top-K Sampling
    repetition_penalty=1.1,  # Verhindert Wiederholungen
)
```

**Sampling-Strategien erkl√§rt:**

| Strategie                      | Beschreibung                       | Wann nutzen?                      |
| ------------------------------ | ---------------------------------- | --------------------------------- |
| **Greedy** (`do_sample=False`) | Immer wahrscheinlichstes Token     | Deterministische Ergebnisse n√∂tig |
| **Temperature**                | Flacht Probability-Verteilung ab   | Kreativere Ausgaben               |
| **Top-K**                      | Nur aus K besten Tokens w√§hlen     | Verhindert "verr√ºckte" Tokens     |
| **Top-p (Nucleus)**            | Aus kleinster Menge die p% abdeckt | Dynamischer als Top-K             |

---

## 7. Training mit LoRA

### 7.1 Datei: `training/trainer.py`

Implementiert das Finetuning mit Parameter-Efficient Fine-Tuning (PEFT).

### 7.2 LoRA im Detail

```python
lora_config = LoraConfig(
    r=16,                    # Rank - Kapazit√§t der Adapter
    lora_alpha=32,           # Scaling-Faktor
    lora_dropout=0.05,       # Regularisierung
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],  # Welche Layer
    task_type=TaskType.CAUSAL_LM,  # Aufgabentyp
)
```

**Welche Layer werden adaptiert?**

```
Transformer Block
‚îú‚îÄ‚îÄ Self-Attention
‚îÇ   ‚îú‚îÄ‚îÄ q_proj (Query)      ‚Üê LoRA Adapter
‚îÇ   ‚îú‚îÄ‚îÄ k_proj (Key)        ‚Üê LoRA Adapter
‚îÇ   ‚îú‚îÄ‚îÄ v_proj (Value)      ‚Üê LoRA Adapter
‚îÇ   ‚îî‚îÄ‚îÄ o_proj (Output)     ‚Üê LoRA Adapter
‚îú‚îÄ‚îÄ MLP
‚îÇ   ‚îú‚îÄ‚îÄ gate_proj
‚îÇ   ‚îú‚îÄ‚îÄ up_proj
‚îÇ   ‚îî‚îÄ‚îÄ down_proj
```

**Parameter-Einsparung Beispiel (Phi-3 Mini, 3.8B Parameter):**

```
Ohne LoRA:  3,800,000,000 trainierbare Parameter
Mit LoRA:      10,000,000 trainierbare Parameter (0.26%!)
```

### 7.3 Training-Loop erkl√§rt

```python
# Vereinfachter Training Loop
for epoch in range(num_epochs):
    for batch in train_dataloader:
        # 1. Forward Pass: Berechne Predictions
        outputs = model(batch)
        loss = outputs.loss

        # 2. Backward Pass: Berechne Gradienten
        loss.backward()

        # 3. Gradient Clipping: Verhindert explodierende Gradienten
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)

        # 4. Optimizer Step: Update Weights
        optimizer.step()

        # 5. Learning Rate Scheduler
        scheduler.step()

        # 6. Zero Gradients f√ºr n√§chste Iteration
        optimizer.zero_grad()
```

### 7.4 Wichtige Training-Konzepte

#### Gradient Accumulation

```
Ohne Gradient Accumulation (batch_size=4):
    Batch 1 ‚Üí Forward ‚Üí Backward ‚Üí Update
    Batch 2 ‚Üí Forward ‚Üí Backward ‚Üí Update
    ...

Mit Gradient Accumulation (batch_size=4, accumulation=4):
    Batch 1 ‚Üí Forward ‚Üí Backward (sammle Gradienten)
    Batch 2 ‚Üí Forward ‚Üí Backward (sammle Gradienten)
    Batch 3 ‚Üí Forward ‚Üí Backward (sammle Gradienten)
    Batch 4 ‚Üí Forward ‚Üí Backward (sammle Gradienten) ‚Üí Update

    Effekt: Wie batch_size=16, aber nur Memory f√ºr 4!
```

#### Early Stopping

```python
early_stopping_callback = EarlyStoppingCallback(
    early_stopping_patience=3,     # Stoppt nach 3 Evals ohne Verbesserung
    early_stopping_threshold=0.001 # Minimale Verbesserung
)
```

**Warum Early Stopping?**

- Verhindert Overfitting
- Spart Trainingszeit
- W√§hlt automatisch besten Checkpoint

---

## 8. Evaluation & Metriken

### 8.1 Datei: `evaluation/metrics.py`

Berechnet verschiedene Qualit√§ts-Metriken.

### 8.2 Metriken erkl√§rt

#### Exact Match Accuracy

```
Prediction: "J06.9"  vs  Reference: "J06.9"  ‚Üí Match! ‚úì
Prediction: "J06.1"  vs  Reference: "J06.9"  ‚Üí Kein Match ‚úó
```

#### Prefix Match Accuracy

Ber√ºcksichtigt die ICD-10 Hierarchie:

```
3-Char Prefix:
  "J06.9" und "J06.1" ‚Üí "J06" = "J06" ‚Üí Match! ‚úì

1-Char Prefix (Hauptkategorie):
  "J06.9" und "J10.0" ‚Üí "J" = "J" ‚Üí Match! ‚úì
```

#### Precision, Recall, F1

```
                    Tats√§chliche Klasse
                    Positiv    Negativ
Vorhergesagt  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
Positiv       ‚îÇ    TP    ‚îÇ    FP    ‚îÇ  ‚Üê Precision = TP/(TP+FP)
              ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
Negativ       ‚îÇ    FN    ‚îÇ    TN    ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üë
              Recall = TP/(TP+FN)

F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)
```

**Wann welche Metrik?**

- **Precision wichtig:** Wenn falsche Positive teuer sind (z.B. unn√∂tige Behandlung)
- **Recall wichtig:** Wenn falsche Negative teuer sind (z.B. √ºbersehene Krankheit)
- **F1:** Balancierter Trade-off

### 8.3 Performance-Metriken

```python
metrics = {
    "latency_seconds": 0.05,         # Zeit pro Prediction
    "throughput_samples_per_sec": 20, # Predictions pro Sekunde
    "tokens_per_second": 100,         # Token-Generierungsrate
}
```

---

## 9. Experiment-Workflow

### 9.1 Schritt-f√ºr-Schritt Anleitung

#### Schritt 1: Environment einrichten

```bash
# Virtuelle Umgebung erstellen
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# .venv\Scripts\activate    # Windows

# Dependencies installieren
pip install -e ".[dev]"
```

#### Schritt 2: Memory-Optimierung aktivieren (WICHTIG!)

```bash
# Setze PyTorch Environment Variable f√ºr besseres Memory Management
export PYTORCH_ALLOC_CONF=expandable_segments:True

# F√ºr permanente Aktivierung in ~/.bashrc oder ~/.zshrc:
echo 'export PYTORCH_ALLOC_CONF=expandable_segments:True' >> ~/.bashrc
```

#### Schritt 3: Nur LLM Evaluation (ohne Training)

```bash
# Testet nur gro√üe Modelle (zero-shot)
# N√ºtzlich f√ºr schnellen Test der LLM-Performance
python main.py --experiment baseline
```

#### Schritt 4: Vollst√§ndiger Durchlauf (LLMs + SLM Training)

```bash
# PHASE 1: Evaluiert LLMs (Llama 8B, Mistral 7B)
# PHASE 2: Trainiert und evaluiert SLMs (Llama 3B, Qwen 3B)
python main.py --experiment full
```

#### Schritt 5: Nur SLM Training (ohne LLM Evaluation)

```bash
python main.py --experiment training
```

#### Schritt 6: Ergebnisse analysieren

```bash
# √ñffne generierten Report
open outputs/reports/evaluation_report.html

# Oder inspiziere results.json
cat outputs/reports/results.json
```

### 9.2 CLI-Optionen

```bash
python main.py --help

Options:
  --experiment {baseline,training,full}
  --skip-training          # Nutze existierendes Modell
  --model-llm MODEL_NAME   # Override LLM
  --model-slm MODEL_NAME   # Override SLM
  --config PATH            # Eigene Config-Datei
```

### 9.3 Modelle anpassen

In `config/base_config.py`:

```python
# === LLMs: Gro√üe Modelle (zero-shot) ===
llm_models = [
    ModelInstanceConfig(
        name="meta-llama/Meta-Llama-3.1-8B-Instruct",
        size="8B",
        description="Large reference model",
        load_in_4bit=True,
    ),
    ModelInstanceConfig(
        name="mistralai/Mistral-7B-Instruct-v0.3",
        size="7B",
        description="Medium reference model",
        load_in_4bit=True,
    ),
]

# === SLMs: Kleine Modelle (werden finetuned) ===
slm_models = [
    ModelInstanceConfig(
        name="meta-llama/Llama-3.2-3B-Instruct",
        size="3B",
        description="Compact model for finetuning",
        load_in_4bit=False,  # Full precision f√ºr Training
        dtype="bfloat16",
    ),
    ModelInstanceConfig(
        name="Qwen/Qwen2.5-3B-Instruct",
        size="3B",
        description="Alternative compact model",
        load_in_4bit=False,
        dtype="bfloat16",
    ),
]
```

**F√ºr kleinere GPUs (<16GB VRAM):**
```python
# Nutze nur ein LLM und ein SLM
llm_models = [ModelInstanceConfig(name="mistralai/Mistral-7B-Instruct-v0.3", size="7B", ...)]
slm_models = [ModelInstanceConfig(name="Qwen/Qwen2.5-3B-Instruct", size="3B", ...)]
```

> **Hinweis:** F√ºr `meta-llama` Modelle m√ºssen Sie die Nutzungsbedingungen auf
> HuggingFace akzeptieren: https://huggingface.co/meta-llama

---

## 10. Best Practices & Troubleshooting

### 10.1 Memory-Probleme

**Symptom:** `CUDA out of memory`

**Neue verbesserte Memory-Management-L√∂sung:**

```bash
# 1. Setze PyTorch Environment Variable (WICHTIG!)
export PYTORCH_ALLOC_CONF=expandable_segments:True
python main.py --experiment full
```

**In der Config anpassen:**

```python
# 2. Kleinere Batch Size
training.per_device_train_batch_size = 16  # Statt 32

# 3. Mehr Gradient Accumulation
training.gradient_accumulation_steps = 2  # Statt 1

# 4. Aktiviere 4-bit Quantisierung f√ºr LLMs (bereits Standard)
llm_models = [
    ModelInstanceConfig(..., load_in_4bit=True)
]

# 5. Gradient Checkpointing bei Bedarf
training.gradient_checkpointing = True
```

**Die neue Pipeline cleaned automatisch:**
- Nach jedem LLM Evaluation: `aggressive_memory_cleanup()`
- Nach jedem SLM Training: `aggressive_memory_cleanup()`
- GPU Memory wird zwischen Modellen vollst√§ndig freigegeben

### 10.2 Training konvergiert nicht

**Symptom:** Loss sinkt nicht

**L√∂sungen:**

```python
# 1. Learning Rate anpassen
training.learning_rate = 1e-5  # Versuche kleinere Werte

# 2. Mehr Warmup
training.warmup_steps = 1000

# 3. Pr√ºfe Daten
# Sind Labels korrekt? Ist Formatierung konsistent?
```

### 10.3 Overfitting

**Symptom:** Train-Loss sinkt, Val-Loss steigt

**L√∂sungen:**

```python
# 1. Mehr Regularisierung
training.weight_decay = 0.05
training.lora_dropout = 0.1

# 2. Weniger Kapazit√§t
training.lora_r = 8

# 3. Fr√ºher stoppen
training.early_stopping_patience = 2
```

### 10.4 Reproduzierbarkeit

F√ºr identische Ergebnisse bei jedem Lauf:

```python
# In config setzen:
experiment.seed = 42
experiment.deterministic = True

# Aber Achtung: deterministic=True macht Training ~10% langsamer!
```

---

## üìö Weitere Ressourcen

### Papers

- LoRA: https://arxiv.org/abs/2106.09685
- QLoRA: https://arxiv.org/abs/2305.14314
- Transformer: https://arxiv.org/abs/1706.03762

### Dokumentation

- HuggingFace Transformers: https://huggingface.co/docs/transformers
- PEFT (LoRA): https://huggingface.co/docs/peft
- BitsAndBytes: https://github.com/TimDettmers/bitsandbytes

### ICD-10

- WHO ICD-10: https://icd.who.int/browse10/2019/en
- DIMDI (Deutschland): https://www.bfarm.de/DE/Kodiersysteme/Klassifikationen/ICD/ICD-10-GM

---

## ü§ù Beitragen

1. Fork das Repository
2. Erstelle Feature Branch: `git checkout -b feature/meine-feature`
3. Committe √Ñnderungen: `git commit -m "Add: Meine neue Feature"`
4. Push zum Branch: `git push origin feature/meine-feature`
5. Erstelle Pull Request

---

## üìÑ Lizenz

MIT License - siehe LICENSE Datei

---

_Dieser Guide wurde erstellt, um Machine Learning Engineering Best Practices zu vermitteln.
Bei Fragen oder Problemen bitte ein Issue erstellen._
